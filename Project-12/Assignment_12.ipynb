{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_12",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ste1kEoy6LkL",
        "colab_type": "text"
      },
      "source": [
        "#Loading google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etjfS1ylvIn3",
        "colab_type": "code",
        "outputId": "25e2d27c-2288-4f5b-d685-8018caa02d94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('../content/drive/', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at ../content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdGaOn3L6SCv",
        "colab_type": "text"
      },
      "source": [
        "#Loading Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rCHp34HmF_f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b7af39a0-e1f8-4519-ca8b-b2fbbacd4145"
      },
      "source": [
        "from keras import backend as K\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "% matplotlib inline\n",
        "np.random.seed(2017) \n",
        "from keras import regularizers\n",
        "from keras.models import Sequential\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D,AveragePooling2D\n",
        "from keras.layers import Activation, Flatten, Dense, Dropout\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PARnOT7v6j2U",
        "colab_type": "text"
      },
      "source": [
        "#Loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vm9_KN8WmeyD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9dd4bbdf-4250-401f-dfd4-c8bf36537f59"
      },
      "source": [
        "\n",
        "from keras.datasets import cifar10\n",
        "(train_features, train_labels), (test_features, test_labels) = cifar10.load_data()\n",
        "num_train, img_rows, img_cols,img_channels =  train_features.shape\n",
        "num_test, _, _, _ =  test_features.shape\n",
        "num_classes = len(np.unique(train_labels))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZRnHZUdmhJ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "(train_features, train_labels), (test_features, test_labels) = cifar10.load_data()\n",
        "num_train, img_rows, img_cols,img_channels =  train_features.shape\n",
        "num_test, _, _, _ =  test_features.shape\n",
        "num_classes = len(np.unique(train_labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07J-ak1rmsyZ",
        "colab_type": "code",
        "outputId": "5e0429f0-3e32-43e8-82c5-52eea048e7e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        }
      },
      "source": [
        "\n",
        "class_names = ['airplane','automobile','bird','cat','deer',\n",
        "               'dog','frog','horse','ship','truck']\n",
        "fig = plt.figure(figsize=(8,3))\n",
        "for i in range(num_classes):\n",
        "    ax = fig.add_subplot(2, 5, 1 + i, xticks=[], yticks=[])\n",
        "    idx = np.where(train_labels[:]==i)[0]\n",
        "    features_idx = train_features[idx,::]\n",
        "    img_num = np.random.randint(features_idx.shape[0])\n",
        "    im = features_idx[img_num]\n",
        "    ax.set_title(class_names[i])\n",
        "    plt.imshow(im)\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdAAAADECAYAAAAvbXA5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzsvXm8JUd1JvidXO769vdqX7UvgBaE\nQAiw2WywbM/QjNzex7gNM3bT7aVt46WZHuzGjds9brcxPW03TbcHY/ACXmCMzW6zSICQEGhDqiqp\nql5tb9/ufm9m9B/nRMa599169d6rK1VJju/3q7r3ZeTNjIyMjIxz4jvfIWMMPDw8PDw8PLaG4FJX\nwMPDw8PD49kI/wL18PDw8PDYBvwL1MPDw8PDYxvwL1APDw8PD49twL9APTw8PDw8tgH/AvXw8PDw\n8NgGLukLlIh+mIg+eRG/fxMRfXGQdfIYPIjo74nozecpO0hEFSIKL7TvcwlEdJyIXttn+yuI6PEt\nHusPieidg6udh8fg8Fzun5f0BWqM+WNjzHdeyjr8Y8Hl+mIyxpw0xgwZY5JLXZfLAcaYLxhjrrvU\n9fDoxvkmPB7/uHHZunCJKLrUdfDwuJzgnwkPD8bl8iw8Iy9QIvplIjpGRGtE9CgR/RPZ3uWCJSJD\nRG8loiMAjqhtP01ETxLRPBH9ByLqW28i+l0imiaiVSK6n4heocreQUR/RkTvl3o8QkQvUuV7iegj\nRDRHRE8R0U8/bQ1yEdigLd9BRB9Q+x2WtouI6DcAvALAe8Rd+h7Z504iuo+IVuTzTvX7vyeidxLR\nPfKbjxHRJBH9sbTvfUR0WO1/3mMJriKir8pv/5qIJnrreZ7r/WdE9BgRLRHRJ4jo0ICa8nLA7XIP\nl4jofxBRgYheSUSn7A5i+fwSEX0TQFXu561E9ID0gT8FULh0l/DsAxEdIKK/kGd9gYjeQ0RXEdFn\n5e956edjsv8fATgI4GPyLLzt0l7B5Y2N+icRfQ8RPUhEyzK23KTKzjsGy/j2YSL6ABGtAnjTM3pR\n54Mx5mn/B+D7AOwFv7C/H0AVwB5wI3xR7WcAfArABICi2vY52XYQwBMA3ixlvb//EQCTACIAPw/g\nHICClL0DQAPAXQBCAO8C8GUpCwDcD+DfAMgBuBLAkwBe90y0z4Da8h0APqD2OyxtF8nff2/bTf6e\nALAE4EelvX5Q/p5U+x8FcBWAUQCPStu/VvZ/P4D/sYVjnQbwfABlAB+xdd2ongD+V6nDDXLctwO4\n51LfgwHdx+MAHgZwQNrvSwDeCeCVAE717Peg7FeU/nkCwM8BiAHcDaAN4J2X+pqeDf/k2f8GgN+R\nvlgA8HIAVwP4DgB5ADsAfB7Af+q5D6+91PW/3P9t1D8B3ApgFsBL5D78mLRrHhcYg8HjWxvAG2Tf\n4qW+VmPMM/MC7dPID8rg+Casf4G+umdfA+D16u9/DuAz8r3r933OswTgZnUDPq3KbgRQl+8vAXCy\n57e/AnlBXM7/VFu+A1t7gf4ogK/2HOteAG9S+/9rVfbbAP5W/f29AB7cwrF+s6ftW/IQnbeeAP4W\nwE+o3wUAagAOXep2H8B9Ow7gJ9XfdwE4hv4v0H+m/v42AGcAkNp2D/wLdLPt/lIAc7a/bbDfGwB8\nvec++Bfohdv3vP0TwH8B8G979n8cwLdfaAyW8e3zl/r6ev89I35kIvrfAfwr8GAJAEMApgD0I45M\nX2DbCbAF1u88vwDgJ6TcABiR81icU99rAAriOjwEYC8RLavyEMAX+l/RpcMGbblV7AW3pcYJAPvU\n3zPqe73P30NbOFbvPYxx4XofAvC7RPTbahvJcXvP92zEpvp1z357AZw2Mqqo33psDgcAnDDGdPRG\nItoF4HfBSx3D4Mna0jNfvWc9NuqfhwD8GBH9S1WWk98kuPAY3O/dcEnxtK+ByprVewH8C7BLbwzs\nuqLz/KRfepgD6vtB8Ayn9zyvAPA2AP8UwLicZ2WD82hMA3jKGDOm/g0bY+7axG+fMVygLasASmr3\n3T0/723XM+AOrXEQ7GrdKjZzrN572AYwf4HjTgP4P3vuS9EYc8826ng54oL9WqDv3VkA+4hI9+uD\ng67YcxjTAA72WXP/d+B2foExZgS8HKTb2Ket2hw26p/TAH6j53kuGWM+hM2NwZfdPXgmSERl8IXP\nAQAR/Th4LWwr+EUiGieiAwB+BsCf9tlnGEBHzhMR0b8BW6CbwVcBrAlZo0hEIRE9n4hu32I9n25s\n1JYPAvg24rjKUbD7Q2MGvK5g8XEA1xLRDwkx5fvBrtX/fxv12syxfoSIbiSiEoBfB/Bhc+HQld8H\n8CtE9DwAIKJRIvq+bdTvcsVbiWi/EKr+Nfr3617cC+7nP01EMRG9EcCLn85KPsfwVfAg/5tEVBbi\n1svA40cFwAoR7QPwiz2/631+PPpjo/75XgA/SUQvIUaZiL6biIbx7BmDu/C0v0CNMY+C18/uBXfC\nF4AJE1vBX4MXmB8E8DcA3tdnn08A+Dsw0eUEmDC0KZNfBvLvAXALgKfAltF/A5NnLhts1JbGmE+B\nB+Bvgtuq90X4uwDuFsbnu40xC+Br/nkAC2Dr/XuMMReyCvvVazPH+iMAfwghdgG4IMvZGPOXAP49\ngD8R5t3DAL5rq/W7jPFBAJ8EkyWOgdeJNoQxpgXgjeD1/0Uwkewvnr4qPrcgz/r3gklDJwGcArfh\nrwF4Idhr9TdY36bvAvB2YY/+wjNX42cXNuqfxpivAXgLgPeA3eNHZb9nzRjcC+p2VV9+ICID4Bpj\nzNFLXRcPDw8PDw+Ly1ZIwcPDw8PD43KGf4F6eHh4eHhsA5e9C9fDw8PDw+NyhLdAPTw8PDw8toEt\nCSmEYWjiOO7a5mRp2ZLVBq3JwnY2CMUk++H2CSSEyEYSdYUUrTOYL2BBE/X86f7urZW2xjeyzE22\nT/992+0WOp3OZuJPNwQTqKj3Ejbzy+3t3+9n9tr6VWJT3ov1+zwdXg9jzEW3d7lUNBMj3ZFPSZoC\nAMIw5M/AnSZpNwEAQcDPgO5bnU5HtnFZGOWystRefyr7qPOl0l6FIof0xjn3u3q1CgBo1OvZNnvO\nMOLnkoJwXR1CqYPR90J+Z+T6kk7b1cFec3ZMN89ut3i/+dXVeWPMDlwkoiiSMWV9n0gTrkcncdFO\nxo43ffpQNtpc/JO37qi6z9I2T5Ado0/3p2zMs8fuHotMmg6kj5eGimZschRB6A4VhtymQdA97gJ6\nLJZPPU5LX0vk/tj+xsfksiiMZF/XhwK5hxT0XjOy+6qPZftjv5Hb/tS2VqCOlaama3fdj+1X+3zb\nc9jfzZ5dwMpy5YLtvaUXaBzH2H+gO14+iqKuirdUZ2/LBRgpC4L1F2cbLw7dg1+QG1rI5Xkf1XkD\naY1UzkOqI5D8Tnf23sGt381qy83qO7DLtlSVtTp87iRZH8YYBAGefHIwhGEiQhTlul/6blbR/Tf0\nNYbyGaz/narnZusAnKdt0vWDi/1uO6QO9czKjO20uizp2qffMc83cA3qhTw5Noa3veUnUCw6PQrb\nZ2s1fnmlDReZUwx4W6vFdW/U3UuIRVSAkZEJAEBcGM5K5mZYEKuY52eHIvcYju1k/Yt9h/dznUbd\nC33+7CwA4N4vOh2JlZUVAMDu/Sz6pO/53MxiVx0Qu5fx/NoqX9fiHF+nTAYAoNnm52F8kt+PlVot\nK2u3+d7917/9xEDUj3JxjGuuugL61WAH3aUlFgKaX17LylKZiHTkGdzoXRaqscGge6Ds2/+l7SI1\n2bF9q9Vqud3smGWNCVUJu38iY0qiBuZsvJB9QjXm2RdTGPG2QN3HZrOJTtW1wcVgaHQIb3zL9yCN\nXb3GJlhQLA65XlHgyjod7hflEj8TQ6VyVlaQ6ydxZK4suzaqV2VMlbYpFl2bTsj5ymU+5ujwWFYW\nh9KmqWvTWoX7X7vBdSmodktl3KjUuH2aHdePc3k+Z0v6dnnYPdfFkujbi56GQT4rq9Ra+OWf/A1s\nBt6F6+Hh4eHhsQ34F6iHh4eHh8c2sC0xeeryM4u5H1g/unIbotudlyTr1xF6XX4A0LF+8IBdYpFa\n17HuEuv+MNjY1dfrEtTnabX5+NalGCtXWrZGZNdAoN3Ctu6BXNd6N+WgEARBt5u2p511mdt2fhdu\n7/pD7zG2AmNduKpN7bZ+btq0Z70h1f3I9N8HWH8Pt1vfCyFNDaqVBuL8ULZtxyS7P48/9SQAYCjf\nyMrCAn9PZB4a5hw/wKTcl+I8u4Za9ZWsLCcuslhcqi3lvxyenAQA5MW9O3P2pKog77/vwOFsE0Vn\nAQBlcTt3mlW3e1vqJ407OT6ZlR07xSlHScpics9YscR1tv263XDXPKqOMSiEZKCGhmy5pCNdwGD9\n84w+/ArbL7LxRvUh22Xc0kKfNdfUruW539lnKlJjg3Xd2jGo+zzddYBRx+oZs4Jo/fCb2D7eszQ1\nqC6fy4U4uH8SgXKp1uX+5qR+eeXqX1tlV38iy+65snODRtLvI/nd5KgrWyMeW5dXK/z7jhsHqk0+\npiE+L6llngDStsY9SwvzrC9fW+W+fWByZ1ZWkOvYPcVLH0ng1k4Xlnm5Je7wsUzd3fOVCj+PYWxd\nt275JYyLoE0uN3sL1MPDw8PDYxvYogVKCIKge/bmqKh8QDWrSsnOvoz6v/t3drZpLRf+Hb/9k5Rn\nNnGkZvY9p41id74kXU/u6bVadJnd384M0UUs6LZ6umesPcy0PhbvIEBECMOoi3xlLdDMEtVWZmDJ\nWmHXJ3/vsVz7zNw3QnZVfe59N4PNWpBSlgR9yqyFoE6Qfbf3py8leF0VBolWs4XjT53EXuP61LCw\nci37Ngrd9URxUT7FskjV4yQz6NFRJkgsth0Rp1DgslqVt60oC29CyCI7JnlmvTBzStWFiUKHDl/l\n6tzmxliY48Q31HHnSROejc8vMJkoP+xkRXPSNXIRX4Mm8VlC4MqS/C7nrquQV96gAcCYFK1mAxS6\nZ9wyiTu2g9D6eX6/5zJjSstnmipmsewXZJ4lB8dktudZ77np91zbcaOjrUxLArJ9QZXZvmOtsS6y\no/UMSbfXnrw4jpEMyAQt5HO45sp9oNiRZqpVNi/jgO9BMecsyfZOrqsldE6MOVJbu8W/67SYpKMt\n9x0jXP+KMMbzw6rf5GSbtFWOCllR0hECaeTISrus10P6+oEpl/WvI0z2tRpblLNLs+p3bKmWclzn\nkfJ4VhbE3J7nZpnQt7S8mpXFcdTF5t0I3gL18PDw8PDYBra8Btr7Xu61wiI1k0VoY83Wz7jsq9uu\nwWizgqh7lpuoWZydXXaEav+8G56Xlc3PMSX/9GmXhrJ3BtnPerazjVSHpdg1ky4zyR4z6KpLr8U7\nOAuJEIZxV/zSOgtUlbl4LvkMdVn37LzLcu2NE1xvbLv1TrPe2iS9BpRt4zYJVZOmQk3PlkMS1f0S\nG4Ik4QnQ6yLoKuu2BgZnjrbbHczOLICKjlZvrcuVZQ6pGN3jZuABsZXYbDJ9X/eV/ft5TabRZOuy\n3nJWpunY0Ij1caCFHF+tXTOtVlxO5x272PKc3Lkn23bk6HEAwMICrxNR6tZA600Jtyjymu7yggvB\nGYpteAGXlcruutZWl6U9+LqiwK2JRcFgzX+TpmjUqogKzuLIx2yRZAaN8qTY/tdvPdyNRfx3TsXQ\nxuKpaoi1r59ZZ4FKWJEaf7L1/X68Ctv31HOWwK5hynMWrx9ijVhNpmu8kbpnsZGuDnEQornl2O7+\nIATIhWWYtuurJVl3bKyx96IRu3CU8Qm2/kaH+JkIjQq9MWzh56QvUeTaYWSc97drkss114+rLfFA\nCQdltaHClIw8U6HrZ5NlDgHbMSUWZE61hVR1SPrx1NhEVpSXteq8eDcKBcdtsK+dEbF+zV63dhoX\nA5SLzireCN4C9fDw8PDw2Ab8C9TDw8PDw2Mb2KIL18AY05d00qvAAQCR0Po74lLU8kyxqI1APnUo\nhC3r53btdU+Wy84st+c+e/Zstq2XVq6Plabd7ijtDtV1BbpVQ6zTrdPpr0Q0KMo5ESEIo8w1C2zs\nwrUErkxGK3J1DqVNw2ybcsUE3QSGnloAUJJviZLYku9pH7WVjpWG02E2ptsdRl3kEHsM27a6JOna\nJQj0PUzVby8OQRiiPDSC8YmpbNuJk0zimTl+DAAwVr4uK8vFTLYoiYupS35MrmN5hd2hVaXmY0Tl\nUTxYKJScu0iiV7B3N7vOorYjDJUn2D3VVq6/kTF2a+3YsQsAsDjvSEehkH9KQ/yMtJquDp0q1+vQ\ngYMAAIqdC3VudgaAC9fQzyY9DQyuICA0m05BJhUyiyMRafUw+egji2ef9Y407IGDh7OyQ4f4Oh99\n5FEAwJIQpABHMEoyF/zG11gqMvFqz14mdZ1bdK7xisgthjb8RUuf2iWjSCQKW+6akx5Zx0bL9aUg\nCLqesYtBHIXYvWMUJRWqlUp/rK4yuWdhaSYrCw37SIfLTCKKSZG9bMiJPI8m0WFpvK0u1xiqsZIa\nfG21ZW6rtVXnMm7IMxEE7jxzIS/NrU7xc7l3YndWlpexbrjArl+z6sLF5hb5HjfXeFug7EV7qzuG\n6xLl3D1PIqBZd0shG8FboB4eHh4eHtvAlsNYiKjL6um17LoIHlJWLvCMrd12tHI7W7ez27yaqVkL\nyu7fbTUKSSXTo3SXMCQz7X6U8371y8QF+ogSbERftzNVa23p33HdB2aCIozCbDbrju9ICtoythZo\nFNlA72hdWabfqcrWhcT0qb4lEXW0BSokE+11yATMg0jK3D231qsjmrnfGWO/25AQbfVwnRPqrCuT\nmqyv8DYQxzF27d2LXVOO7n7mxBN8BiE3nJx2s/NGi9vk+ptuBQDk1H06feY4AKBA3EZDeRc2sNLg\nbW2h7A/llfYu8TF37eJZdiFw96ku4TXn5p3FkyvLPc+xFRtFTnN3coKJQW25r5XlSlbWWuTv87M8\nS4/K7l6sivYoCXlIE2GaWu53AKCAkC/mQUr7tGkFIBL+1NaIkTYOrWdJe4FE6D4VUpcNrgeAF91x\nJ/9ezvPQ1x7IytZStpJSOw6ocIyOEGUi9bxccw17IW4UAuNnP//prGxpjsMoQrFSNVnPhr0EBb5X\niieDliWiZc+Zstg6ycDC4ygg5AohYhWaZMTDNbSb+8v4lOtD1jIr5HhsbSu952yMTMWCVI9hIm24\nssDhIWtKy9dq0+YMt5G1SAGgXeWyiQlXh4khfj72jLJXZrfSzrWMxFNPstDJkccezYqWhFS6Jv25\n2VL3VZQ7KOZrzxXUoBcarK64Z2UjeAvUw8PDw8NjG9h6GAtR39mQtWy0NWYDhm06pVCVdbL1R8lM\n0CcNk1m3NgEEgQT2yizu1ltfmJUtzPOM48tf/nJXfTW60ur0hH30W2vtl41BSxL2/i7ltEMYBAhs\nrek1UJtiys5mdXo5+z2KZL1CSXLl5Ltdlw5yziKy66LUd21SYO+FluaTdau2ahvbTnZNq6PCN2wq\nLO2JyK611+pVTZhksTSyQWWLQJogMevXoreDXC6H/YcOolpZzrbZjBPj45yZZGl1IStbrnBCktw4\nB3Zfe901WVlDrj8X8PXr8K68WJyj47ymWW+72W6zUZdPbse1umvb0jhbCAtLT2Xbzpzj8IDiMJft\njJ01O7mb67y4ylZmqtZAFyuSGk1MymbTrQnarFn5Ah+rXHbro5XK5taGNosoijA+Po5Q9cdFCWq3\nQg6mK0xE+mrGAVBhVWK12fCIiNzwtmsPr1fe8oKbAADNM86Kf3SW72MttenfXOfLiaV26NChbNud\nL3spAKBclNAf1Z/bkrWHZMyiLhlSK3AifyqBlLgnbCJQY16n0dDOmotCq93GyZlZ5CO3VhhKO9lM\nK3nlLSmGvK0qYS9NtTZbq/J9suIauaCYlVVrTTmflMXOaiwX+XyTE+zpoUOuvZti4eaUG6wsHIiC\ncChWpt06//ET/P3hhx8DAEyfPufql9VB6qzuRSoyrG2xnnNFlQ2sEKHV3tyY4i1QDw8PDw+PbcC/\nQD08PDw8PLaBLYexdDqdvpkJrNtSU/mtEd6Wbzqswh3DdO8MpxLhvBha79KqALG7YHjYET4yF6Zy\nXWakoz7Zz5NMk3U9wSgVN227s97tmKny2L/N+V26FwMiQhjFPSQiIY0IsUOrrVi3dj5f7PoEgIJ8\nj4VsEqvEuDlx2WQqRWpaReIis17SjnJtNOvsnmw269m2hrgg6+LKaqrwjVYsLhVx87aaym2ftLJr\nBoC29uGK+olTgFH1IyDdZOaEC8GA+8vsjCMKZS5yIYWEgctGYkMWlpfYlXV6+vS63xnJ8rBQca7s\nKGaCxNgkf9bPOndiu8lki0Dua1B0CkErdXFlKU3PuXkmT5Rkv0lxVQJAW/p3XVyijYojcozt4mMU\nhaBx9qnprKwkyZOvuOIKAMDMOecWayaD6dsWJk2R1BpIm27cSESbNWcskU/9wGoS5+W+KDJMtuwi\n3cokzv09IvFB+3by/Tu0z2X0qIFd6GeW2T2fz7tlkb072Q1+0wuen2278ZrDAIBTQsCymsYAMldr\nkpHidOVFScs+Uyocy5KN7HJKqMawOJ9D2hyMrdNJUswvrQHklimKos28WudzDuVdnxuqSZ1hQ69c\nmzZlSaAhimKx8jO3RW0rN8L9rN3Uqkt8jWUJpRkvOJdxq8BjRHPFadPOn+KMRIuiW3vkhAtTPHWO\nl+2qVa5XJ3XtZESByAh5saVCpexw3pFnpFF3Y9haaDKluwvBW6AeHh4eHh7bwJZJREEQdBFXbCiH\n/dQkHdNDPAm6gve7CSs6p56bcQpVXU3iOjKziUUv0xIdAODwFQcAAOMTTg9xYWFBzmNp72pxXmYZ\nrdb5w2WybZpYI1lmTB9xhoHmqhQhhUCHqlh9x7zVd3RWpiV7FEpFKXNtUxQdyCHRPNXhP1b0oJlZ\nhm6WaYRrnwu5vccVhTwZ5jZptNzsrSpZEQo13r++6sgRdpbXaDSlDm6W3W7JbDbLtar0hW1ml3Q9\n2Svpo1W8XXQ6bczNzWWWJQDkQ6uP2q0dCwDlUbZOxsc5wPvRRx2FflIM/AO7OAPKzoNOEAFCYKg3\n2PIpllS2EzsbzwmhY9zR+TtiDTQSZ0meneX+ffXV3OeVUY+Tjx/h0y3xPjqv55SIADTEqlW8mUwb\n2j6TqWb4q5yng0Da7qA6u+CIYnwWAMBkka9dZyJp2zm/eKJ03Yxob0c2r2fdWTGVebZaGpIHMgid\nhbFTrPBCLIRGdb4x8c7sLOk8mDbXJYestDTpaIjvmxUVSJUhY6SvGhmDDLm+mwg5MhWrua3Id3mE\nXVldLgZhEGC4PIxWqoQ9pEntY2XHQwCYr/I11sXabHacFQexoG3GosCojEOSqaggBw3V87xHnpdx\nSQnUWnI6uasrfIzp48ezbUePMkFoZo49Q8trrm0sQSiW7DLFUBGg5DORPLqra65+tYrcmLYVflEW\nsjGbltj2FqiHh4eHh8c2sCULlIgQRVGXBdC73qclp3rFCHS2k17xAu1zzoy+Pjk8h8ps0dx2220A\ngKKifx8WqvnNN9+cbfv4xz/edT69lplZzz3iDBpOMuz81/x0gUAIw7BrzdmucxaLPCO24hH8XayW\nEm8rqHXOfI7nY4sib7U059a82iK3NS8B+hM73PpQcYhnl3UJRi7GzuK9+bYXAQBS4yzW/DiveVTF\nUi6oTB6VTOCBjxWrNfGWDaXJLFDlkUhsYP16iz8ZTASLHCvB6upKFvIDOOk6m5M1iFzZrn37pUyC\nslW3mJnhdq5L2McL916blZXEe3DmGK+ZxrHyiog8WjWxa6/O4q9X2WJ99Kij8cd5vsf2uVuYc/kQ\nbRjIqPTrkVF1LAlfWZMZf9SV8YQvZGWVLbiaWjuamnJrwINASAFGwwKMznYiYRVWDjFQ4Sj2O9nc\nmuqRbYuVUxFLsrrgQo6OPvoI/14yh0SjzlI5WJbMOdaTosJmTJPbtdZwXpazpzjs5eRTLLJhM/YA\nwOgkewKskEBHrf01hDNgl2adeIiTq7RZjAKdW7NQgluFvjgYY5B2WmgrS9L291TMrtWWs9yXRQZv\naYU/m2oNNJL153KFPQXDZef5KxMf49AIl127y2UQynf4mXj0K18DADz8yJGs7NwMr82u1ZS0o73H\n9gHrI1/aPzsV35ei9O2SWi+vylprtcL31Vq+/EPatByrt0A9PDw8PDy2Af8C9fDw8PDw2Aa25MI1\nZr3STq8rtl+i2o2INTbspSvbAHUfU5OWXvaylwEAXvWqVwIAxsccpX9sjF1Ur3nNa7JtX/jCFwA4\n12UXyamnnv3QV+O3BwMlDnUdmOurQ1Ws63Z4mMkp2oVrSUSWWKXJR0sLTPc+c5bdf5Gi6uclCe1u\nOWZBqcK88CZ2h89JaMcTiihz9ImHAQA33Ogo/pbU9OQqE11KinzRi0C3W6Y6xf2hHTtXeyvg7/be\n9WbKGRwIACFV7uNA3Ig2fChfci7s4TF2T83PsINtZMjR/+vSn5eEjn/ypHOtHto31XVM7RZLY74H\n0TBnV6HI3d/GnKg8Ja4/7N7HyxYRcRvNn3Ku+TDlbaVJyeyiXObzi+zerK2y60qT64Ykw9GSJBFv\nKfWpXbuce38QCAxQ7KDHhcufiU2arQgosSUfCrkt0NlOxB4YFgJWK3bP+socL0+YMe7b7aIrK0lS\n5VxL2qfo2rcjLuNlRTJpnWNCUuUMf5a0SpnE0Fj3c6rcu5HoKbflGqwWMgB0RBEnlqWsUaWBfKg0\njKWmI85cDIxJ0WnVM8UrAKgnon3cllAatYRRqfF+NXE/NxTBKBHd2pk57uMjJUe+u3KSx+WDV3D/\nzHdcG33pc18CANx3/0MAgIUVVxcjutdGZX1JhWxlmySvdIl7l9i6ln4sO0rCZiLVH0plG8LI11Nv\nuGvmpaLNjeneAvXw8PDw8NgGtiykkCRJF6nFzgCsFaZ1FLMwlNRmLTn/+7or36bsZ4P2b7nFkYL+\ntze+EQBQFIKMJhFZ/dUXv/j2bNvrX/96AMCHPvTBrnr2fu/FlohCel8CNs2BvgAIhCiKuyzQWIg4\nUZjr+gRcpolArJ/HH/1mVnbxncAZAAAgAElEQVTunBBWhDquuTe23cpiLb7kjpdnZXe8+A4AwL1f\n+gcAwMouJSRQFwKKSisxfZIJFiee4k/rFQBciIbdlg47i81mbalK1ob5eSdm0JawmpqIMnRboAaD\nau8wCDA8PIzqmgovEOswJ+Eb45PO49Fs88zZzmItwQsA0iZbpxSINdRwxzx2hHOLlmOh4JedVTs0\nwaFY82tCrw/d7Hx4kokYpWF3D1bOspUYkey34gLkLSFlROpcqTiixNoy7x9KLsiiqrt9LiprTFoa\n3enOF+e7NVsvFiERhqN8ZrEBQCImqNVsaCmropVwW5N04Jwqy1vLSX6XV7mCcyIg0pLcq+GEu44d\nIxyO1D7HfY7GHPmuKt6cZQnYB4Cwzvdtr4hXNMgdqyIhWk2pV0uNeU3xzq1ao0lZz3m5/qKUHco5\nz814VMz0xC8WaZqiVq1ieW1JbRSrOeseKmxIrD/h8aGtolhi8VStCdkMy67vXXXDLQCAsMPX+Lef\n/mxWds9Xvg4AqDZs5iat2c3HzCsr2Bhp0yZbuB3j3jGBEITcmKBCJe17RLwCoXpvBTbkSTwvQd5Z\n1o1KY9MjirdAPTw8PDw8toEt5wMNgqCvWEKQZRZQaxkyObQzjO7f2WDi9RlAWqL4f/gQS4n90A/+\nUFZmM8tbgQTt17Y0Z20J/OAPfj8A4OhRppzfd999WZm15tp91mE3WvtclzhEW7V999gmiKXqxsed\n1XPllRKQL9VqqhCDmliEE2OSu09lvA9Dm7tPaPLKBI3leygiFbsmp7KycyLjNisB+42W+2Eks8Vv\nPf5Yts1mwogjCcKHm9lV61yfkog67FLU9qJYBm1ZK2nUr8zKrFReZYnXsY4c+VZWNn16GoNq7zRN\n0axWnZgBgNIUrw8HEfezWHlYlm0AuNyLhk6WKbPdHRL2UVJr1U9+6zgAIJTA871XuGuty4z9nEj/\nNSNlwcha0cKau97I8gXAZUNq/S6R9Ssbza9DVcriwYlkfUh7chZXbGYWvof71LrnyrLL2jIIEAi5\nMAIpOUZrVdhMJlX1XFYk9MNaHM1E5aeUpuoYG0Li+n9L1tandnLfbg6ptUnxLozu5utsl5T4hxy0\nPOXGgY7khE3lfo+rusfyvSFDa0NZyLamqYTJ1FXcUySFUxJutluJoCQdDMrJAgoCxKUixnJqm+S1\nNbHNeqTCuEQCNJXxfbXisrhY0Ycx8WpdObYjK9spgi2f+iRnxrrni9/IyqoNvti8DV1TF5fI899U\nIhO5PNcnDvgZShRnwObPtWO/XUsGHH8B1sJV/T8vC6o5CSGLc847k9LmG9xboB4eHh4eHtuAf4F6\neHh4eHhsA1tUImKyjybfpKnNwmK1cHUWDZs5JZKyQP1OdhGKunafHjzIRIo3/diPAQBuu80lzbaw\nLmOdqSRTNeo4t86+faz5+eM//uMAnOsXAJ44crSrfv1IRRtty5zWOpNMOiBfi5wnimO87GV3Ztte\n8YqXy3m4vapKt9VuGxliks6rXvUqdSwusy7S5SWnpzo5xW4tq9LUVFq4a5JpYv9Bdp0XVXLluhwr\nUsotJcl6YbNL5IqKICNkGatlWVPJohdX2R1qCVN55fKMRK/1qmskAW/qXHPz83NotwdD8e+025if\nOYdYufB2H2C1oVFx5U4fP5GVtes9icEVEc4qo5RHLYnIXavVKG5Uud5PHD3pDjHFBKO9Q6yOs6Sy\nn8wf53vWarrHdtJmiRFFlSivVHFidqO15R4uzDviyIiE4OzZzdc3Pe3CXypC1srLvcypxM/HTxzH\nIGEApJ0ERXL1zlltVsnuYdTyjs3CVA+57WtKh7kt7lwj7rdUJ14v8DWU5Fpydddnzp7g9t85yv1r\ndcEtU7TLHFa0f9K5sVdX+ZynZB2kErh7Ww0lA47cN50BpCyuzikhyOjlkLzoPO8ocH8hJfK7kqZZ\nSM/FIgxDjIyMoTS8O9uWs1lLhOi2PO/a7VtHOFTn0DW8FLF3vyMFUluIOC1RhVpw7v1P/e1nAABP\nPM5t22xrNSmpiywRxMqV3RINYGPc/lb322pnJ2qppCVjfSBjUKCOlVqZqo5N+K2WEOUQVpdYq0mZ\nLZiV3gL18PDw8PDYBrYspJAkSZdYgoW1Lru0cNG9rd/vduzghec77nhJtu3OO9nieuEL2fLUmUOs\nxdVo2EVfZ6n0I/7Y3JO33MK06re85S1Z2R9/8EMAgMceY1KKJuToUB3AZV4BVB7QPtfF5x5cPtBC\nPo8vf/nL2bZjxzj/4ytfydblyZPOctizh2eVhw5dAwA4KCQsACiVmfwSRWKlK4KRJSnZazaKFNGS\nGdqcaKx+7f4HsrIHHmby0PKq085sS5faIbqru4ecxVoTmvsTYu0cO+Py+lUW2TqymR0OHDiQldlw\nhCuvPszHWXSiBLkoHqiQhTFOsxRw+sC79jHhSYdbWbJapcLhHroe4xKqY/M7zswez8pSGzQvISFp\n6vpavcJlp4+xtbmiCB1FYutr/05H8srNPQUAaIlVtLjoSB6jE2w1tSS0oqa8FTkJQbAeoKro7AKu\nH1iRjoV557WprLljDATGAO0OlMGBlmgeW4JQByqkQcK2bA7bNFR9VcQBUiFNTU268Bv73YgQQCnn\nbIc5EbtYlGxBdSUWMHlIsnw0HcmkmXB7jk1y+5woujosyhhSkzbXQT9jEpqSl6GkRI6QFolASlP6\n16Lyoq2FhPaAunjSSbC8sIaayr9qQ9ssqef4tAvZsYS30TEReCg5vdulM/ycnHqcn92ls86TsrYi\n3g5LUlMhRTbbS8d6D7pyLcunUd4dIQ1FAbdmokJV7PhkQ/fCyKifcd/OS1mgxjyrqdwSMlpOEU93\n7tuHKPcENgNvgXp4eHh4eGwDW7RAUzSbrS6LUK9rAm7tC3BZTqwVaNcjAeDbvu3bAACvfe13AACu\nuspR+QuSIb0pMwadOWBF1spqdZ4xp8bNMi21vaWktXrDUOx5AeCWW9nC/fSn2V//F3/5l1nZE48/\nDsBZFV0hODZPopYffBoQBiHK5XJXiM/Ro7xGNjXJUm96TdcKT+zezaIJO3fvysqmIrYyJ0ps4enA\n+TUJmLfXWFDrlp8XKcS/+qu/AgAcOHAwK7MKb021lvnEIyz1d/optpR3KOu83LBcfb5n1yjBiz23\nswdi1y6u8+49LsRlz362rJ93DVvUle9+fVZ28Lrr8Qd/8B4MCinSLEAcABqyHjg3w1ZvZdWtHdsQ\nEiseoqXyymK9WQv23MzprGxtme/ZmAhJXHuDW+OPZcZ+dprvSTLs2mFkN58n6ZzJtlkZwLy1borO\nshiS49usKqMjLrdoQUI3FjLBCiXlJ16DYfn9woKzSAYVTqEP1yago6TeLI2CxFuSqmevZc8v3SoI\nnRhBKDk+G2LRRKpsuMzXnstZvoO73olRtuhrEiY1rrwmk7IeHiiLsCTW4KKwIKpKSKQlQ2MoISET\nJWd5lUQ4odOQAP/I9TNr/6/JeLNWcHVvmxTJgIQU2h2DmfkGQiUWAsmmVIiEx6Ks7SvHZJ1zjfvQ\nkQfms7KzJ9kbMXNiTX7vjhkLlyGMbGiMO10ntSExMo4qGUl7f6DaOxXvytCwyCoq7oHNIBOLJyKn\nnkHrbYuH5flU6+xnT4oco8gRju9wY+UVN96IfP4ebAbeAvXw8PDw8NgG/AvUw8PDw8NjG9hyQu04\nF3YRKSL5btV8NPnm6muuBuAyqLziFa/Iyq679joAwIiQRrQ7dF5cRsefYnflvj17XSVSSw/vSJ3c\n+ay7NafUW+yqdKXKboBi2bmxdu1ks/3uu+8GANx6661Z2ac/9SkAwCfl88Tx4+hFb8LwQSMMQ4yN\njeGqq67KtlkXriUP6VOfOsWuvdIQk6JOnXVpeMfGuZ3HRvn6S7HW15VMI+LWtW5yAPj1X/91AE51\nZ3zCkQiefxMn1J5QGXHWhtlldaTObp0nlpyL+XpJuBuJe2anIsO8+HZ251oX7qQigIyLNqllmpdG\n3T285SUvRvEDzuV2MUjSBKuVCoaVy9Bm+1mUbB7nFPEpJy65w4cPAwCuvNItQ9j7syruU53IfVVc\n5jlRnSmrkJ2lNSZaNSUhcTTq6rcqQqSxUlQJYmmblN1UO/bsz8psFqN0mYlFHaXgkh+RjCurXJeW\nCgW64gp2la+u8D2sVFyoiK7rINAhYCZnUFTEtaKEgAyLRiup7Bt1cdnZ8AOdUd3I70Jpn0bDXVNT\nwojGRBc4r5IrT41wvzx3ijMV7dzjQlYi0YeuqQTP9RbXdW5JtJnbbuwaFvd/WfRxx1QCeivuW5Hx\naVXFS9Tle10UchoqG0uSpkgHZOskCbBSMUiXnZs2TzJ2S4aacse121CBvy9Nc79/8pTTuzWGrzEi\nm+heES0tGSgLc3RLCxUhsy1LGFdDJR0vi+pQqeiuP5X2LYlC2BWHDmdlkzv43k1M8fim8t0DUq+y\nhM9VFl3dz8n3qqicGVW2P0myUKgLwVugHh4eHh4e28AWSUQGaZIo3VsglFf+lUICes2rX52VvfrV\nrwUAXH01W6KhCrhfWeFZ8TcfehAA8OhjLs/kfV/9KgBgcY4tqFe+3FmucZ5nhMUhnkkmKtDczhrO\nnXGEjdOnTsh5+PilEWdB3fYitnr27WUL94brr8/Krhar7+UvZ+GCj370o1mZzTE6O+vCKSyYiDMY\nzjkFhDjOZXq0gCOCHDvKIhBlldGkJov/E0tMurGZIQBgWaj6NquIttLHxtjMGR3lz6/d95WsrCX6\ntbvEIpzc6QKwjx1lotVNtzjL/eobnwcAGJrk8KRHH34wKxsSDdz/5Q3fCwC47aUvysomJOylKvqt\nx44dzcoWxPqdESLOvfd+yV1zq425PvdhO0iNQbPdwnjOETgssWrhLHtFdGB8W8hqNoxFZ82xs/G8\nzJpHVL+zggZ5aY9FpS9rrb2mWCDlvc6iHBvn56jTcmQ8tHk2Pyyz7KDqstjML/B3awXnc85ytZqj\nq6s8824o8QwbztWW3JRDZdfHhgdsgbYImI6BMePqtkMsyIIY7UnbWSiJEEOsuEOoQoc6VvpUBCAS\nJWpiLe2pvUzKGptwpJGieGCiPD9b4zuVaEJVdJiXHXHr9AIf69wCt32u4c4zImSfMfEMkApDWpXw\nnHkJ9l9WITgpbAiZCJEkSpQjDQalrg0yQK5NWFtyoWeQvjBf5+s5cs6Nn1ZxIM6X5U+V/UYymIQ5\n/tRiDyYRER0SHdvQeYlWwP2rOCyZcQqKhLXCZaHyOlx7K48pt387k+3KZacTbMR6NiKs0VCE0468\nGzrg55lKzp1z3U08ZtVrnBlmcdZZoMefPNElJrMRvAXq4eHh4eGxDfgXqIeHh4eHxzawJRduQIRc\nFOPGG27Mtt11110AgNtvZ3fcocNO/aYt+rgPPPBA16f+flRckdaVBACpuA2GCmz+z551ajsjo+wK\nu/1Ojuc8cFi5uMbYtP/sZz6TbfvMpz8BAJhfYpfxmXOO1LJDSESWPGTJTgBw0003AXAKRppg9LnP\nfQ4A8Od//ucAgK98xbk8K5VK12L6xSAKI0xMjOOGG5xr+eiRIwCARoPdLVO7XAqhVOKcrK6jVZkB\ngLqorNgYWeWFz9ymq5Jw+anjTlHkBc9/PgCgI+7hmtLvnJlhYsHxE04f0yZ7NqIWUh52BKNJcUe+\n4EXcV+69x8Va3fMl/m7VQ2691bl3p8SltrTCrrPHHncqIaPj430VrraLxKQYHnYkpdNCLJmfYxcu\nae1l+W6XI+bmXIxcuycWcedOR4QjcSnFQmR54sjjWZn1rA+Psisqv+Z+F9X5e63p3ILDZY7L3SeJ\nzp+872+yMpsQ2/r/xlVyc+s9tHF3ReWaTST1l23XffvcM1apuDjYQSAhwkqUQ1stxaRCKDJWF1m5\nBkliFgPROc2rZaFYtG+rqXXrufOUZZkiljRuoUoXZvOgDY3xD1Yqzr15/AST9s4q8lhNFI+smlOh\n4+o3KsStnNzjJeWKnbM6vrFNGO6INZYTZdO6xcr9HJIZmAs3jgl7pnLYpVzYCzImHpvmvr4w6whG\nRtzOY+N8PWOT7tmIh/l68tJpI7WEMSRLbQsz/EycOu1c4BN7+dwvvoXHlvk5N/Z/9R/uBwCEBaWh\nLc9/Tey9VtXVL5ezbcn9IlRJyq0uec0mYVd8rgPXHQYAVCs8jn7l824Mr9Urm47x9xaoh4eHh4fH\nNrAlCzSfz+OqK67AL7/tl7JtNtzgoYe+CQD4yIc/kpU9+i2eWZ86zYvSy8tupmEtIRsKUiw6VY5E\nJG4yGnrHWRhLyzyrLg4/AgC48qprsrKcJNe+5957s22WYFIXIoKmJ58+w7OiU2JlfOITn8jK9u3j\n2f4tQpDRITjXXMPn/LVf+zUAwBGxCgHgfe97Hz75SXeciwERIZfLdVk2jz3G+rPHJawmjF27Te3m\nOi8t8P5TUy5MZFSybwyJJqVqUjSlna3OaVORj3ZPiiqNaGA+edLNJCEWriZtlYYk5EGIIB0VvmFn\nax/72McAAO/9g/+sjsUfN1z/fKmL02admeFzHhFi0YpSA9p/+AoEKqzqYkBEyBeLmboJAJw4wSQ0\nqz6ls/9EMtuNRf2kpvRkZ2eZwNMRIs6efU7ByWa2eeSRh3jfOaf0E4jEjhVwGTMupKg1zc/Y7Iw7\nT2eIH+HDV0tGnaojP7RbXOfdYkESnMXTEEtyRFR3yiPOOq2KJ6Jek3Yed2XWOh0UjAHSJEBDWZKL\n8r0hFsSUMr/GhMRC8jzrhCsk5JyAuKyjPEGHr2ZS4OGr2ENWr7rQnKeeYo/LwhxbmXOzzuO1KESs\ntlY3C9iUKYn1EynlqkLI36tiIS+oIXYhFqtXridv3DFJNFwTsZ4TVfeAkixh9MUiF4fYv38UsbLA\ny5Jh6eGH+PmqtN3zVJIwnKIQyXIq8XpBSIcTohM9NOTIZkV5NqbnPg8A2HnVoazszm9nYub4TklW\n//ixrIwklCYuu/N0pGs8dZrvC6k+OCwa3zsknEXfi1RCZ1Lp97WGe27CQEheQqwrjzkPzA03PQ+f\n/YIjP24Eb4F6eHh4eHhsA1uyQDudBMtLS3jfe9+bbZuZ4RnaGbHm7FobALRk7SKXaYW601nLxM6r\nUhVIbancddFYbSRuphbGPDM5coSzUJw57UI8pibHpMxZhCsSyG/XB9tqAk1iJVkLRgf72qwnVrjg\n7/7u77IyG2hu85TqvJsve9nLcM89X8QgEEYspGD1VHUdJyfY8l9adhbKzAJ//8bXmZpdUnq34xO8\nFjkl2W927nJraxMijmAtqnbH3cOlJclw0ZZMIAfc75oSfP/4Ey7kJJYZq2W7h+qe5yT7yGnxSCSp\n0heVcI+T09zeH/hjNyttCL2+KX1mcrfSyd13sCt85GIQRhFGxsdwZsb1qRXRAI2seIGyDApS55xo\nQ8+cc9Z5ZdVmReF+vbzs1t7HJ3jmnpM8h5OTzlOwuMj7NRp8n5sr7ndrZ9lCWj3jQlVykqc0nWJP\nyVDinqM5yaNaHuHzzc45TwGJRW01YouqDZfq3N+s1oBJXGB9sbClIeOCCGBQpBZAbu3KXkJNFmpn\n1QpgW4RTdsvYUlJ5VhMZTUjuS14ds77IoUJPNtgLdkoJo5w+xt8Xl7gsUjq0BeFh6NyvVv8gFe7A\nmBpG2yIusBCxhVNRmWRCuY5APGxGCSm4NMqS11Lp64amARrQKmhqDKrNFkjxBio1tswaco2xUiMY\nGZZ1aNEZHiq4Ou8WbfOGWNZrdfdsnJ7mZ2FuiZ/dO+58XlaWK7K1t7zCZSsqN3Eg4/T4hLNm85Lt\nptrh/fTzXpUwrsZZ/hwbcuFi5QKPeXWbLUmts59b4mfo4YdYdGbXAWchU1zs0h/fCN4C9fDw8PDw\n2Ab8C9TDw8PDw2Mb2JI/Jk0SVFZW8MXP/0O2zeptRqIcYV1xgEsca12kmpzREaKAVTsxyqVgtXY7\nosaSKPdupgwjerlLS869ad3JlbpSo5BLtOfRruKN1uXteazObVUlI37gAaZa33//fQCAD3/4w1lZ\nsVjoSjF2MQjDCOPjE5lCEAAclETTraYk/1WKGYuitjInrsTZGUe9n53l9jpxnF2jLeWutqmHxiSx\ntk4JdIMksT54+KD8ztXvuKQsW1PksAlJ4pxF0ChyyPTJ4wCAM6f4k1QKrWqNXaU2JZtmkZdEBeWl\nL30pAOCFd9yZle3aux+f+RvFT78IEBGiXIxTR5yrMxW2FQmhJVTqMSOiydsQgsnZs4pgJbDLFqWG\nc4uvirJSWxJ371bKN1eKzmciSYdnZp27ttlsSF3csYZK7JKfO8dEuBHV3rskNKUj52mrtFGjw7zc\nUR7ia6jX3DGtrrUt06E7+ci5NwcC4lAWrScdRN3Pf1upDaXE7VKQDpI3rj6WWESppJlTiZfnJIn7\n8hq3pyUMAUBzlUlT+YCfg2LBPW9tUTVSEWEwyzwWBKKFSypt9rJoEi9J8ueaSqEVS9iKJTJ2VCyZ\nbeHQpudSLsQA0cBcuO12grOzqygVHYno7ClZNljjfmLJmAAQ5mUZboivZ+9Bl+h+cgcvPbTkWitr\nSu+WuG0CCd355je/lZXVmnwPc0Vu74UFp8RlZFlnasKFyxTzcj+lzlqLvSYhLTal4krijrUq4WWh\ntG6cut899SgvO9m83QVFgDq3vIT2JkPjvAXq4eHh4eGxDWyNEUAsplBSCZd7M5IkyrIJJYg8FfJH\nvqR+J9TpJZkldFQCVUtwsAm5m0p/tN60Wok8mziniBtjEqrRVjEadt3YZHMFlTHA2CwCG171OsRZ\nsC7/cGXFWWBLS2lXQu+LQRAEKJXKXeSmggQYpylbFyOK0j0hZJTD+2VxX1s9oodqrWOtr2st9yWx\njObXXCD5XiGp3P8Ah1xojcqSUOFXV57MtlnRC7sIr9btcUTCT6oSQqEMi+y4Njl1rMJzJibYyhoZ\nZeIUqSDztNnsIvZcDCgg5AoF7Nrj9H6XZXZcWeM6x6Gr1w7JHDM7PyefzhuSy/oIX+SOSSd4sSTH\nXJH2zinq/XVXc4hUTSzCJ590bVupseVjMxgBQCC6vY99i7Web1eEpAOSeaQiddCJz4Nxbsu2tKUO\nDcoX+NksSSYSm5Cbz63SwwwAJgXaLUKY1yQi7gupELciZVW35esZIbB1lEG8IxURA7lMaqkQKukj\nBSHWFcsqg48keA7Fcg2VBZZKmEPgYvdhVnkMykli7MXIHWtRsrA0xfNgtFavPAxWM1Z75HJi4haE\nTaTt/E44KHVtwBhCuxNiadn1hYVFIeJItp5Qharkx7k/HXo+C8vUlGW29OQ5OSb/vapCqOwjOipe\nrenp41lZaYj71b5DHF7VSdzzGwkhz+pzA0BkM9SIty0XKe3hiMf8Zk5Id01HeKOA71OpzN6WmeMu\nXGxxjvv05Bg/lx1FaDRxsOkG9xaoh4eHh4fHNrA1C9QYpGnaZRHZ9U070dJrGVYIwcqaNTqaCs4/\nsC96nS8RZNfBeGaiw1+stN63vsU+9elpF/T8Uz/1kwCctQUAH/rQhwC4jBn96Mm2zv3yeto6EOkw\nGztjWr8/EXXtezEIREih+9wye5VZsy4Lshk7f1prDgDKIqBgQyYOHHCB/XZ9165FzM7ocAy2rvfL\n/tpaekDCZZot5yE4I6IKseQbJZW7MpL1szGRlIsVHb0olkFBvBvW+gGA4eHu/TsqFmmQM8B6rY5v\nfPMbXWIJe3ezNWpDlxoqTMsup58UIQ4t4ViXNhmXLDOJWtQ9qjLNAO6+AcCy5O60WnuFIdcO1kor\nlZzFc3qWrYAzxzms68U7XB7V4SFuyzkJhYmVJbcmog+zq7Kep4z4Ycnp2pI1xboKFSmWBiukEBpg\nrG1glMiDlWCDWNeBCkdpSDuuiqVm1DpiWSLuiw3ep6V4C23ruRLrsq28GHZNLsr4G4qPsSThdqdd\nuFhQ4/JmwPWaV16JSsDfs2gio9tLxji5DVHqzpOXHJxFm1FGecpaHcBgMF6WTppgcW0NK6tOVODs\nCovNpEU+x+RBJ/O34xCHd8yKh2J5cSUrs+N7XsYEm+EHcGNQsWCzsbgxfPYcW4IjI2ydBqlrv+Eh\n7r+tluurnSZ/J2nvdl15oET6syk5RcPIHavZ4vu/WuF7f+Ko8xDt33ct12GU+3o7du+fhml0SZ1u\nBG+Benh4eHh4bAP+Berh4eHh4bENbFlWxBjTTWoR09y6OnWZdcVYN2NXmZjImVKNcq3asACr5foD\nP/ADWdndd98NwGVEufdel9HD6vK+9a1vzbbtEOWd97///QCAM2cdfR3m/Ha6dZXacJZAZ+EQanqa\nKYqYdb8bCIgQhmFXthF7Ttum2oVrv4d9tGFt/W2Z3se6eq0m5i6V4WVFlI5OnmS90FPKZT47xy7f\n8rDTSi0K1dwSXfLK3WjdspaEVlRktKItE6JZIa80N4WsNCxhFZpEUygWEWxSNeRCaLVbmD5zWqnC\nAHMS/vOKOzlTz8233JyVfV7CuepC1upa2pD2HhUd2dMzLhzl9Dnug1ZRxfZRAGiJq+7YExxutLzi\nXGZ33HEHgG492oeEPFQXtaaWUtFZEIWZFXHDD4+4exFLeMYpCQebGHWuX0t8WVzi+2uJHUA3KWwQ\niCnA/jiPtnJRZu5uIbXUVf9vWDe5hLo0tEqRdAOSG9huOUJJVXR986Kso8NCrCd1WYhbtZrSgl7j\nax9ZcfuvyLA5K3XQ+Wmslm3eLlEpF64N0bCKSbnUlQ0l4mKWZa6WJh+lwdaZjudBp93BzNk5pMpn\nPzzOz9fUJOsF797tiGhxUTTLZcDeM+T6kOnw97Eh7uNp29VxWbJfWQLanp2uj8/Osiv10QeZmLh3\nt0sQb5O3a2JRsybu7ZIo1Kn7c/YMH6spyl179jj3s+WSnpMQr1jZi6vikm6Kyt3Ova7/58IwU0S6\nELwF6uHh4eHhsQ1syYSOF5EAACAASURBVAJN0hSVeq0rTKNYkNlAxiJy+9vF5ZyQAdqKSGEzJ9i1\nfG1EBDJ7e+1rXwMAeN3rvjMre/xxJg8lkuPtlpudRfCxj34UAHD4CpeT9K7vukuOySf4vXf/XlZm\nA8bt5C5Vi/rW6rMWZTcxyF5kt9hC7/eLhpC2dG66TscSrGzd1YJ6mnZt07+z3+119ct3Z8laBirD\ni+RMHRYyzD5FPrpZsrboECQrqBHJPdfZEfJiVebzbAVoQlIs57ZEoVgRx6wOrQ0NiVXOvyAIuuNh\nLgoEEHVRwypCtjl5ii3viUmX39QyDSamePZqQ10AR66zIQvHTzvL3VqskVhY48riPycW4ckzPGvW\nAgenRahBe0PstU9K6M2qKmpWZZYt5I7h2GWcKOZ4x5EyWx8tNeM/fZaJSTkhguWUNuq5WRcKMAgE\nQYBcroww1d4p62XhvyPtgZG7U5GhK9TkGvldO+D+GBZdMH6zLX21JkH86ppiGzazzO2lpLfRanGf\nTTvOIlmVVDmrMmiRcf2/LBanFZEJFSGpk1j9by4sJe53Qx2unxU4aZLr/ynMwMaVpJOguriK3JC7\n/vFJ7hfilECxpMJ4pE0Tm82q7cryErLTlj46NzfryiTcbkRIcG0l+DIumX9CIbWRuvfW+9FqO++B\nEaJbp23HJ1f3yV0sQhJIe+mnNxIvwxVXcD3ra+5Zml/i53qHiJjklEBIEJpNe7W8Berh4eHh4bEN\nbFlIwQSEWOXu7MjMyFohoXpz5ySrShjyjCHps87Rlt/pGVZB6Pr338dZwr/+wH1Zmc3daQOiv/3b\nX5mVfe6zfw+gW3jh2muZrmzXm3Rev5ZYAFaqTFtlvZZaPyvzfOEvg5otpsag2Wx0WfzOMk7X1dl+\n77U2dZ02KsvWfUnPQHmbXdssqvCSVI6R9LFmQ5vpRs3sbBiLtS51pha7JhvLPoEKuQh6vAA6zGRQ\ntqc+njZo89LPbKaabz3xRFZ29BivU9o1zOtvuDErs7Prx2X/hRUXBmF7R1naMiq4GfUxCUdZkLXn\nosrb2JDQGPsMAEBVLOQbb78dAFBT96IlJ4pkHTqvwoYWZ3kdNhbDKlJrzgvzbEns2sVZb9pt1f86\ngw1j6SDAYlRCQMo7JWu8tgvkyJ1/R8rf84n9vZIAlL7ayYlowoi7JqviZsUrCircYSjPbVywso11\nlalE+uWKykJTl3AK27PzqRtvCuLFKltxBi1fKt9tiMqwuuaS1N0G4C2qIcT0CZfbLkxq0Kx3ujLO\nNGvcpolYy8MF56lI5Doa0iYtJZM6VOJjGKEyDI05LsTYKK9llsULkIsd92Q+5HXLSfHm2LzMALBW\nYSuxqsbwQLqcXRceHlNCDzl5hiSzTUPl5B2VcKyy9O2zHVeHqyZZkvCqK1m4RDkD0GzUkY+1lMX5\n4S1QDw8PDw+PbcC/QD08PDw8PLaBLblwAxCKUdxFqLGhKinZcA9F/xXPg3WpJkqpxVK5rQKRDquw\nxz85fbJrH11mt33mM5/JyqoVJVgpePDBB7vqQkpiwiXzbsu1aJ3cbrdm2kWAor77DBomTdFo1Luu\nv9dNu5ELd7PXs9F1uHttuj4AFwoQKEeq3d+6aQPV3iF1u7C0eyuy7tlMiUWdR1y2WeiOcuEGQTAw\nNy4RL0Ho49mQmRFxSR1XiZiboog0fZpdUDtVou+DB232Gt5nYqcLDViVZNtDw+xaXVPKWTNCFKoL\nQWti0mVqaUg/WFhwiiptceuuiYs4rjnX08Q46xgHlvSilHmaoqg0WmY32ppSG8pZ36kQYnTYwPiY\nI+YMAqkxqLRThEpxybpns34cuud6QtSGrJO1majMSzZkZIRddwWV0cMuRbTlOptVd8xSyufuiLat\ndVsCQMW2nVpSsISrojwvkVI1alnNXSG1FFRGmFSSZFvVoQkV4hLLM9GS8ImCes5G0g4WBuXGDQiU\ny6Ft3NAvXlOsLUjoybIbb4oSVmbDPtptN26MjEiC+wluo1zOHbO9IqFd4uiOSm754ODV/GxYHlc7\nds9GUTK6JCpzCgIbsiT1XXKhXSNlGc9EkSinYtDyHa5PRZKo77vakUtLkgkmKgpB0TiXfrVZ23Rr\newvUw8PDw8NjG9i6kEJq0FG0Y0vosMSTVJFTyFL5ZWG9i2oikzwbkqAtoi5d3B5YCr8l1qysuNm7\nDX7VYQ691qJilW+ko7BOqKCfvm0/a26QSNIElUqlK0B/IwJTb3361Wuj3/XDZq6/X1lvGFD/Ywd9\nvwO9whW0bltWhn6KxNsDEaksKoxdknGlJhbbqgpVyYkAhb0/FWXhnTzJYSsLiyxGMKE0al94660A\ngIbQ/0+dOJmV1WtsGVkhiQmVlWJWrNPqqpuBWyJGIB17ecllBhoRCzcfF6V+LjRgqCShBDk+z9qc\ny85jiWKBtLvOhhJuklyxWRCAfJqirTWTxfqw19RShJ9ULLtYBpBEDWFN2xGs1vKQa7umWH2x5Kdc\nXHZWfKvFx6qlOTmfyg4iFmWUujZoSuYUCmQwUTlJq9l4KLrUiXs26iJeENvwHJVk1Ib1kYQOldSz\nm0sTuB5ycSAKEBeKiPPFrm0AgJDH3WbHXWtNLMkV+TRq0KzUuR8urbblOMraFtGIYbE8i0V3zFER\n9AhCGdfUY50KiXBxyZGBrJ62DUXSD/zKiujxSrsVlBW8sMbPws6d/IzMrLo8zckivz9GpY+cOeWE\nTpYWKqjU1nsz+8FboB4eHh4eHtvAloUU1hq1TIVfw65h6swpRZkV2JCGMKdmr5Zy3llvxVkLdKM1\nOXueWIVJJMl6OUGLfpZXJ+0OVdEWjr0eu61fXc63bWBBz0mC5eXlLVu4/a6110rc6jE3kg7sZy3a\n4+u17fUWshaBMF37Bz3rnLqsK9woCAa+Bl0ur5cftKEj2jtizzs0ZDPdOCvz6JEjAIAVm+9WeW1u\nFfGPhuQ8XRlyIS6prKuWizJLV79blryjZZU786AIKFhBBEOufkmHZ9GtutwLFZwf5yUfr6xtlYbc\neqHNjFOVDEb7VH7U1oByr1oQEXL5PIzK1kE9j68OjbPhQYnkiEzUGmNH+sl8XaRD55xHwIg6wuo8\nt0m9rrIY2TAJyf2ZqswwNqAjVhZyUc6Tt3wMo/q/rPlbycOqCgFqpDbnJ/9d01mW5Borco9ayl8X\nBmZgGZ4AAqUBKiobS1Hyf44Ncx9K1fjZkq95yThTV2EsVuSjusqfqWIPWLGUNbteqdd7I7HSQ77G\nXMF5GAzx75ot5eWU+wIJ+8mFLoylInKNJck2VVO5XCHhTzYzEin346h4ZxYXuR3OnXYCIWRySNLN\njSneAvXw8PDw8NgG/AvUw8PDw8NjG9iyEhHIQK2noylkHiOeikJOqX/YxAnibo3U6m8q7q9W5q5V\nSWzFE5CFrCiXgtOCtQQW7T5ZPx+w+rbWvavdf/Vmo2tbFDk3zfrsJa6prDJI5jpMuwk5g3IpGmPQ\nare7iFm9R+7n2NkMKUjvQb1lZn2pDUfp58LVrli7n223zSo4JZkW64XJSuvCqAbU3kEQYGhoKMsC\nBABrQhpaWFjI9umt1+goExGqikS0LOQ2qw2s67gqSbPPSvhLZdURk54nakaRtOOZU05Dd0pCSKYm\nnB7vHtHytAKuoyNORWaozESRs2fYPRXHjjhixPW5sMIurJ27XLhMRUhKNrxmv3Lhmo5i4Q0CAYFy\ncZeblqz7Ulx2+T4/q4uPtKLcpwnx9VVF73b2qHPLWW6JkXCHdluRlqxLUJaDQuPcrvboWm0rkL5K\nkYTGaHdrpo/N2xpaC9fq3Irrckl1246QcxbkUHXlKc+RwaBavdNqY+7MLMhFlSCGdd9b3VsV4iaD\nfSxLblo1qy7tZLPX6AwqOXF5V9b4mYgCt+xgc2Vnz3+g1eHYxZ7Lu/F2dNKqoPF9Mk31/MuKxfIs\nE4YaLUf+CWJxmdslD5XF6IzUwSp9tWqu7sV8CWmyuaUKb4F6eHh4eHhsA7QVa4mI5gCcePqqsyEO\nA2gBOHOB/S4HHDLG7LjwbhvjGWzvPICr5PM0gNmNd7/scLm39wsAHEd36shnOy73Nt8M9oL7/FPn\nKX8egJO4PO7bc6G9Lwa3AXgYQPNCOw4Im2rvLb1ALyWI6A8BnDLGvP1S1+W5BiJ6H4BVY8zPXeq6\nPBdBRMcBvNkY8+lLXRcPByJ6B4CrjTE/cqnr8lzA09nPiQNlrzHGHB30sS8G3oXrAQCHADzSr4Bo\nk6nZPZ5WENGWRU88PC4XPFf772X7AiWiW4noASJaI6I/BVBQZW8hoqNEtEhEHyWivarsO4nocSJa\nIaL/l4j+gYjefEku4lkAIvosgFcBeA8RVYjog0T0X4jo40RUBfAqIholovcT0RwRnSCit5Owt4go\nJKLfJqJ5InqKiP4FEZnn6gNzEbiFiL4p/fJPiTgb8QX6siGitxLREQBHiPE7RDRLRKtE9BARPV/2\nzRPR/0NEJ4lohoh+n4iK56nLPzoQ0S8R0WkZTx4notdIUU769hoRPUJEL1K/OU5Er5Xv7yCiD8u9\nW5Ox6eZLcjGXIYjojwAcBPAxGUfeJv33J4joJIDPEtEriehUz+90G4dE9KtEdEza+H4iOtDnXC8n\nomkieuUzcW0bwrJGL6d/AHJgP/3PgdPu3Q2gDeCdAF4NYB7AC8HrF78H4PPyuylwSr03ghnGPyO/\ne/OlvqbL+R+Av7dtBOAPAawAeBl4glUA8H4Afw1gGLwW/QSAn5D9fxLAowD2AxgH8GkwyTe61Nd1\nufwDr39+FbzmNgHgMWm38/Zl+Z0B8Cn5TRHA6wDcD2AMTFe+AcAe2fd3AHxU9h0G8DEA77rU1345\n/ANwHYBpAHvl78PgNf93gDXK7wIQAngXgC/33LfXyvd3yFhyt4xJvwBeO40v9fVdLv962uuw9N/3\nAyhL/30leBnufL/5RQAPyf0iADcDmJQyA+BqAK+Xe/niS329xpjL1gK9A9xJ/5Mxpm2M+TAAm1X7\nhwH8d2PMA8aYJoBfAfBSIjoMfhAeMcb8hTGmA+DdAM6tO7rHhfDXxpgvGebitwH8AIBfMcasGWOO\nA/htAD8q+/5TAL9rjDlljFkC8JuXpMaXP95tjDljjFkEv9xuwcZ92eJdxphFY0wdfC+GAVwP5i88\nZow5SxzX838A+DnZdw3AvwPfNw+Oz8gDuJGIYmPMcWPMMSn7ojHm44bj6P4IPGifD/cbYz5sjGkD\n+I/gyeUdT2vNn/14hzGmKv33QngzgLcbYx43jG8YYxZU+fcB+AMA32WM+erTUtst4nJ9ge4FcNrI\n1ENwQpVlLDJjTAXAAoB9UjatygyALpeBx6Ywrb5PgSczmrl3AtzeQE+b93z3cNATuRpYJW6jvmyh\n+/NnAbwHwH8GMEtE/5WIRgDsAFACcD8RLRPRMoC/k+3/6GGYePKzYCtyloj+RLnKe+9LYYPlB30v\nUvDYsvc8+3owtjIeHABwbIPynwXwZ8aYhy+uSoPD5foCPQtgH1GXAORB+TwDJr0AAIioDGASHH5x\nFuxKtGWk//bYNPTEZR5s+RxS2w6C2xvoaXPwQ+CxOWzUly26aPLGmHcbY24DcCOAa8Fur3kAdQDP\nM8aMyb9RY8wQPAAAxpgPGmNeDm5vA+Dfb+MwWd8WDsB+PDvC6p4p9Avp0Nuq4IkegIygqCd502DX\n+vnwfQDeQEQ/czGVHCQu1xfovQA6AH6aiGIieiOAF0vZhwD8OBHdQkR5sKvqK+Ja/BsALyCiN8gs\n8q0Adq8/vMdmIa6tPwPwG0Q0TESHAPwrAB+QXf4MwM8Q0T4iGgPwS5eoqs9GbNSX14GIbieilxBR\nDB6MGgBSsYbeC+B3iGin7LuPiF73jFzFZQ4iuo6IXi1t3ABPNrajin8bEb1RxpafBcckfnmAVX22\nYwbAlRuUPwG28L9b+vDb0S009d8A/FsiukYIczcR0aQqPwPgNeDx5qcGXfnt4LJ8gRpjWmAi0JsA\nLAL4fgB/IWWfBvB/AfgI2Pq5CrLWY4yZB89SfgvsCrsRwNfwzAXfPlfxL8ED9pMAvgjggwD+u5S9\nF8AnAXwTwNcBfBw8+Rmw5ttzDxv15fNgBNzeS2DX7wKA/yBlvwTgKIAvE9EqmMx13dNT82cd8uC1\n+Xmwy3YneL15q/hr8Fi0BOYAvFHWQz0Y7wLwdllCuLu30BizAuCfg1+Up8Fjil5i+4/gCfknwWTQ\n94HJR/oYJ8Ev0V+myyC64lkjpLAdiJvlFIAfNsZ87lLX5x8DiOi7APy+MebQBXf28HiWgLzogkcf\nXJYW6MWAiF5HRGPirvlVMB3au1meJhBRkYjuIqKIiPYB+L8B/OWlrpeHh4fH043n3AsUwEvBTK55\nAN8L4A2bpFB7bA8E4NfAbq2vg2Mc/80lrZGHh4fHM4DntAvXw8PDw8Pj6cJz0QL18PDw8PB42rEl\nvdI4F5lCKQ8KVEJTSYSddIQVrizaQJIqp5L8NumTpNQmYM7nXcLVXI6/tyUZdrvliG728NZy1oGi\nhXxejunmBTYZdyQJXhOVVLXVbnUdQ4edBpIIOZBEv+22+509pk3mHYSqFmTQbnTQaSX9cl1vCeVi\nwYyPDkNfpa5/b52NJP21icV1O0RR7zaVGBvdictbqr1tQvJAzhPH7j5lSYZVHUI5vk2YblQiYhsR\nZvtFqBKYG5ukPFnvEWlL0nXbDxLVx9qdBM12G51O56Lbe4QCsxMR0j7hbPbgYeDKwtAmJ7f92+1v\nu7rJjqWr113VoOu7kWNLe6i+ZR/WUP3ASL/uyD2I9PMniZGTlLo+AcBIAudE6qKv2X5P+rSD3XIK\n7XkzgPRahdEJM7yzO3T4om/kM4yt+vC2c31rs9OoryxedNMUS0UzOjqKdqcfeVj6gnpmczlOoG3H\nGZ1o2j7jgfRBPQ4UCyxd3mlxAITp10p2TMm5SJbQvlt0snEZEzq2zrqsY+tAUl/3SovjsLta6tmw\n41pLEqxrxYEgCLCwsIBKpXLB9t7SCzRXiHH9iw4hV3SDaLPDDbQwuwQAyMfrX4T1Glcy7bjT2cGw\nUOTRoDzkygplbtBEBv2OGpnGRscAABPjEwCAuVknJDJcLvOx5BMATp3mmPR8jm9oc6Xq6tfguo8N\ncWxvo64y0Sdc9527WCNgbs397tHjnD4wKMrglbobUxrN46kHTmIQGBku403/5Dv0fcfaGqcmDCRD\nfD7nRtNEOkWxyMzvqUkXQjU1xe1lJw3FwnBWFgTc9rNzfA/PnJtzJ0x4f9Phz8nxUVckE4jxqals\nW7XC7VSv19fVIZT9W/KihnqBBhHXYW25JudTVZD7v7q2CgAoqftbadTx/33koxgExhDhZ2gn6ioC\nxw4XZPhbQSWnKYG/R3L/9csrlCcyZ2yfcmV24hXLpoJ6eofkws0oH/v4QaeFMDnDbbu3Vcu2Vcrc\nr2eu3sN1qriyscf4frbq3L/rYZaPAR3Dx2/J67ueugG1Cq5zXa5ev0hTefH+PKYHklNydOcBfP+7\nP9G1rVs/ZTuQybVxg/1mjpjKfTFdk8sLnQVdA/NmEEi76mPbI3RNOFUt/uRn79rSOc6HPXv34Z2/\n9VvZ8wm4l2SlwmPLzIwbU6+8inUNSiXuQ7kol5VVKw3eVuDxplJrZGVDJX5Ggzr3Rz1ZtvO4guwz\nuWdPVhaF/Ow1am68BfEz8NDD3wAAHH3iyazozOkVAM6wKLmhAePjXOfxER7rdu1wY1GxwO+YWoP7\nei5211UoFPCrv/I2bAbehevh4eHh4bEN+Beoh4eHh4fHNrDFnI0GaZqgXlduBrveRuK6Ne6d3BAX\naSIuLvs3ACRt8V2LaZ9TLvmCYXO6XeONw8PO3XjFfnapxjlxQbUq7mLE531m1kmJNjrsQqjV+HMq\ndDb+Fbt3cl2a7M548sx8VkZt9jOE5XEAwOEJ56acW2IXZ1ridpibd3VoVJOuNYSLQdLpYGFhMVsD\nBICOuD/zeb51raYri8VlXiyyeyKM3L0oigtm+nGW7kwSd635HLtgWrJmZtcvACAkWVdOZS05cl0m\nkTWJpna3iAuyLHUo5N3+rSa7SxI5VqPq+oN1/6wtcVvWqs7FVBG3dbXK57n+eiewE4XJll1o50MC\nYI0Ia+r+WWeuXZscU2u0UcLXlpN56Ag5v3NJvgfifmpDrz/ytRVD/hwruLJCwH1/rcz3yxxwehTp\n8iwf66zT516u8j2o3ciu9VWl27KWcrvlDG9M1RpoYNuM+AojNZcuyj2PZZnArpcC/7O972ySJD3O\ny3Jd7c342d3Z3VlzFudwEkCAlEAygqRAkBJAUgwpglJQX6jfop+hL5SJoEQoaESKIgMAQYEHEAec\nW3t7s7tjdnz77vL6kJmV2TODvTF9pwjF+3zYnu3qrnrrrbeq0zz5JEBGYc5zCeGdAMvi41gT711o\nn6fYz0nh0zxvP5Gvs07YV55UO+HzzwFPOYeYT8oxWyfv66TPngdZBhBHDviFev5eHsIFvAdLZXne\ntg9xDY0ovdVqtPJt/T6u34zu41DlR3d2DwAAwCe+Q109w5k7USTaQrklz7BykfOpkgq0mNtB63B2\nVlLvB/tDGguOfTiW9Eu8j/vd3cWGLoftbr7tymXsAbByFfPvMy05L8+xwVMh3efBeKAGBgYGBgbn\nwJk8UMuyoOC74CiiEP9dKaOlHajkdKOO75VLaH2sP97Jt+1so1VQIwLPzIyQJRYWONmL1sjy8kK+\nrT/ApPHBAVo4c7NCaumP0UMJQvGIXI8IGxZaFKtz0inqchGPubWNXlmgmGllIh2NB2jhlOti2ZTJ\n0/Va+BnNYOwMelOzFm3HgXq9BqORJOf36byHA7Sm5ueb+bZqlaxKstT6fZmHnW30mouU8D/YF2ss\nDNDCq5N1WVbW4niAFqhr4WdG9H8AgAH93e/JvorkvTKRaag+zx7yeIxrZBzJPA2HaMVuPcNrEY7V\nOe/jWgnIg710WdZDFo9zT/iiKEIGt+wAhrZcayaU8I1SVl6mb1GEhBjbTfU9n0g5g5DIUal49Qng\nWmxVcF+rVxVbuoj72Pbx+09mlFd7Fee0MZJ7ZTfG/XolvA9GrsxFp46eZIF2kcbqvBL8HLPZRxO2\nNK7vGnmszETHsU+XI2sBgAcZgKVJVqc/hqXHQ3/mu1KRCevoZ3REgF9t/r/aZzb5GUR6ZNsp73fa\nrfOcMZxEW7Ky7MJeOSMMQlhbW89Z9QBCuswA14StfhYOD/B569N9PehL1Gh3H0lqPYoWBZGsr1oD\n1+PcpUV8nRMCj0teKRMOHV/ujVINx+IE8jvS2cZn3sP72OlsMBayJ0fbAvp8rIljLj5vhuSd7h20\nZSKIORxxIYGqWFienzn1fBsP1MDAwMDA4Bw4mwcKAK6dAVhiHczMotcSk1UWDMU7fe02diD70hs/\nDwAAh3tiAWzvbAMAwM1bnOMR6+XDO+8BAMD69hoAAOz3pCn5xw8eAgBA1UcvaXZWcpMHPczr+a6M\nYUwmhkO0fX4FAPDIM+6n+PleJGZHrYae3U4fx9XeOMi3vfzy2/hejBaYV5bxWYcpbHmHMA1YgDlH\nLl0BAOh10dtLibY9HIjHYVvoLVcq6KHs73XybZsbON/Lyxj7n5uXPAJHA7pdzD9GqmwoHFOOoU05\nDU/mL6F8Z60mHpFD1mVM5TKQiXU55n2Rx7q9L+e1T7nP7U0sASqXJQexchWtWM7VqMsLnd4Y0nQ6\nHn/VB/jadQciW90WXOsGXN8qHqFFlPtyGbcVVB1oOqB108ZzjA6Vtx2R10hTWa7KF905/LtB3rpf\nFBt3f5bq2palbu4O3VJFKm2pz8u1aLfwu8Fun85BlQ05XCOK4+pm+rria0RRh1TlQMdTiq4wLADw\njpjx5/a2cs+Or9nzPnv8PFLaQaaPf0IN4dFtZ03BSyX26b7oWNPzdNI0g9FwnJcYAgBEueeI44l0\npILc8pieo92+PFM4r++WqEwqlagR+6kJFS1XVI4xoUjSiDzYvbb8LgwiqhuNxcsEKpXc28PnVKcn\npVqLi4s0ZvxMqq5rSrWhtof38ygS73n/EJ9nAXFKOm15vr/56osQhur4z4HxQA0MDAwMDM4B8wNq\nYGBgYGBwDpwphJtBBglEOZ0YQKjx3Q6Gia4s38y3/fLXUD3jxWuv0ofFva5UmDyEyeZAJY3feg1D\nv3/4R/8RAAD+6m9FqaTdxs8ViTy0tSHh04MOhgYjRaceDdEVr1cwJLuxIeUbrTKGMQMqRwhDFRtk\nAtMV/F67L6HSX/uV3wIAgL/8m78EAIDHm1JWYFnW2bW9fgayDCAK4zy0CgCwt4dhj5UrGIrlJD8A\nQEKyVqXiJHkLAGB2BpP4HGLViiccbmVJrkiFTxaXcI7KRZyjgSIMZRT6mZRhpDArlbs4SnduxEol\nFKMbK8LZgMLUS1RadP3apXwbywfyvgeqxMV23YvXPfBxChYsXnZhQoQxmaTQZ5laIyxh5uG2eKxI\nIS4SfuokydcJJEQ67hCJgjheB9uytuoUz3RKNH+qDOhOiOe/fSghrAO6ha928D54cVlIdeUmHnsZ\nbzWojtX95+D4BiGe30Eq53VIdnXHISJUIuuhBxTCntIaxxCuNXEJz0IiOjGyymSd537+pBAulaWc\ncPyJMR0J3Z42hJuX12SsRHS6L9pwErXofLAslPXUIVxWz2PFN50ScYggx+8N1D0LRBQsUQonVKkP\nLgmi5Q/hQNbs4S49g0mNrtuT59sBkQ5LnuyrRGVvPM+xCjHn3B/ayNKyAAAZk71sPk+ZxRH93kSU\nr+irEG696EAQSLj3eTAeqIGBgYGBwTlwNhKRY4NfK8GwLxZp+xA9EitC6/gX3vrlfNtSC0tG7t+9\nDwAA7/zg7/Ntt27eAgCA3/znv4Hft4Q0cmnhOgAA/Nvf/fcAALC+Ll7mX33nzwEAYDNEvcbt3e18\nm+OSJqkiutjkP3BznwAAIABJREFUXXoWWksjZVkc0nmMxixkLtPhEFOlUsf32iP53oP7KAMaDNHC\nGXZlW+wGUxNScF0XWjNzMNeVchTe9ewsatvGqmRnbg69N9b97SnLrlpFslWdrMWtrS05DhFKmk10\nVQLlgdabuC+bBCgipZla8XBfxbIQhYolKu0hz81WVmlM892h80mVMP7SIp4PizI8uPuxmge8ngvz\neH6esp5d3wZrSmbgKMzgwydRTibBQU42SbBAxsx2ekQEnGGghLYTIq3F+DrWBd5U/kIOHuzuyBxl\nRL+wfSRWdNriDfx0iH+vizEPczVc1+MAxxV0xZK+1cL9XpmlCMOBXNdiQsQnJn1pvVvyNtKUGjAo\nOzs70a87PyyLSETKOziLt3VSCQl/3z5pT88h/oiO+Qn7PMkrPieJyErP5k9O08vxXAeW5+pQLMrz\ntkARJM/FiJXWHs81k9lTU6IuNdK5brXw9emGCNhw0wqPXrc2ZVu/i15mvYrHO2zL831Mz+fl+cX8\nPZe9YLoGcRKpbXSf8fVR5SgWaYPnpDKlY53SxxIiH9mKxLq3+0yE6z8FxgM1MDAwMDA4B87kgaZp\nBv1RBMOB0JW5pdN8Gb3Na4vX820/+N73AQDgu9/7DgAA7O6It+hRW6gOCQPMzkpxfDBGa7pVRsvm\n3/yrf5dve7yBnsnaYyx1KThiSfkeej+Zsh5mG+h5Nai0Y3Nfxdup+0CHPLWxojk/I2r1IQkWxCBe\n1r0n6FH7NbRamgtSVnA47E2t6DmJY+geHkCdxCYAABwLc5KlIlqNbkV5zZRv5Ph9pyP50VGI12xh\nEec5iMVbKtPcjCkXWqvJ8VLyOH0qWG6p7ioVkgcMQsmL5MID1Fao2ZTPpyTRGIR4fVnUAUCEMR48\nfpKfO2NpaQkAAEL63lCJLHTHg4mcyEXQDQH+/DFMFuBT7jPPX6lbhmcwofdCFXhIyKItZeguVi1Z\nPy2LWuWRN9dXX0zJGB8W8b3HmXiNA5LCvNKUeesFuHbrZczVv1STHOhyBa/LJ48e4JhSmacqlRJE\nKV7XtrLAx5RHYgm/grKzyye0m7ooHBvAmigrOcsNdNJnT2ohd/RbzzvGaY/PUodndUFlNZ39OxeD\n7xfgxuqVvJsTAECV7nfOzUbq2RBmuFa3DyjSqFsX0vj7HXxWOmp9lSkStbeLAi6joYRNOD/areLz\nt1KSe2NIxICSpZ5r1HnLJU85UzqSzLXIeRKu7hxE46Eokm6DmF8zyq+yNCzA2WQTjQdqYGBgYGBw\nDpgfUAMDAwMDg3PgTCHcJEmh3RnCSJFaGlUMy73y4ksAAHCwI2Uif/zf/gi/R+otv/d7/zrf9voX\nXsdtKXXoUFR5l7qI9HsYgrp6RTpSfPM3vwkAAP/1jzDkt70r+roWJbybdSnfWJrDZPRsFUOJax9L\ns+s2hWcHAZ5PfywlGnMFDP3euP0WAACEqnRnfQfLVg7aGE72GhIyTp34RJWT8yCKAni2uQa+0opk\nhYwkwpBFuSbdZVIiuHCZx/qmNMbd72C45PqN6wAAMDMjyiBWAUM4LoVwvvjCC/m2IMTvsRqSp7qx\ncGNr3fVF6N84X4kqKYooRLywgGHkS5ekVOXDDz8EAIC7d+7gZ+aERNCksbaphObZtqQCgjSDMJqO\nFm6aWTCKCxNhKovp+GRr6tBfHrBi4oKjQ0S4dUhrUpPkCvSeR/dFGEm4OiRyz66LodWPQjne7DyG\nZ39uaSZ/7/sf3gMAgP0O3nerC6/l20oxXrPvNtYBAKD9VELfM9TcnkPUI0s3EefzoLImzamC6cIC\nJO+cpGl7sb1+DuDSprN+7xzDm9YZhWEIGxubcO3aSv5ek0KkXUr5JKHcT1wSOCYSZaUk6Z3ZJt6X\nXF5mnxCFnyN9bbslqRy+v7ikztWkQArd6udMq4ka37UqPm82M70KSS2LQrGDjqToSg1MdXC4Vofa\nuYzPIsKcfsYmaXbqi2o8UAMDAwMDg3PgTB6obdtQKZVhsCsJ4b0N9N4aX0Nyy92Hd/Nt73/4PgAA\n/P7vIwnoG9/4Vr4tIQs9Ius7VOURrGcY2dSf0hOSzm/9+r8EAICvfPErAADwbE880JSo1rWyeGW+\nh+/9+O+xhOYnP/og39Y+QFECLtvwfPESfukXfxEAAL7+dTzeex/cybd9+0/+OwAAbGzg98tD5ZW4\nCSTT4bSAbdtQLBVgfV2EGliAollCi85yxHpjuywgC3JLRQM2qT/f7iEm/F98UbzMMOYyHvxevfa1\nfNtwjHvd38N9NZqiPVygchkmAAEA1Iiabud9YiU5zxq93NP0+vXr+bYWaWVuEBVee4FMOrj3EHWQ\nubgbAA3d5xNCTg8HAOpHdpXrlnJUQYuIABfE46smNzCZKrNwLbqpc+x7XSZTqPF3qHxqm0hb5dtX\n8m2/+Q3UlHbe/X7+3mET9/+QyETvPlnLt71M21pL6M3vbIhG8zZ5FA6VVARq7Cn51uyJRsocj6fJ\nHgIAsLDAfcLr/5wcyAuDeShTnpITMaU5GQ5H8O5PfgrVumgmX16hDlXUi7agSD0J9RteIg3thSXp\nZlWi+5k7MK1elUih4056kuxt6lPhyEsY69IwnMyiKkVMaXujgZ4ol/ABCDFuSOVvIyXI49fkdwMP\nrIRO6N4r5GUw4kuGUXRqYpjxQA0MDAwMDM6BM5axJDAa9o/IJeFvsO+jZ/Ro52G+bZZ6wH31q18F\nAIB6Q7qgf7KGYgQZxbO1hZJ7peRJVSrS85KlA197FXOTs9uS57v7EZaXLMyKlTRD4gBbT9CzmV9c\nyrc9+BjH6pJs1IIq3q2UcayFAloxL7/ySr5tZw+P2Ruh59UeSE7OL1bBtqR85CJI0gz64wiWVsSy\nm5tDD3BEklqZspyePMVc1/YzrIUoKU/87TcvTX5mSwqbaxW0IOfJshsqua75BYws5HkOVY7xw3d+\nDAAAH3wgXv2tWyiQ0WrhNbt9WzzdGzduAwDA48ePJ84BQCzCm7fxMysr4nkFlPedm8dzt1V+ZBwn\n8Pg//Q+YBiKwYCe1JyxVzuuclNZmL9PinoLWcSu7SLkWR3l4Y87bkLUdKO9iXKX8EOnv/cbXJRrw\npVdvAADAO3/7p/l781TOtEXz9yHNLQBAI8Jrt9DC3Gnytlju77cxOjTskQCDI4OIyQONaR5CldyK\n+WOnUzr7VGCHJ+tM8n3/T2Cd8B8W1/jZjVqm5q9Pa3YyyCCME3i2s5u/d2OI96HlUNRDPTeqTfIg\nuQWSet44DgswUFmWEjHQf+Nxj4P5Erq/Z97FKVXlW0OSib28DAAAzaaUao166Hk+XVvD4ygRm+d5\nkSmFCUtFvM8ilfd17NNfN+OBGhgYGBgYnAPmB9TAwMDAwOAcOJsWrgXgeRksLokL3awTqYRce78g\nNOc/+IM/AACAf/o1JD9ESuknIcJKu43hTu1uj0dIt+dEdDmQsOE+EVa4POLP/uTP8m3f+w6SK373\nd34nf++tN18GAIDrN7BLzIoirjzdJD1YcvtXrkonmUqZKNoeUqEtpXjEIWzLOq73alkuTCvgEqcp\n7PVGcLUlza/Xd/D8n1EIdjSUMCg3b97aQrLIKy/dzre9eOs6AADMErU7Von7X6LrU6LuCmOl9FMs\nYljbpxDu/p40wX733Z8CAMDBgWhZrqxcpX1weZJSPCLlomvXMCStQ7+PHj0CAIDGDDdol/XApIar\nq6sAICpHAAARJOAXj5AFzokYMti2JxlgHFkUDpFOX3DjZlYrOm6PWg4R1FQI12F5XV4nviJaUchs\nYQ5fG0Mhgm38gAhzaogzy3h9Vg7wmqV9IXSxstY8pUCW5iWEe+8uzunafbx244LcYzGt64Tu6UCF\ncCP+e0ohXACALE3A9VSXm+cSOI4ER3Xo90inlRNDeBZfq7OpDU0QUIg1xNfftY+TU3j/J+nqnhln\nVTp6LiywbAfUbZmTDm0X7/G9QynnY11vfnU1GYgIdTa9p0tP+Nk9Mz977BRGoyG94iIKItUZKsT3\nkliea8MB/kZw1ye/KCV4B6x3mwtkHS+0ytMDahAcYq7QM6/TkabeZVWq82kwHqiBgYGBgcE5cCYP\n1PdduHljAdJUfnf9AnqjXerj9sbrb+bbvvZL/wQAAErkQfRVTzjWL33/fSQzvPXWW/m2XdJPvHwZ\niS+6UL9awTKJA+qD+aN3pMPLu//wDgAAvHz7Rv7eW6+R53kVPaO5eSERvfgCij88+QSJFyuXhKxz\n6yZu80hf18rEY2MXYBR0Jv4PAGDZ1tSEFOIkgYNOH0oHYh0xscotoOXU8MRzKJWQ+FSvo6cxNy/F\ny9xdoFrB773xhlynL739RQAAeLqOXi1r4wIISaxPGpWO6tN35QoSfa5fW83fu0qEpxLp3O7viUc0\nJD3M119HEY2JUhWycD0iK/zd30qpxo0beD3LZC2mqpA6tpOza5H+DER2BjvlYMJazkeYy6umJ2w7\n7tVwww0m4PiqyWgh3wVuqys943qd+rVGSJz4+Mc/yLe1auhJJlWxwIuLON81H+d5cFcR2lzWNqXO\nGIFEFq6t4j4+2MR77VB5UXnZCnXLidR6DrLjFv5FkGUZZHGUl64BaC/nuJdo5+O08u/nOHKxTvT+\nrCMfPf1I1SAmd6bHYJMedZJ3Ark4Mmua/UAt8AoFaDQkipj3+uzj/fngYymbAzqPgNYOe2wAAGk0\n2aezUhHyUYGiQoUKd8GStTfosxAP3TeZ6kZErnGsxFG65B3GKZeLCZg0lOb6zUrH+igxTX2RvWX+\nfqzETKyKA6edceOBGhgYGBgYnANn8kAdx4Zq3Z/o6lDyMG7OPdteUeUeVfJkOA/m2KpPJxfa06tW\n639GvSpvrF6f2DcAgOuipcB5t709sbgHA7RU7t39MH+v2/01AACYu4Se5/IlKY+wyU146SaWWsSq\nInp2Dr04jwp6WSYQAKBWQy84iSlXq1ok+gVvapT8aqUCX/nyl2BuXnKgRZ/yfeQJzM5IiQ/P0+Mn\nKFd4946IWjzZ2sTxUa7p2qrke22ir3NOYmZRcorceq/oo3VpleTc3mIvVkUk2JPsk5V52BEPNCZq\n+soKyohdvizlRhx1aB9i/vbOBx/l2yLKyX75S18GAICC6uKSeM7EuroIQiuDx0442ZtSkivH8Lyr\nHHNnC3qtqDy5R14KkFDIjC35nstEq2fLOFKWe9ggi70mkYUn1GP13jbmR1+bFS+gSV18xtTn9lJJ\nvI5oEf9uzOBcrh2IxT/OJQm5i4WcVzRlMT8LUAkxL18AAAuoswbl3fX9lCTJxHu2fdJVYM/wpC3n\nc0H1GHitsmiIzsGzB+TSNZ5WdGRaLqhj21CrlKBZlShTQjnI0ZB4KYeH6vN4YJbh1MG1hLzE/BTV\nHMUkrRdFuLbHah2n9Pvh8c+PjupQxGasegUPR3jsUhnXqr4nuBQmL4lT812wJq9BqsVC6Fk/HOL3\nUy2QcoZrZjxQAwMDAwODc8D8gBoYGBgYGJwDZwrhZgAQgwUpSLglIvd6aRZVIhYWRc2HG/JGIYc1\nJNSxOE8dOZbwezvbomm7Tmo5zFfQ0YuUYoqee1zrg0UyNjbW8/c2n2EZwLUXXwQAgNdek24VNoUe\nX3oZ1XM2NzbzbaUSTk3Bp64igUp00yELRPuuNlQJQBrkHTwuinK5BG+//Ro0GxKm5c4jQyJtzShV\nDg5nJQmGfAeqpGFvH8Ni9QoSjRaVItPGJpKH7t7FkC/TxQEAbAolMoFnMJBOPB7Froeh1DSskSLI\nmEIqriIdxUTEYuLYVSJ2AUjodzhA8szbXxRSmU8axd0ukrZqakE0GnM/I4x3dkRZBptJNLHgjoVw\nNWclX4KZfgEAgJjCVKzqU8ykDEorKQEARCr05VLYsmDhdZ6fkXDtwhKS6tKSKHo9+gC7sVTp88tz\nQjDiUqWCz6UVcp1Yneg2lcH8ZF8UjHbzmC2OK/0sQ7iWBX7Bz0luAKiNiwem1I+ar5hCuFvPUA1M\nryG+Vvmles59eHKa5Xh3Ff5YogSuH32MJVeL9Ky7cUNIdLyO8/2fuDTPtl6zLJua3rNtW1DyPfCU\nUlAwwrkPxvgaq3JDi1I+fP6RKn/jpvcW7StSYVcnITIc3QepSvvxBHPqpVyR52exhOu/VJbfiq1t\nDJknEe7DV9roTP7hNKGt5+lIl6ZMa+7SdepRuV2oUggN01DbwMDAwMDgs8WZPFAAAMhyZjMAANjU\n5/DWbfTwuFsIAIB7xNLWVh8X7TMh58GDB/m2jz/GPpva2zkKn8g0VVVyweZipydlH588QK/q9S+g\noMKlJelB2SMChl9Fi/6lL6ieddkkWWE0kuLinT0k6WSU/B4MJaldKE1PSMGCDGwrAseVCa+RNzbo\no5UYR9L/rlFHr2J+Dud0pvXFfFua4rXoUy9XR5lO9+9ip5m/++53AADgcEf0hV+g3qBN6v03VrqV\nQ0rgb++JrubGOnr/rCGsS05eIoIZl7NoMQcWV+DSpVJJrEzu1XdI5IbdfREXKLdKuZ7yRZFkGXSS\nBPT1Oxrn0NGFIw7oZKSE2BbMTQgzTWBgaxlflz3xTi3qSlSn87+0uJBv476NoSf32OoCvnelhNZ8\nwxKhizHdB40azvPOjkRmFheRwPUmCWz81RO55h/s4/csC72PTLmg6TSEARQsC8B2ATxbCSmQN+BS\nj+BsLOukT+VrGZE/vFhm3WEPlN4an1BOJtdTlxxRr1ciCsaWem4RGaa3LxGyAZXjvX8Hn1kLcxIh\napabdBweiyq54sidzb1Yj5PfrJNKoqba39QCsD0oFNVzmv4Ou0jMnOibeayXploLLBqRl5IcFyqw\npZ9Rvs2hZ3ejjs/bZlMiKkV6vhV78rxdvYbPuJ1nGCH01MOLI2N8vDiU51Ovi99LUiZJCVGV70Eu\nCywocZbV1aunjmoZD9TAwMDAwOAcOLsHCgD6d3dpActCFhcwp+YpSS7+m62D9ASlfJZ3298XObgn\nVIbB73EhPYB0bSkSdXx+Xiz0hKz8/lCsl0dU0vLwXcyVXL71Ur5tgbp7sFVdcJUVbHHsHv87GotX\nu7OPORB2BMZjyRlEvRDSZDoeEecrXGVJ+x7OZZm6cBwoy7hIVlS9hgOzJzonoKW2S7saDpRcF/VD\nXV7C3OmDB9L79P33UW5vgQQoXFe8pTDlbgoSKWA6ea+HntCcuj6/8mtYUtRsopU+r8pzeB24Hlp+\ng754UtwRpl7HEo1RIMdzHXu6nTxSa7JQ+4h3mT3HG8hO+I+oiB2XgitkaP3OKkekkOCcLs7g2qyV\nZL457297cl2vtDACk7n4vfGmeJkWhYpcwON0RxKt4FK0W5fxOC8sihf19/vUJ5dyodoDzaYkEqLh\npFku2AEguXyOQYxSGfeH938IAACvUz/g2NN9hCcl2zzlSXLOK/eoNHeC3ism3JVE5wfx2A9+8jf5\ne/e+93/wczVc2x8v1PJt/+jtn8Ox0O61tzTqoQfkUucqLRCTr5d8velIx7QyoABgWWA5HswsCAci\npLKhcANLAqMjuUP93knPcH7O64gjC6JYxDMpl6S8qkJduZrExyip5263h/Pd68k9zuIKXL7SWpBn\nCj83OG+ux9du4zOOn0knjx3HvLQk5Y2VShXsU5bGGQ/UwMDAwMDgHDA/oAYGBgYGBufAuUK4WSoB\nBXZ9i/5xBfujoVsdGuDkLRNKdnYkFMmhW3a9WW0CQIgnHoUSlxeX820RUeEDS0gHu5uo63j/HdQU\nTVWSeeYFJBYVScORQ5kAiizBpRojaZIdZRjOZWEXxxN3f9gJYGptdDMM6fiKZOKR2tB8C0NvDdU5\noFIm5Seao25fwrTzC3iOl5aQeq8abMCbr2Npz9wMntC3//Tb+baHnyCha5OadBcUhRxcbgwt890i\nQsBNIqfcvCkNtXk9cBh+e0dUpBw6rxp1BQlCUef5hMbARKZbt0RFqT8OJ0LVF0b2fArYaUNpEvKd\nVPUBALCJ7r9EGrhvXZLSk2aRyrRsmtNE5sGxuCxFbtv5Kl6P7gGGvIdjIUrYTMyhda0JdymF2+ap\nO8+ry6pc5gGWNT0OaAxqsUw7gGsDgJ9ZOZEHAMCmtcAlVMsLEl7+7X/xqwAAMAYc9zCS77ESV0zP\nCzuW5waXTPD1mCSeUTrJIoKdOsmag/tYduXZNRPjXFsZ3i+766IdG7z9NgAAJByWVGFDi8pEOMtl\nTWR6jq6TyRDuVGc+y6DfV+F86jjlUVqs1ZL1yPcsd2jyi1JywmVRrBhla23rjNXn8PuNmjynuEzO\nIWWyoC3KRwd7+GwdxXK+f/anfwIAAJ19JCt+61vfyreViKzo8vpR6QbWW4+o/MVRknE8hnod7wnd\nDegsZXHGAzUwMDAwMDgHzuSBpmkCw6AHTiJWyGwL6fD8a68tJfYWWXcwVcXI7UP0aB6vISFnd3cr\n39ZqkcYkl2wMhFDClg3vq1ZTZSxs96fiEfk2Wn3hEK2cntJmrVJS2mGykyJ6SNNGfC9OVM9GEk5o\nWtRlxhFL13OKk+SACyBNMxj2x7A4L15fvYpWb+CiZ/LJlnjuV69gQTcTeHod5Y2QrTQcoZVdUEny\nZh3nu08e6NwlEcMorlHf0RHOaaio6ivLmMxfWhKL9fZtFKV4i4QQHNVHNUnwb44obGxs5Nu6XfSW\nX3v9ZTp3ZbmLYgEASAkTAIDjVsGxz8mFO4IMABL7+Zb+5Oajn50oZMF/WfZWfdSh4u+bJHrw9nUh\nRcz7uK1CZSlhINew4bAgicxNj3ol9nZwLp0JvU+8RxISuvB8IXIAeaBRhOvhckUs8CvUQeNRhPeM\n9g6nJRIyASubKPdgLzElj7tpyxryAeflf34PyYHtQN9r+PfKZVy/C1dkXTKvLyHPRoRYZM64fGUY\nKj3ViEpOLJm7agWjP5aP901ZRa4cKlVJmfClel065MW5pOVs6T6iXPZ0QicZC1KwplSqFUcR7D17\nBvfuiF74zJXrAADQIlEWfTcd7QfqF+Te49GzZ2fbmrRIZSVEiutsy3PXo/N26DmwtyYljG4N57YT\ny3q8+xGWImYRzp/W6q2SHjELKISBEvmJuRTxeClNiSJ3XE6phTIs6/Ttb4wHamBgYGBgcA6YH1AD\nAwMDA4Nz4Gwh3CSDYTeEli/1exUPyQdJfJwoxOAauPFYCBFbW1iv9vTpJwAAMBoKSefmTazZlBCu\nqvsjwg4TZUCFQUqU4C47MgY/V/HBUFiSimoQ1yqx0kes6jddB/dvZ0yEkpCnR26/N8TvNZuqvVY1\nBcebTtWW57mwvLyYJ+kBALodDHWORxR+diU53xvgeyEnzVXNZpua0vYoVDo/NyfHKeA8eDR/165J\n3e1wRI1qE5yHWk0IHV94GUPGjYaMgQlZLl27MFRtiQZ4/YukLDQ3J8QVDuE+20JFnBs3r8s+aVzS\nwkqWbcEtToSOLo6Tr511TJFF1eodCTHrbS4R7kJLQlJLJRzvm3OkxhUKoWO+hSSs+avY8q2jVL+4\n0XXal5rkrcd4/ySk1lNQBD9IcD1EVDebqKbBjRDDxgGFNIuZhL5uV3GsP2rjmu9nimBBtaupaiJ/\nEWSAjcwzkHu24OC+wx6G6oZtqbP86QeY6vnxj9YAAKATyVpgFZ/DbUxhfLl0O99WquG8dru4bWlW\nNKQ9Svk8/hhDhQvzQkz0KQyYKrUtj/SaXQfXfW0k16MW4f4D6ppeUXWg7TE+4ypEgLIc1aaN01y8\nltRzzXWsqek9Q5ZBFgcQarIZqa6ViETkqGbbvLZZj9hTrSW5taJ1Qg0mr/+QUgSPH92X79FrfIhh\n3acf/jDfNncd1co2x/LsCigsaxH5bmtT0n31Ou6f04WJeobz33kVrUrRMWnRI4JmVbV3s6zT190a\nD9TAwMDAwOAcOFtDbcuFljMLS3UhPRTIIg8pQR4UxFMLiLzAxtPGptC9N0gxpddHq8xXiisrV5GY\nFATosbTbysIjS1I6L4jV3yIyTNUVS3tElpadEEU9lH0lAVqSI0oga+ulwJY5dbJo94TwMiJP2iaC\nTKKsZ8uNp6YaYtkW+L4Nu7uiU5qQOkytigSJm7ekTCQhHdX2HhKLtJ4sW1rz5D1z8hxASD2VOu7z\nrTf+cb7thVuvAgBAk6jt+ty4muLZtswNX41dsi6HQ9Usmix71tNt1Gfybaur1wFAiC+28tgyigKw\nIk63o0qKosO82e808LOFdlij9IQPsCaobsSdzxR5zUr39+WrGMF5YxVLwCq2eHOeT17pzDx9W2zc\n7T3UAB4pEgU3G6+QN2CPxLOIad5YaUbrhAak+NKax+taUqVYy2VcK3NkpQ8jrUR07OwvhAwyyLIE\n6kpdqUZEnFodx/2//uJ/59vWO3ieMZUf2CDrOKZzr1BpTl151UUiswSkmhVuyRra2cQyqfvfxfKt\niiqTAormzK2LB/WWh9GSDj2f/I6s8dG9nwIAwJe+/s8AAGB+RohM64DXqhfi61iN78km3rNzi6gF\nPQxkvbz25itQLCiVtAvAcR1ozragMSvjKhbxenPnFB3xclxcF7ye9RoXj/N4dIZJRBwdqpbFw8uI\nvJlQJKmkugvtUueVO0/lOe166LOy0twHH3yQb+Px9Em5LE2PR0C5nOuk7jysaKdLXNL05GbsJ8F4\noAYGBgYGBufAmTxQ17agVfEhjaSsZDDAYvg0RastDMV6GY3xc3t7+JmtLfFU2m30UCwy+ZeXpXTC\nIkr72hPsdVjwxMq8fh3LJHyy1FNVsrJAVtWMDAGCEZbLHFL5Srmt6NQ7GEsfAve8Ew+0XEKrp1LB\nnT1++l6+7YC8K+6zmCiKue3YU7PS4ziG/YN9cGzxJCtUDM9db7Sh1KE8IotTRKqvH3e9YQGL0Xh8\nbFt/gFZfvSbW6Y0beF3ZktzblVzQcMBWvJwwRwhi6t2pdVQZTaLL82cBADwSaCiQpa29ShbUCMmD\nKpdlPViY4rvxAAAXoklEQVR2PK3mN1NFru1KHTeaKirylVeuAwDA3Dx64K7KTcbkSQ7preqseOnd\nIXpP7WfSt3a2hnNRoO4kofJcRlT+MUo5fyUTNaQoSiOhQnIl/dmg0MLlIr4+U+to9CllPmeFAxZU\nUhuqqlOG38HnxOosjv/NeSmba1K+vXRAuteBeH8dihJFa+hRDkHy/M4sRs18yskfbkn/0/e+/xcA\nABCsofeYhbLGffKSZhN55vkpHvO9rTUAALBGoqP68M8xOjBL5XMPyzL29z/EPrgfUa71sCvcjv02\n3o9XVjEH+Ovf+O18W913JronXQRJlkE/DODhU4kGrpEGLkd9Coo7UTzS7aSshFuYm+C6rIUri4jv\n4wKVXlXKkseOPf4evlYHcg3vv4veZftQIgSFIn53REI5B2pbSj2GI+oTnSkRHSkTYu9d5ZxpX3kJ\njiVjt8CB0z5UjAdqYGBgYGBwDpgfUAMDAwMDg3PgTCHccRDCwwePYG52JX+v3aVQG7n/RaWV+HQd\nQyl376FbrpWIuF3MEoVubaWusrWF4YW9QwxV+QUJG9guhgRmWhiSSVMJ9V29hAn4qzOiGjI6IJ1G\nIimlBdm2T5T2foDufKwURYo+tYmi8pf7H/8k38YJ6zKds05Ax5F96gT0p8FxHKjX6xArEgeXCbHy\nU7Eox65TKJZD4Puq1RlrWTKpZ06VsbAuZp+IJVkqdhWTjbhtkKfIDBUHQ7Az81KOwk11a9TcW5cg\nRRQ+bjSwFKZWFbp8pZzQOHF81arWzsSw5tYWhtxZaQkAYPnSPLhq/j9P5ES2E0pcgNRPbAph316S\n8p+bpPS0TyF3TxEz6peQaBXaeB+VVEuuxSXc5icSfocnWMYSBzh/QSYhLJfumxGFiEt1pZtMCi5M\n1HMsuTcbdI9dpZZ59wZyvNG0SURRDPFuG3Y3RI3GWf8IAAB6HXwORKGEd1slCmk/RELVUkOV1G3h\nXLQ8DDmvfSKh5wOKiZcolPg2lWABAFwOiaRHKaDevhxvROvXV+TDNqWdwoCeRb60YuxSqugP/8O7\nAACwowiG20RaDDMqL1LNwIMI1/iNq1h6s9CQ9fLgowcQjORcLgLbsqDgF+FAEdHGfSp/C1irV0K4\n3KIsb/+lWo+5lHbhs9ANtfkhyH2q7RNaDnIp4vBQtbIkTfQ4VeUy9F2+z11FeGs08Jr1+zhmS6UY\nAjqfw30uE9N6y7iPglJWkqGf/gFuPFADAwMDA4Nz4EweaBJn0N5LIUvEU+uRZef4+IseKaGCj+6g\n59nrofXWaopVVSZLMArRAnjvvX/Itz1awyR7kcowikVJQFerkwXR3b5Yi6vUePvWJfGIkgC95ZXr\nuG2gNG2f7lKh9hAtlPbhXr5tSI10gxAT1ltra3omAEAIPIkl85FlmQhvTgGWZU1oZjrs2RyhidOn\ncexENhkropBP4gUzMzg32mtmj65PBeLttnh4XOKyuooWe0mRCMpV3Fem9FfHRPhhJtVQXR9uNM6e\nmy6lYbENNlQrFdnWbncmxhKqcow4jidKmT5rTDbv/tnH5QbOM3SObyphiG4X52jQo24WyvtrjvF7\nNr2CIpo4Fl7PLJPPZ+RxBgU8zr6toihlvFdmSfwiiJQnSXPokMVeLMg6mqmiNX+pjGtkoa8aHlMU\nYTr+EMDBYRf+y3/+C5jrSanW7TF6nqtjJAU1CrIeiz7O/5dHOD/FTCJKxSpFAqhkZbMr3YiWSEPX\nD/G9hQO5jo0F9GIyuq/dgkTRIMR5KblShmGX8bnU3sN1WTgQEszdZ+hBDUMSg9B6t6wPG1DkQnmg\nXb6vyft5uCYkH3+nBaPx8fKM88C2bahWKmApL7O7j3Oyt4tjf+vtL+bbGiSqwF6ZJttIeQheC75P\nAQC2t/F6ukTG1E9E3mdMRB7Pl30u0jw8eSKdmnJREnpmWSd4s/ze4qKUWB4c4POdPdDJMhvcFzfU\n1vuM4/jUXqjxQA0MDAwMDM6Bswkp2C7Uay1wHbFIfR9/g7mrgy65ODjA2HafxBK0B8qq/oM+fm9X\nlUd0u2g5RAnRl0eS17l/H73Tchmt04Iv3unrL6GowOKVa/l7FbLCm1SgfP/jh/m2MVm4Tx5zz8uP\nZRtR2UMu2UnE5nYznLbgkArobS0fNYY0no4HalkWOI6Tl64AAHTIO+R8ZRiIN1KknBXL4mkPdGVl\nUoCio8QI2ItlOnqtKlb9zIyUUQBIiQwAgEXF7wVfrFmbcnb9Do1vqDrVUEIkoEJqzRTn9cA50EiV\ndrCQBhdu6xxNsVidbj/QT4Gt6wny8iWSe1RWa0qe2uoizt+KyhMfksMysui6DmXtv/cQ83gvkSRf\nlsr6blZJMk3Z8yyz9+EBXpe7uzKGVZv7jeJ9NzqQnPj2Dh7TX8L916ri8Ter6DW0qCPMoooUPBtT\nrhWmgyC14OHYgYNUjnHvEea6rwfohX1tVfL1deID+PQMshU/IE74OuD6LdniSbL8IzvvG4/6ahtJ\n65GHXy6r60hlSF2VW3YGePZZiGPoqfWXzWJJixfgNaqrTjIRdWYZZXitSirKUqzQ/VlbAgCAP/7r\nd/Jtq6uvw2A0HQ8UpTksKKrc3xzf47mQgjzf+e9cSjBT/VfdydIz/WzYpVzmkIQKdLTp1Vdf5aEA\nAMBAlSIN6Hlmrcta5cap7CRq75CfCWUq65uMTnE3FuYqyC65TI45ITNK8MJxnBO93JNgPFADAwMD\nA4NzwPyAGhgYGBgYnANnCuFaNoBXtKFRFwWZmKjCCb02ZyRUNSb3eJ0aJzcUNXuOlEEWl/B1ZkbC\nNHtUfjEaYkjFVlqhm6RmxO748rKogJTLXwAAgGpd3PF6HUMiGWlm+kp3cY+SzD/+CRKYRoFqtk0V\nFi51VfCVokingyGcmRYSlFKlaRknA7BtCWVcBGmSQL/XhzQRO4fLQkpFDP9pxZ44nmxUzRqXABLG\n8Kh5OBNyAABKlOjnbjauClPyewGFVsfqexGRX+bnpJSgRyUxXLJSKkroBqg8gklLserc4xMJJiT9\nZF1SlFAXBg7vspoSAMD2s72JcO9nDVvZnNyhghWsMqWK5VBYfL6B6y1UocaQwoJAoehUKW2NYiLj\n0TyEI1X61cR75DAU0sV37yHZ4nv38Z7Zj+WW3hkhKa5OIflZV3WLobBYSNrKBdWkvFrBv2ukSLTg\nSci4ReQLKTy4GGKwYdcqQ1vxdmD1dQAA2A3wWXKotFmrIa7jiEKaWrc1oRAk0Dl56vHGjd3zqLsK\nRQakzWxZuM9iIPPLxCK9HrkXveNT6NKTaxRQeH1IpS4JSDh0RPHjfokaa6uuQpGFc772CT47Ykue\nU8/e35haCNcCKilRREe+f7m0TSegMkol5A3JExkH33dhyCUhcj6vvvoyHi89rhOdP3vorUh1bMob\nYqsIKvMkLZev2fEUGaefNDmSu8VwyaQmO3L3FtbsrVQkbeW67qm73xgP1MDAwMDA4Bw4kwfq+yW4\ndesV8BwpZUgoaVuizg1VX6zpK8tY+P3JEyTnPH26nm9bmMNi8hdfQm3bb37zt/JtGxtfBQCAf3j3\nRwAA8EgRf+xczxMtGy7wB5CiWEuRnIAo6RbpO5YrQspod9FbGpNHkyQy9sN9tFYc6ic601LeM/Vs\nrDcx+d7pKX3YYDC1ooo0zWA4DCHLVLcH6gNaLpHQg+o+sL+/x18EAICKKjnZ3UdPpULeyIEq2XEO\niUREpl5BeaDDHs5bgTzR0UhKCpwizs3aJ+KP9LvU/Ya8LF8VXmdk6Y9Jr3XYU0QOKjMYExHBsuW8\nKhW85oM+RTnqQqqKomFOo/9MQRdVa/vm5UW8TYkgc8F5mUhsunzKLfB84zw0Fi7l2xaJ4t+YpU4V\nFZmHjQOc+2//tegyv38HCTef9MgLVsSUXTL01+l7M0uyLSf/UXQjtnTxPI61Sq/znpzzMpVwPBxO\nJ8oCSQbQTmCk7lnLxshOUsS5+OkJAiyJS8QS3eORxpuSp52ppxt3MeJXXY6RUf9ecV4S9T2cf8WN\nzDsG5eVEyrtiz8WlchsnlrmzS/heQOU2jmK1JPR3j0qGWBsaAMCLU4iT6TxVLMsC3yvAOJJnygb1\nZubevb7yxsplvM7i6atSEJrnk7RwWzP4vCy6kz1DAQASup5MAErHygMlUYcJyWVukWTxi55vIo4l\nHCmQL7IIRIsIQgVFPpydw/GtrGDvae7Kwt87bY9h44EaGBgYGBicA2f2QG/efAPGfSnujime/e4P\nsav4SHkVTFfujvG9w0MpbN7YRJm+za0nAADQbEjuNI7I8p0l66Ag+YAMKO5OReG1qniUKUnQ6YxY\nSqaMQ1Z0nIlFX6Oc7CtfeBsARPoJQGStxgFa7zqfWKNuJcMxno+turSPg2CiM/tFkGUASZxOlPgs\nLWFOd3YWxx6OJSfZ6+K8Vcgr91VeK6Si5bCDeV/uiAIAUCni5wdULjLsiWzZeEAUf8pDP9uSAu8C\nJa5Ytg9ACpO552caSO6oQtuWqAtJRfWArVZwX0M6XlfJ9Y3GWHLjuWh5asr57sHe59ONhY6RKK8h\nS1hQgzweNQ4uaelT3lbntIZdnN9mDS39uRnxQKstnMudGO+xn34kHYx2t3Eefvz+o/y9BkVdGkXq\ni6jyagF59fskdlIoimdRoruEnffxUBem4Njr5G02C1IOtVQ+Ln12EdhZBuUwhlR7hBmuASfD9RGr\ngNKAc4sW57B0YT/e/5wzB+W55iVGdBzHlu+lKXuE+QhkG3mjicrXA5e00FynlswJ5wFj8lJTW+a1\nQBJ0I4oaeWp8QGMWsQXZ5qZw+gaVn4IsSyEe98FXz6ynT7Azzfo6ih+U1TOV86I8p6kq5ylQDpif\nMxUVDeRIAXd2KXgqwmBx2RxFnVQ3ljaVPuq+uykdm8vVHLVWOA/LucxmS55FBYo+NhoURVQ8m/mF\nGRonvaHmt1wu5eP/NBgP1MDAwMDA4BwwP6AGBgYGBgbnwNm0cJME2ocd6HckvFYrYVjo/XvY/Pq9\nu3dlW5OaK2fsgsu+ehTGCihxvbMt3QF8Kn0Yxxg6ChJFoumTUk2M309iITN0qEtKEGvKN+lb0rGb\nDVXiUsOw8XqGJQDcWBoAwKrhuOKUzkGFcDg86Xuk4qIaQzu2Aw88CYFeBGmWwDjsQ6hKVQ7bGM4t\nUolB+0DC4o8fYyiGlTqWikuyL54AMpleuHFLDkShpGcUtohUqYpDJACPykx8FQb0qZSmVpHwO2sI\nVyo4vnAs1yemLiIFv0X7VuFQIG1WGuf+gSglRTFec4/G0u7K+ivVW6cOt0wDmVrE6VFSgyJKcHh3\nlzpNLM7JGgEipnAoekaFnfb6SO569959AABY3xBFlkqK51myJRx2mUKqYyK0hJqYQp/b6OD8jZT2\nao3mcjzisiEJzXF3mEaNQvQFWWOt8XQJW5ltQVJ3wAWlZhUREYdKTVJfwpmWR+Ok0F10AoHMTYnA\nFSoCD6sFZaynqtSD8luD9yX7tCnMmKTquqdMXOG4voydpz+jdRyqRuohl2PQsVN1n/mTwwNX5QNK\nKUBnSmmKLIlg3N6GQl1KzzwKLYc0Hl2qdeU1bPAdUVP1zWeiWTzoYZqhu4/Pu0P7eDg9o3tj4h6l\nc+Gwrg4LDwf4vHBVU28mRVq0D9vR4X48jk9dtgqeEAyr9DwvzOBan18QnVy+VfnYBU9+Cv2CIREZ\nGBgYGBh8pjhjN5YEOoc9SFThekxCAzbpIe7uS3lEd4QWygL98ruOWBUxlYdUiB7dU8SVDhFIKi1S\n8o+ULiKXsVBlbxSLFfdsmzQ0V8W7WiBzp0S08FJTKrZ/4ed+HgAA5udwfHfv3ZExkIfMSfCS6hjQ\narI+JBEShH0Al5ZW4J3vyX4ugjRJoNc/BEdR/A8OcS58mu8okGOz9u3KCpYBVJVH/ZhIW3Uq7A8U\nucehhP3iEpYW+YpCH5OFz7XOlZp48FcuXQcAobEDiPdbKJJtVpFr3tnBUppeH+fWUySnTh+9nCEJ\nBzieqqwnsgaLMhR1KdJwDOl0+BUAcHKnB/2+ckRyK9ahNeYoe9QissshCUsc9IR4d3MFxT9aC4v0\nKt7AvU30PIckpLB8WXrv7t/BbZdqUo7SID3iWoEK1lVJQELztk8kje22jGGmxWv4OPGOIxKex51I\n5KTLaQLTROpYMKx44KoetFyOklFJQkF5Ly0a7zjjAn15hHGpRUiiB4Eqv2HvhYev7yl2QbkkzC/I\n8UISx4iV8ACXwmQ0177SX7VpLTDpJg5UuQ8N1SUCj53IrI+oi1FKHag8TbqB6SFNMwiCCNTU5BE/\n7mHrqmNfunQZAABmiLS4uLWZb+PuK/x7oL2xhHRoeyT8MlBlT9wH1MvnWb45IvEdfR8e5U+VlTZ4\nk8q+6iTu45fkucHiICySwGUtAAAB3V9MPtI61qfVwZ0cuYGBgYGBgcGpcSYPFMnFTm6ZAogVzh3H\nlxYX821sRfEvf6Akm9jJGY7RoxrHkgN1KZ49HFGfukOh7c/WsLTl2pWXAGBSgiykHnxrqnfn4gJa\nUE3qdlBV4gLXruA2jySiDpT3zLF1VvePVI4oIMuzQAXxqXKBCgXneW0iz4QMANIkkxYSajydDs7N\nXEvmm3O41SpaaPPz4tl0qINEM++uIlYW5zdYgIL7SAIA2JQbiMh0dwoyfxXqzeopy67d2aO9T0qA\nAQD4RCtPyYLXlm7eg5HKjjxfrMyDfVwbXIoDKne0t7uT9xW8KLj7TaLKC3JrNE+ayAkxDd8hD8Zz\nZE497jZEEmzP9iWnu3oTIyRdstL3VK/He08xxz0i6b/L83ItDsmbaSmxBJa5tCkKkqq5yEtp6L5Y\n35cozw2SBeROGoHq3JOxsAhdc1/nh+zpzDXDtVyY8WfzQngAgNjhkjPqEav4BxEJiXBzJN0dJ6Qu\nSBGLDqiOI1FeFkJelnpucE6ShzDoCucizw+GSgaOPEde94Hq3sI+VUZj0bKYvHZsiupY6t4oUilZ\nQNcjdVQUKLEgm1LHIdtxoFhtQrkqwjClCnNVaOzKrxrTM5s5CYGKPrpUxpJ3bFFeHK+nuoPP8mZL\nokb8u8DRj7bi1HQ5UqPON6F5KxDvY3ZWOBe550nX01FrlT1PPh7nV3EeKGpEUQt9nQoF33RjMTAw\nMDAw+CxhfkANDAwMDAzOgbM11HYsqNUL4LsSXmjWqFkvNUUdqrIFboTMjVZbsxJSzBwMy2zuok6u\n1hi9egWJE7aD4YJeX7nstXkaCw7dUXRjbijdaUuoiks7tjcw+a3VMuaI3LRPurAjNXZWHuJIghvL\ncZjyz9wG7uYAgCFWTTi4CBzbgVp1ZqJRrUNlFF7ezFZIOhyyYDKRJiJcvoxNxjkUrdn/3OGlR2Gn\nek3o3hUK7wyoCXaxIkumRCUUJdVQu0QavS5RzSPVjSKyhnQOBTqOEJJsOh/bIyJHSUJMZSqTaXew\nW87Orqgh1cqFfE6mAcuyTgzf8DtaozNv6kxrt6AUUupEdBjTffH0mahJxe9g958ZKvNSjUHg/mPU\nJb1Eak0tTzoEFazJ0gAAgJBKLkYUho9VuD9lAgyFijf2ZF9783gN5kgBKtXdMlhhib6nSWK2M61W\n2vkgAQbjfKz4Fh2fQnehGluU0r1GLBjHUYRGmgMOkboSlYYk4nMp0tdlo03Hi/gZpO6NjErIPEX4\nKTB5jMhVY7X+EtoHPwNcpfjD5SEOERNtV3Uqos5GFDEGW+WBoiic6CRyETiuB/W5S1BQaZoKPcPr\npAan9cV3OexvMWlT5s2yuUULvsaKmMjKTfMLuE9OKwHI7wF3ZRqr7xVIS93WnWqoEXmZ9qGJQhaF\nXss13FZWjbtZpYq7v1jqOlV4XxyG1qHfYmHid+V5MB6ogYGBgYHBOWBlZ9BYtCxrFwAef3bD+f8G\n17Ism//0jz0fZr5PDTPfnz/MnH++MPP9+eJU832mH1ADAwMDAwMDhAnhGhgYGBgYnAPmB9TAwMDA\nwOAcMD+gBgYGBgYG54D5ATUwMDAwMDgHzA+ogYGBgYHBOWB+QA0MDAwMDM4B8wNqYGBgYGBwDpgf\nUAMDAwMDg3PA/IAaGBgYGBicA/8XX+JaT2U4UocAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x216 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoG85OjsmwWI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_model_history(model_history):\n",
        "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
        "    # summarize history for accuracy\n",
        "    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n",
        "    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
        "    axs[0].set_title('Model Accuracy')\n",
        "    axs[0].set_ylabel('Accuracy')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
        "    axs[0].legend(['train', 'val'], loc='best')\n",
        "    # summarize history for loss\n",
        "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
        "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
        "    axs[1].set_title('Model Loss')\n",
        "    axs[1].set_ylabel('Loss')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
        "    axs[1].legend(['train', 'val'], loc='best')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IP5-SQIm1RZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def accuracy(test_x, test_y, model):\n",
        "    result = model.predict(test_x)\n",
        "    predicted_class = np.argmax(result, axis=1)\n",
        "    true_class = np.argmax(test_y, axis=1)\n",
        "    num_correct = np.sum(predicted_class == true_class) \n",
        "    accuracy = float(num_correct)/result.shape[0]\n",
        "    return (accuracy * 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OnsILhbm7Qp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_features = train_features.astype('float32')/255\n",
        "test_features = test_features.astype('float32')/255\n",
        "# convert class labels to binary class labels\n",
        "train_labels = np_utils.to_categorical(train_labels, num_classes)\n",
        "test_labels = np_utils.to_categorical(test_labels, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HM678yUN6qyV",
        "colab_type": "text"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JecAW26nBIT",
        "colab_type": "code",
        "outputId": "f595f01e-1da4-41a6-cc85-e2e4fb74d06e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Convolution2D(48, 3, 3, border_mode='same', input_shape=(32, 32, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Convolution2D(48, 3, 3))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Convolution2D(96, 3, 3, border_mode='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Convolution2D(96, 3, 3))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Convolution2D(192, 3, 3, border_mode='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Convolution2D(192, 3, 3))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0815 05:57:54.843187 139844136843136 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(48, (3, 3), input_shape=(32, 32, 3..., padding=\"same\")`\n",
            "  \n",
            "W0815 05:57:54.891857 139844136843136 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0815 05:57:54.899214 139844136843136 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(48, (3, 3))`\n",
            "  after removing the cwd from sys.path.\n",
            "W0815 05:57:54.949635 139844136843136 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0815 05:57:54.952914 139844136843136 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0815 05:57:54.967867 139844136843136 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(96, (3, 3), padding=\"same\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(96, (3, 3))`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(192, (3, 3), padding=\"same\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(192, (3, 3))`\n",
            "  app.launch_new_instance()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82j27WM1nKcx",
        "colab_type": "code",
        "outputId": "6f5c22e4-e0be-440b-9769-ad4beeaff3a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_13 (Conv2D)           (None, 32, 32, 48)        1344      \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 32, 32, 48)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 30, 30, 48)        20784     \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 30, 30, 48)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 15, 15, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 15, 15, 48)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 15, 15, 96)        41568     \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 15, 15, 96)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 13, 13, 96)        83040     \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 13, 13, 96)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 6, 6, 96)          0         \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 6, 6, 96)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 6, 6, 192)         166080    \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 6, 6, 192)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 4, 4, 192)         331968    \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 4, 4, 192)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 2, 2, 192)         0         \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 2, 2, 192)         0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 512)               393728    \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "activation_24 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 1,172,410\n",
            "Trainable params: 1,172,410\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghpNiY6F6uNX",
        "colab_type": "text"
      },
      "source": [
        "#Cutout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLtpPxksn16e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "    def eraser(input_img):\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "\n",
        "        if pixel_level:\n",
        "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "        input_img[top:top + h, left:left + w, :] = c\n",
        "\n",
        "        return input_img\n",
        "\n",
        "    return eraser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWYashokog6K",
        "colab_type": "code",
        "outputId": "f698781d-01e0-4ff4-e97d-d308857f2772",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(zoom_range=0.0, \n",
        "                             horizontal_flip=False,\n",
        "                             preprocessing_function=get_random_eraser(v_l=0, v_h=1,pixel_level=True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/preprocessing/image.py:440: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  image.ImageDataGenerator.__init__).args:\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTH-XXk91I1E",
        "colab_type": "code",
        "outputId": "dc4b213a-d690-42be-d065-d7c2185e56e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# load dataset\n",
        "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
        "\n",
        "\n",
        "print('Statistics train=%.3f (%.3f), test=%.3f (%.3f)' % (X_train.mean(), X_train.std(), X_test.mean(), X_test.std()))\n",
        "\n",
        "# create generator that centers pixel values\n",
        "datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
        "\n",
        "# calculate the mean on the training dataset\n",
        "datagen.fit(X_train)\n",
        "#print('Data Generator mean=%.3f, std=%.3f' % (datagen.mean, datagen.std))\n",
        "\n",
        "# demonstrate effect on a single batch of samples\n",
        "iterator = datagen.flow(X_train, Y_train, batch_size=128)\n",
        "\n",
        "# get a batch\n",
        "batchX, batchy = iterator.next()\n",
        "\n",
        "# pixel stats in the batch\n",
        "print(batchX.shape, batchX.mean(), batchX.std())\n",
        "\n",
        "# demonstrate effect on entire training dataset\n",
        "iterator = datagen.flow(X_train, Y_train, batch_size=len(X_train), shuffle=False)\n",
        "\n",
        "# get a batch\n",
        "batchX, batchy = iterator.next()\n",
        "\n",
        "# pixel stats in the batch\n",
        "print(batchX.shape, batchX.mean(), batchX.std())\n",
        "\n",
        "iterator1 = datagen.flow(X_test, Y_test, batch_size=len(X_test), shuffle=False)\n",
        "batch_testX, batch_testy = iterator1.next()\n",
        "\n",
        "X_train = batchX\n",
        "X_test = batch_testX\n",
        "\n",
        "Y_train=batchy\n",
        "Y_test=batch_testy\n",
        "\n",
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "Y_train = np_utils.to_categorical(Y_train, 10)\n",
        "Y_test = np_utils.to_categorical(Y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Statistics train=120.708 (64.150), test=121.529 (64.061)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/preprocessing/image.py:440: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  image.ImageDataGenerator.__init__).args:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(128, 32, 32, 3) 0.011583991 0.9846106\n",
            "(50000, 32, 32, 3) -1.6605131e-06 1.0000001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV21n_pw60vN",
        "colab_type": "text"
      },
      "source": [
        "#One cycle learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUh8cebqIZ6X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "from keras.callbacks import Callback\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "# Code is ported from https://github.com/fastai/fastai\n",
        "class OneCycleLR(Callback):\n",
        "    def __init__(self,\n",
        "                 max_lr,\n",
        "                 end_percentage=0.1,\n",
        "                 scale_percentage=None,\n",
        "                 maximum_momentum=0.95,\n",
        "                 minimum_momentum=0.85,\n",
        "                 verbose=True):\n",
        "        \"\"\" This callback implements a cyclical learning rate policy (CLR).\n",
        "        This is a special case of Cyclic Learning Rates, where we have only 1 cycle.\n",
        "        After the completion of 1 cycle, the learning rate will decrease rapidly to\n",
        "        100th its initial lowest value.\n",
        "        # Arguments:\n",
        "            max_lr: Float. Initial learning rate. This also sets the\n",
        "                starting learning rate (which will be 10x smaller than\n",
        "                this), and will increase to this value during the first cycle.\n",
        "            end_percentage: Float. The percentage of all the epochs of training\n",
        "                that will be dedicated to sharply decreasing the learning\n",
        "                rate after the completion of 1 cycle. Must be between 0 and 1.\n",
        "            scale_percentage: Float or None. If float, must be between 0 and 1.\n",
        "                If None, it will compute the scale_percentage automatically\n",
        "                based on the `end_percentage`.\n",
        "            maximum_momentum: Optional. Sets the maximum momentum (initial)\n",
        "                value, which gradually drops to its lowest value in half-cycle,\n",
        "                then gradually increases again to stay constant at this max value.\n",
        "                Can only be used with SGD Optimizer.\n",
        "            minimum_momentum: Optional. Sets the minimum momentum at the end of\n",
        "                the half-cycle. Can only be used with SGD Optimizer.\n",
        "            verbose: Bool. Whether to print the current learning rate after every\n",
        "                epoch.\n",
        "        # Reference\n",
        "            - [A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, weight_decay, and weight decay](https://arxiv.org/abs/1803.09820)\n",
        "            - [Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates](https://arxiv.org/abs/1708.07120)\n",
        "        \"\"\"\n",
        "        super(OneCycleLR, self).__init__()\n",
        "\n",
        "        if end_percentage < 0. or end_percentage > 1.:\n",
        "            raise ValueError(\"`end_percentage` must be between 0 and 1\")\n",
        "\n",
        "        if scale_percentage is not None and (scale_percentage < 0. or scale_percentage > 1.):\n",
        "            raise ValueError(\"`scale_percentage` must be between 0 and 1\")\n",
        "\n",
        "        self.initial_lr = max_lr\n",
        "        self.end_percentage = end_percentage\n",
        "        self.scale = float(scale_percentage) if scale_percentage is not None else float(end_percentage)\n",
        "        self.max_momentum = maximum_momentum\n",
        "        self.min_momentum = minimum_momentum\n",
        "        self.verbose = verbose\n",
        "\n",
        "        if self.max_momentum is not None and self.min_momentum is not None:\n",
        "            self._update_momentum = True\n",
        "        else:\n",
        "            self._update_momentum = False\n",
        "\n",
        "        self.clr_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "        self.epochs = None\n",
        "        self.batch_size = None\n",
        "        self.samples = None\n",
        "        self.steps = None\n",
        "        self.num_iterations = None\n",
        "        self.mid_cycle_id = None\n",
        "\n",
        "    def _reset(self):\n",
        "        \"\"\"\n",
        "        Reset the callback.\n",
        "        \"\"\"\n",
        "        self.clr_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "    def compute_lr(self):\n",
        "        \"\"\"\n",
        "        Compute the learning rate based on which phase of the cycle it is in.\n",
        "        - If in the first half of training, the learning rate gradually increases.\n",
        "        - If in the second half of training, the learning rate gradually decreases.\n",
        "        - If in the final `end_percentage` portion of training, the learning rate\n",
        "            is quickly reduced to near 100th of the original min learning rate.\n",
        "        # Returns:\n",
        "            the new learning rate\n",
        "        \"\"\"\n",
        "        if self.clr_iterations > 2 * self.mid_cycle_id:\n",
        "            current_percentage = (self.clr_iterations - 2 * self.mid_cycle_id)\n",
        "            current_percentage /= float((self.num_iterations - 2 * self.mid_cycle_id))\n",
        "            new_lr = self.initial_lr * (1. + (current_percentage *\n",
        "                                              (1. - 100.) / 100.)) * self.scale\n",
        "\n",
        "        elif self.clr_iterations > self.mid_cycle_id:\n",
        "            current_percentage = 1. - (\n",
        "                self.clr_iterations - self.mid_cycle_id) / self.mid_cycle_id\n",
        "            new_lr = self.initial_lr * (1. + current_percentage *\n",
        "                                        (self.scale * 100 - 1.)) * self.scale\n",
        "\n",
        "        else:\n",
        "            current_percentage = self.clr_iterations / self.mid_cycle_id\n",
        "            new_lr = self.initial_lr * (1. + current_percentage *\n",
        "                                        (self.scale * 100 - 1.)) * self.scale\n",
        "\n",
        "        if self.clr_iterations == self.num_iterations:\n",
        "            self.clr_iterations = 0\n",
        "\n",
        "        return new_lr\n",
        "\n",
        "    def compute_momentum(self):\n",
        "        \"\"\"\n",
        "         Compute the momentum based on which phase of the cycle it is in.\n",
        "        - If in the first half of training, the momentum gradually decreases.\n",
        "        - If in the second half of training, the momentum gradually increases.\n",
        "        - If in the final `end_percentage` portion of training, the momentum value\n",
        "            is kept constant at the maximum initial value.\n",
        "        # Returns:\n",
        "            the new momentum value\n",
        "        \"\"\"\n",
        "        if self.clr_iterations > 2 * self.mid_cycle_id:\n",
        "            new_momentum = self.max_momentum\n",
        "\n",
        "        elif self.clr_iterations > self.mid_cycle_id:\n",
        "            current_percentage = 1. - ((self.clr_iterations - self.mid_cycle_id) / float(\n",
        "                                        self.mid_cycle_id))\n",
        "            new_momentum = self.max_momentum - current_percentage * (\n",
        "                self.max_momentum - self.min_momentum)\n",
        "\n",
        "        else:\n",
        "            current_percentage = self.clr_iterations / float(self.mid_cycle_id)\n",
        "            new_momentum = self.max_momentum - current_percentage * (\n",
        "                self.max_momentum - self.min_momentum)\n",
        "\n",
        "        return new_momentum\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        logs = logs or {}\n",
        "\n",
        "        self.epochs = self.params['epochs']\n",
        "        self.batch_size = self.params['batch_size']\n",
        "        self.samples = self.params['samples']\n",
        "        self.steps = self.params['steps']\n",
        "\n",
        "        if self.steps is not None:\n",
        "            self.num_iterations = self.epochs * self.steps\n",
        "        else:\n",
        "            if (self.samples % self.batch_size) == 0:\n",
        "                remainder = 0\n",
        "            else:\n",
        "                remainder = 1\n",
        "            self.num_iterations = (self.epochs + remainder) * self.samples // self.batch_size\n",
        "\n",
        "        self.mid_cycle_id = int(self.num_iterations * ((1. - self.end_percentage)) / float(2))\n",
        "\n",
        "        self._reset()\n",
        "        K.set_value(self.model.optimizer.lr, self.compute_lr())\n",
        "\n",
        "        if self._update_momentum:\n",
        "            if not hasattr(self.model.optimizer, 'momentum'):\n",
        "                raise ValueError(\"Momentum can be updated only on SGD optimizer !\")\n",
        "\n",
        "            new_momentum = self.compute_momentum()\n",
        "            K.set_value(self.model.optimizer.momentum, new_momentum)\n",
        "\n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "\n",
        "        self.clr_iterations += 1\n",
        "        new_lr = self.compute_lr()\n",
        "\n",
        "        self.history.setdefault('lr', []).append(\n",
        "            K.get_value(self.model.optimizer.lr))\n",
        "        K.set_value(self.model.optimizer.lr, new_lr)\n",
        "\n",
        "        if self._update_momentum:\n",
        "            if not hasattr(self.model.optimizer, 'momentum'):\n",
        "                raise ValueError(\"Momentum can be updated only on SGD optimizer !\")\n",
        "\n",
        "            new_momentum = self.compute_momentum()\n",
        "\n",
        "            self.history.setdefault('momentum', []).append(\n",
        "                K.get_value(self.model.optimizer.momentum))\n",
        "            K.set_value(self.model.optimizer.momentum, new_momentum)\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if self.verbose:\n",
        "            if self._update_momentum:\n",
        "                print(\" - lr: %0.5f - momentum: %0.2f \" %\n",
        "                      (self.history['lr'][-1], self.history['momentum'][-1]))\n",
        "\n",
        "            else:\n",
        "                print(\" - lr: %0.5f \" % (self.history['lr'][-1]))\n",
        "\n",
        "\n",
        "class LRFinder(Callback):\n",
        "    def __init__(self,\n",
        "                 num_samples,\n",
        "                 batch_size,\n",
        "                 minimum_lr=1e-5,\n",
        "                 maximum_lr=10.,\n",
        "                 lr_scale='exp',\n",
        "                 validation_data=None,\n",
        "                 validation_sample_rate=5,\n",
        "                 stopping_criterion_factor=4.,\n",
        "                 loss_smoothing_beta=0.98,\n",
        "                 save_dir=None,\n",
        "                 verbose=True):\n",
        "        \"\"\"\n",
        "        This class uses the Cyclic Learning Rate history to find a\n",
        "        set of learning rates that can be good initializations for the\n",
        "        One-Cycle training proposed by Leslie Smith in the paper referenced\n",
        "        below.\n",
        "        A port of the Fast.ai implementation for Keras.\n",
        "        # Note\n",
        "        This requires that the model be trained for exactly 1 epoch. If the model\n",
        "        is trained for more epochs, then the metric calculations are only done for\n",
        "        the first epoch.\n",
        "        # Interpretation\n",
        "        Upon visualizing the loss plot, check where the loss starts to increase\n",
        "        rapidly. Choose a learning rate at somewhat prior to the corresponding\n",
        "        position in the plot for faster convergence. This will be the maximum_lr lr.\n",
        "        Choose the max value as this value when passing the `max_val` argument\n",
        "        to OneCycleLR callback.\n",
        "        Since the plot is in log-scale, you need to compute 10 ^ (-k) of the x-axis\n",
        "        # Arguments:\n",
        "            num_samples: Integer. Number of samples in the dataset.\n",
        "            batch_size: Integer. Batch size during training.\n",
        "            minimum_lr: Float. Initial learning rate (and the minimum).\n",
        "            maximum_lr: Float. Final learning rate (and the maximum).\n",
        "            lr_scale: Can be one of ['exp', 'linear']. Chooses the type of\n",
        "                scaling for each update to the learning rate during subsequent\n",
        "                batches. Choose 'exp' for large range and 'linear' for small range.\n",
        "            validation_data: Requires the validation dataset as a tuple of\n",
        "                (X, y) belonging to the validation set. If provided, will use the\n",
        "                validation set to compute the loss metrics. Else uses the training\n",
        "                batch loss. Will warn if not provided to alert the user.\n",
        "            validation_sample_rate: Positive or Negative Integer. Number of batches to sample from the\n",
        "                validation set per iteration of the LRFinder. Larger number of\n",
        "                samples will reduce the variance but will take longer time to execute\n",
        "                per batch.\n",
        "                If Positive > 0, will sample from the validation dataset\n",
        "                If Megative, will use the entire dataset\n",
        "            stopping_criterion_factor: Integer or None. A factor which is used\n",
        "                to measure large increase in the loss value during training.\n",
        "                Since callbacks cannot stop training of a model, it will simply\n",
        "                stop logging the additional values from the epochs after this\n",
        "                stopping criterion has been met.\n",
        "                If None, this check will not be performed.\n",
        "            loss_smoothing_beta: Float. The smoothing factor for the moving\n",
        "                average of the loss function.\n",
        "            save_dir: Optional, String. If passed a directory path, the callback\n",
        "                will save the running loss and learning rates to two separate numpy\n",
        "                arrays inside this directory. If the directory in this path does not\n",
        "                exist, they will be created.\n",
        "            verbose: Whether to print the learning rate after every batch of training.\n",
        "        # References:\n",
        "            - [A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, weight_decay, and weight decay](https://arxiv.org/abs/1803.09820)\n",
        "        \"\"\"\n",
        "        super(LRFinder, self).__init__()\n",
        "\n",
        "        if lr_scale not in ['exp', 'linear']:\n",
        "            raise ValueError(\"`lr_scale` must be one of ['exp', 'linear']\")\n",
        "\n",
        "        if validation_data is not None:\n",
        "            self.validation_data = validation_data\n",
        "            self.use_validation_set = True\n",
        "\n",
        "            if validation_sample_rate > 0 or validation_sample_rate < 0:\n",
        "                self.validation_sample_rate = validation_sample_rate\n",
        "            else:\n",
        "                raise ValueError(\"`validation_sample_rate` must be a positive or negative integer other than o\")\n",
        "        else:\n",
        "            self.use_validation_set = False\n",
        "            self.validation_sample_rate = 0\n",
        "\n",
        "        self.num_samples = num_samples\n",
        "        self.batch_size = batch_size\n",
        "        self.initial_lr = minimum_lr\n",
        "        self.final_lr = maximum_lr\n",
        "        self.lr_scale = lr_scale\n",
        "        self.stopping_criterion_factor = stopping_criterion_factor\n",
        "        self.loss_smoothing_beta = loss_smoothing_beta\n",
        "        self.save_dir = save_dir\n",
        "        self.verbose = verbose\n",
        "\n",
        "        self.num_batches_ = num_samples // batch_size\n",
        "        self.current_lr_ = minimum_lr\n",
        "\n",
        "        if lr_scale == 'exp':\n",
        "            self.lr_multiplier_ = (maximum_lr / float(minimum_lr)) ** (\n",
        "                1. / float(self.num_batches_))\n",
        "        else:\n",
        "            extra_batch = int((num_samples % batch_size) != 0)\n",
        "            self.lr_multiplier_ = np.linspace(\n",
        "                minimum_lr, maximum_lr, num=self.num_batches_ + extra_batch)\n",
        "\n",
        "        # If negative, use entire validation set\n",
        "        if self.validation_sample_rate < 0:\n",
        "            self.validation_sample_rate = self.validation_data[0].shape[0] // batch_size\n",
        "\n",
        "        self.current_batch_ = 0\n",
        "        self.current_epoch_ = 0\n",
        "        self.best_loss_ = 1e6\n",
        "        self.running_loss_ = 0.\n",
        "\n",
        "        self.history = {}\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "\n",
        "        self.current_epoch_ = 1\n",
        "        K.set_value(self.model.optimizer.lr, self.initial_lr)\n",
        "\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        self.current_batch_ = 0\n",
        "\n",
        "        if self.current_epoch_ > 1:\n",
        "            warnings.warn(\n",
        "                \"\\n\\nLearning rate finder should be used only with a single epoch. \"\n",
        "                \"Hereafter, the callback will not measure the losses.\\n\\n\")\n",
        "\n",
        "    def on_batch_begin(self, batch, logs=None):\n",
        "        self.current_batch_ += 1\n",
        "\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        if self.current_epoch_ > 1:\n",
        "            return\n",
        "\n",
        "        if self.use_validation_set:\n",
        "            X, Y = self.validation_data[0], self.validation_data[1]\n",
        "\n",
        "            # use 5 random batches from test set for fast approximate of loss\n",
        "            num_samples = self.batch_size * self.validation_sample_rate\n",
        "\n",
        "            if num_samples > X.shape[0]:\n",
        "                num_samples = X.shape[0]\n",
        "\n",
        "            idx = np.random.choice(X.shape[0], num_samples, replace=False)\n",
        "            x = X[idx]\n",
        "            y = Y[idx]\n",
        "\n",
        "            values = self.model.evaluate(x, y, batch_size=self.batch_size, verbose=False)\n",
        "            loss = values[0]\n",
        "        else:\n",
        "            loss = logs['loss']\n",
        "\n",
        "        # smooth the loss value and bias correct\n",
        "        running_loss = self.loss_smoothing_beta * loss + (\n",
        "            1. - self.loss_smoothing_beta) * loss\n",
        "        running_loss = running_loss / (\n",
        "            1. - self.loss_smoothing_beta**self.current_batch_)\n",
        "\n",
        "        # stop logging if loss is too large\n",
        "        if self.current_batch_ > 1 and self.stopping_criterion_factor is not None and (\n",
        "                running_loss >\n",
        "                self.stopping_criterion_factor * self.best_loss_):\n",
        "\n",
        "            if self.verbose:\n",
        "                print(\" - LRFinder: Skipping iteration since loss is %d times as large as best loss (%0.4f)\"\n",
        "                      % (self.stopping_criterion_factor, self.best_loss_))\n",
        "            return\n",
        "\n",
        "        if running_loss < self.best_loss_ or self.current_batch_ == 1:\n",
        "            self.best_loss_ = running_loss\n",
        "\n",
        "        current_lr = K.get_value(self.model.optimizer.lr)\n",
        "\n",
        "        self.history.setdefault('running_loss_', []).append(running_loss)\n",
        "        if self.lr_scale == 'exp':\n",
        "            self.history.setdefault('log_lrs', []).append(np.log10(current_lr))\n",
        "        else:\n",
        "            self.history.setdefault('log_lrs', []).append(current_lr)\n",
        "\n",
        "        # compute the lr for the next batch and update the optimizer lr\n",
        "        if self.lr_scale == 'exp':\n",
        "            current_lr *= self.lr_multiplier_\n",
        "        else:\n",
        "            current_lr = self.lr_multiplier_[self.current_batch_ - 1]\n",
        "\n",
        "        K.set_value(self.model.optimizer.lr, current_lr)\n",
        "\n",
        "        # save the other metrics as well\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "\n",
        "        if self.verbose:\n",
        "            if self.use_validation_set:\n",
        "                print(\" - LRFinder: val_loss: %1.4f - lr = %1.8f \" %\n",
        "                      (values[0], current_lr))\n",
        "            else:\n",
        "                print(\" - LRFinder: lr = %1.8f \" % current_lr)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if self.save_dir is not None and self.current_epoch_ <= 1:\n",
        "            if not os.path.exists(self.save_dir):\n",
        "                os.makedirs(self.save_dir)\n",
        "\n",
        "            losses_path = os.path.join(self.save_dir, 'losses.npy')\n",
        "            lrs_path = os.path.join(self.save_dir, 'lrs.npy')\n",
        "\n",
        "            np.save(losses_path, self.losses)\n",
        "            np.save(lrs_path, self.lrs)\n",
        "\n",
        "            if self.verbose:\n",
        "                print(\"\\tLR Finder : Saved the losses and learning rate values in path : {%s}\"\n",
        "                      % (self.save_dir))\n",
        "\n",
        "        self.current_epoch_ += 1\n",
        "\n",
        "        warnings.simplefilter(\"default\")\n",
        "\n",
        "    def plot_schedule(self, clip_beginning=None, clip_endding=None):\n",
        "        \"\"\"\n",
        "        Plots the schedule from the callback itself.\n",
        "        # Arguments:\n",
        "            clip_beginning: Integer or None. If positive integer, it will\n",
        "                remove the specified portion of the loss graph to remove the large\n",
        "                loss values in the beginning of the graph.\n",
        "            clip_endding: Integer or None. If negative integer, it will\n",
        "                remove the specified portion of the ending of the loss graph to\n",
        "                remove the sharp increase in the loss values at high learning rates.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            import matplotlib.pyplot as plt\n",
        "            plt.style.use('seaborn-white')\n",
        "        except ImportError:\n",
        "            print(\n",
        "                \"Matplotlib not found. Please use `pip install matplotlib` first.\"\n",
        "            )\n",
        "            return\n",
        "\n",
        "        if clip_beginning is not None and clip_beginning < 0:\n",
        "            clip_beginning = -clip_beginning\n",
        "\n",
        "        if clip_endding is not None and clip_endding > 0:\n",
        "            clip_endding = -clip_endding\n",
        "\n",
        "        losses = self.losses\n",
        "        lrs = self.lrs\n",
        "\n",
        "        if clip_beginning:\n",
        "            losses = losses[clip_beginning:]\n",
        "            lrs = lrs[clip_beginning:]\n",
        "\n",
        "        if clip_endding:\n",
        "            losses = losses[:clip_endding]\n",
        "            lrs = lrs[:clip_endding]\n",
        "\n",
        "        plt.plot(lrs, losses)\n",
        "        plt.title('Learning rate vs Loss')\n",
        "        plt.xlabel('learning rate')\n",
        "        plt.ylabel('loss')\n",
        "        plt.show()\n",
        "\n",
        "    @classmethod\n",
        "    def restore_schedule_from_dir(cls,\n",
        "                                  directory,\n",
        "                                  clip_beginning=None,\n",
        "                                  clip_endding=None):\n",
        "        \"\"\"\n",
        "        Loads the training history from the saved numpy files in the given directory.\n",
        "        # Arguments:\n",
        "            directory: String. Path to the directory where the serialized numpy\n",
        "                arrays of the loss and learning rates are saved.\n",
        "            clip_beginning: Integer or None. If positive integer, it will\n",
        "                remove the specified portion of the loss graph to remove the large\n",
        "                loss values in the beginning of the graph.\n",
        "            clip_endding: Integer or None. If negative integer, it will\n",
        "                remove the specified portion of the ending of the loss graph to\n",
        "                remove the sharp increase in the loss values at high learning rates.\n",
        "        Returns:\n",
        "            tuple of (losses, learning rates)\n",
        "        \"\"\"\n",
        "        if clip_beginning is not None and clip_beginning < 0:\n",
        "            clip_beginning = -clip_beginning\n",
        "\n",
        "        if clip_endding is not None and clip_endding > 0:\n",
        "            clip_endding = -clip_endding\n",
        "\n",
        "        losses_path = os.path.join(directory, 'losses.npy')\n",
        "        lrs_path = os.path.join(directory, 'lrs.npy')\n",
        "\n",
        "        if not os.path.exists(losses_path) or not os.path.exists(lrs_path):\n",
        "            print(\"%s and %s could not be found at directory : {%s}\" %\n",
        "                  (losses_path, lrs_path, directory))\n",
        "\n",
        "            losses = None\n",
        "            lrs = None\n",
        "\n",
        "        else:\n",
        "            losses = np.load(losses_path)\n",
        "            lrs = np.load(lrs_path)\n",
        "\n",
        "            if clip_beginning:\n",
        "                losses = losses[clip_beginning:]\n",
        "                lrs = lrs[clip_beginning:]\n",
        "\n",
        "            if clip_endding:\n",
        "                losses = losses[:clip_endding]\n",
        "                lrs = lrs[:clip_endding]\n",
        "\n",
        "        return losses, lrs\n",
        "\n",
        "    @classmethod\n",
        "    def plot_schedule_from_file(cls,\n",
        "                                directory,\n",
        "                                clip_beginning=None,\n",
        "                                clip_endding=None):\n",
        "        \"\"\"\n",
        "        Plots the schedule from the saved numpy arrays of the loss and learning\n",
        "        rate values in the specified directory.\n",
        "        # Arguments:\n",
        "            directory: String. Path to the directory where the serialized numpy\n",
        "                arrays of the loss and learning rates are saved.\n",
        "            clip_beginning: Integer or None. If positive integer, it will\n",
        "                remove the specified portion of the loss graph to remove the large\n",
        "                loss values in the beginning of the graph.\n",
        "            clip_endding: Integer or None. If negative integer, it will\n",
        "                remove the specified portion of the ending of the loss graph to\n",
        "                remove the sharp increase in the loss values at high learning rates.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            import matplotlib.pyplot as plt\n",
        "            plt.style.use('seaborn-white')\n",
        "        except ImportError:\n",
        "            print(\"Matplotlib not found. Please use `pip install matplotlib` first.\")\n",
        "            return\n",
        "\n",
        "        losses, lrs = cls.restore_schedule_from_dir(\n",
        "            directory,\n",
        "            clip_beginning=clip_beginning,\n",
        "            clip_endding=clip_endding)\n",
        "\n",
        "        if losses is None or lrs is None:\n",
        "            return\n",
        "        else:\n",
        "            plt.plot(lrs, losses)\n",
        "            plt.title('Learning rate vs Loss')\n",
        "            plt.xlabel('learning rate')\n",
        "            plt.ylabel('loss')\n",
        "            plt.show()\n",
        "\n",
        "    @property\n",
        "    def lrs(self):\n",
        "        return np.array(self.history['log_lrs'])\n",
        "\n",
        "    @property\n",
        "    def losses(self):\n",
        "        return np.array(self.history['running_loss_'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_gbQhmPPxsW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from clr import LRFinder\n",
        "num_samples = X_train.shape[0]\n",
        "batch_size = 128\n",
        "nb_classes = 10\n",
        "nb_epoch = 1  # Only finding lr\n",
        "data_augmentation = True\n",
        "\n",
        "lr_finder = LRFinder(num_samples, batch_size, minimum_lr=1e-3, maximum_lr=10.,\n",
        "                     lr_scale='exp',\n",
        "                     validation_data=(X_test, Y_test),  # use the validation data for losses\n",
        "                     validation_sample_rate=5,\n",
        "                     save_dir='/content/drive/My Drive/Colab Notebooks/12/', verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LV069o19P8ON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "weights_file = '/content/drive/My Drive/Colab Notebooks/12/lrs.npy'\n",
        "model_checkpoint = ModelCheckpoint(weights_file, monitor='val_acc', save_best_only=True,\n",
        "                                   save_weights_only=True, mode='max')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-5RU8ZJQFRx",
        "colab_type": "code",
        "outputId": "0b98e35f-bd18-488d-cde1-0c7dc45f84cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size, shuffle=True),\n",
        "                        steps_per_epoch=X_train.shape[0] // batch_size,\n",
        "                        validation_data=(X_test, Y_test),\n",
        "                        epochs=nb_epoch, verbose=1,\n",
        "                        callbacks=[lr_finder, model_checkpoint])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "  1/390 [..............................] - ETA: 2:21 - loss: 2.3602 - acc: 0.1328 - LRFinder: val_loss: 2.3061 - lr = 0.00102390 \n",
            "  2/390 [..............................] - ETA: 2:09 - loss: 2.3376 - acc: 0.1289 - LRFinder: val_loss: 2.3037 - lr = 0.00104837 \n",
            "  3/390 [..............................] - ETA: 1:36 - loss: 2.3345 - acc: 0.1198 - LRFinder: val_loss: 2.3048 - lr = 0.00107342 \n",
            "  4/390 [..............................] - ETA: 1:19 - loss: 2.3338 - acc: 0.1133 - LRFinder: val_loss: 2.3028 - lr = 0.00109907 \n",
            "  5/390 [..............................] - ETA: 1:08 - loss: 2.3354 - acc: 0.1125 - LRFinder: val_loss: 2.3049 - lr = 0.00112534 \n",
            "  6/390 [..............................] - ETA: 1:00 - loss: 2.3365 - acc: 0.1185 - LRFinder: val_loss: 2.3020 - lr = 0.00115223 \n",
            "  7/390 [..............................] - ETA: 55s - loss: 2.3365 - acc: 0.1161  - LRFinder: val_loss: 2.3055 - lr = 0.00117976 \n",
            "  8/390 [..............................] - ETA: 51s - loss: 2.3332 - acc: 0.1133 - LRFinder: val_loss: 2.3077 - lr = 0.00120796 \n",
            "  9/390 [..............................] - ETA: 47s - loss: 2.3303 - acc: 0.1120 - LRFinder: val_loss: 2.3034 - lr = 0.00123682 \n",
            " 10/390 [..............................] - ETA: 45s - loss: 2.3349 - acc: 0.1070 - LRFinder: val_loss: 2.3050 - lr = 0.00126638 \n",
            " 11/390 [..............................] - ETA: 43s - loss: 2.3363 - acc: 0.1037 - LRFinder: val_loss: 2.3018 - lr = 0.00129664 \n",
            " 12/390 [..............................] - ETA: 41s - loss: 2.3321 - acc: 0.1081 - LRFinder: val_loss: 2.3020 - lr = 0.00132763 \n",
            " 13/390 [>.............................] - ETA: 39s - loss: 2.3320 - acc: 0.1070 - LRFinder: val_loss: 2.3038 - lr = 0.00135936 \n",
            " 14/390 [>.............................] - ETA: 38s - loss: 2.3331 - acc: 0.1055 - LRFinder: val_loss: 2.3015 - lr = 0.00139184 \n",
            " 15/390 [>.............................] - ETA: 37s - loss: 2.3346 - acc: 0.1026 - LRFinder: val_loss: 2.3048 - lr = 0.00142510 \n",
            " 16/390 [>.............................] - ETA: 36s - loss: 2.3348 - acc: 0.1011 - LRFinder: val_loss: 2.2998 - lr = 0.00145916 \n",
            " 17/390 [>.............................] - ETA: 35s - loss: 2.3346 - acc: 0.1002 - LRFinder: val_loss: 2.3027 - lr = 0.00149403 \n",
            " 18/390 [>.............................] - ETA: 34s - loss: 2.3362 - acc: 0.0998 - LRFinder: val_loss: 2.3031 - lr = 0.00152973 \n",
            " 19/390 [>.............................] - ETA: 34s - loss: 2.3362 - acc: 0.0991 - LRFinder: val_loss: 2.3025 - lr = 0.00156629 \n",
            " 20/390 [>.............................] - ETA: 33s - loss: 2.3354 - acc: 0.0996 - LRFinder: val_loss: 2.3067 - lr = 0.00160372 \n",
            " 21/390 [>.............................] - ETA: 32s - loss: 2.3365 - acc: 0.0990 - LRFinder: val_loss: 2.3044 - lr = 0.00164204 \n",
            " 22/390 [>.............................] - ETA: 32s - loss: 2.3365 - acc: 0.0994 - LRFinder: val_loss: 2.3055 - lr = 0.00168128 \n",
            " 23/390 [>.............................] - ETA: 31s - loss: 2.3357 - acc: 0.1005 - LRFinder: val_loss: 2.2995 - lr = 0.00172146 \n",
            " 24/390 [>.............................] - ETA: 31s - loss: 2.3358 - acc: 0.0990 - LRFinder: val_loss: 2.3029 - lr = 0.00176260 \n",
            " 25/390 [>.............................] - ETA: 30s - loss: 2.3349 - acc: 0.0981 - LRFinder: val_loss: 2.3039 - lr = 0.00180472 \n",
            " 26/390 [=>............................] - ETA: 30s - loss: 2.3357 - acc: 0.0989 - LRFinder: val_loss: 2.3043 - lr = 0.00184785 \n",
            " 27/390 [=>............................] - ETA: 29s - loss: 2.3364 - acc: 0.0975 - LRFinder: val_loss: 2.3038 - lr = 0.00189201 \n",
            " 28/390 [=>............................] - ETA: 29s - loss: 2.3350 - acc: 0.0999 - LRFinder: val_loss: 2.3040 - lr = 0.00193722 \n",
            " 29/390 [=>............................] - ETA: 29s - loss: 2.3335 - acc: 0.1008 - LRFinder: val_loss: 2.3055 - lr = 0.00198352 \n",
            " 30/390 [=>............................] - ETA: 28s - loss: 2.3324 - acc: 0.0990 - LRFinder: val_loss: 2.3032 - lr = 0.00203092 \n",
            " 31/390 [=>............................] - ETA: 28s - loss: 2.3311 - acc: 0.0995 - LRFinder: val_loss: 2.3016 - lr = 0.00207945 \n",
            " 32/390 [=>............................] - ETA: 28s - loss: 2.3308 - acc: 0.0994 - LRFinder: val_loss: 2.3008 - lr = 0.00212914 \n",
            " 33/390 [=>............................] - ETA: 27s - loss: 2.3314 - acc: 0.0985 - LRFinder: val_loss: 2.3057 - lr = 0.00218003 \n",
            " 34/390 [=>............................] - ETA: 27s - loss: 2.3307 - acc: 0.0983 - LRFinder: val_loss: 2.3027 - lr = 0.00223212 \n",
            " 35/390 [=>............................] - ETA: 27s - loss: 2.3312 - acc: 0.0969 - LRFinder: val_loss: 2.3034 - lr = 0.00228546 \n",
            " 36/390 [=>............................] - ETA: 27s - loss: 2.3314 - acc: 0.0968 - LRFinder: val_loss: 2.3049 - lr = 0.00234008 \n",
            " 37/390 [=>............................] - ETA: 26s - loss: 2.3317 - acc: 0.0959 - LRFinder: val_loss: 2.3035 - lr = 0.00239600 \n",
            " 38/390 [=>............................] - ETA: 26s - loss: 2.3316 - acc: 0.0960 - LRFinder: val_loss: 2.3026 - lr = 0.00245326 \n",
            " 39/390 [==>...........................] - ETA: 26s - loss: 2.3310 - acc: 0.0956 - LRFinder: val_loss: 2.3055 - lr = 0.00251189 \n",
            " 40/390 [==>...........................] - ETA: 26s - loss: 2.3309 - acc: 0.0951 - LRFinder: val_loss: 2.3046 - lr = 0.00257191 \n",
            " 41/390 [==>...........................] - ETA: 26s - loss: 2.3306 - acc: 0.0960 - LRFinder: val_loss: 2.3018 - lr = 0.00263338 \n",
            " 42/390 [==>...........................] - ETA: 25s - loss: 2.3310 - acc: 0.0960 - LRFinder: val_loss: 2.3039 - lr = 0.00269631 \n",
            " 43/390 [==>...........................] - ETA: 25s - loss: 2.3308 - acc: 0.0968 - LRFinder: val_loss: 2.3014 - lr = 0.00276074 \n",
            " 44/390 [==>...........................] - ETA: 25s - loss: 2.3304 - acc: 0.0971 - LRFinder: val_loss: 2.3007 - lr = 0.00282672 \n",
            " 45/390 [==>...........................] - ETA: 25s - loss: 2.3300 - acc: 0.0967 - LRFinder: val_loss: 2.3038 - lr = 0.00289427 \n",
            " 46/390 [==>...........................] - ETA: 25s - loss: 2.3295 - acc: 0.0966 - LRFinder: val_loss: 2.3044 - lr = 0.00296343 \n",
            " 47/390 [==>...........................] - ETA: 25s - loss: 2.3295 - acc: 0.0967 - LRFinder: val_loss: 2.3024 - lr = 0.00303425 \n",
            " 48/390 [==>...........................] - ETA: 24s - loss: 2.3290 - acc: 0.0980 - LRFinder: val_loss: 2.3050 - lr = 0.00310676 \n",
            " 49/390 [==>...........................] - ETA: 24s - loss: 2.3288 - acc: 0.0977 - LRFinder: val_loss: 2.3062 - lr = 0.00318100 \n",
            " 50/390 [==>...........................] - ETA: 24s - loss: 2.3284 - acc: 0.0975 - LRFinder: val_loss: 2.3040 - lr = 0.00325702 \n",
            " 51/390 [==>...........................] - ETA: 24s - loss: 2.3283 - acc: 0.0971 - LRFinder: val_loss: 2.3054 - lr = 0.00333485 \n",
            " 52/390 [===>..........................] - ETA: 24s - loss: 2.3284 - acc: 0.0968 - LRFinder: val_loss: 2.3048 - lr = 0.00341455 \n",
            " 53/390 [===>..........................] - ETA: 24s - loss: 2.3285 - acc: 0.0961 - LRFinder: val_loss: 2.3052 - lr = 0.00349615 \n",
            " 54/390 [===>..........................] - ETA: 24s - loss: 2.3278 - acc: 0.0971 - LRFinder: val_loss: 2.3037 - lr = 0.00357970 \n",
            " 55/390 [===>..........................] - ETA: 23s - loss: 2.3278 - acc: 0.0966 - LRFinder: val_loss: 2.3031 - lr = 0.00366524 \n",
            " 56/390 [===>..........................] - ETA: 23s - loss: 2.3276 - acc: 0.0970 - LRFinder: val_loss: 2.3022 - lr = 0.00375283 \n",
            " 57/390 [===>..........................] - ETA: 23s - loss: 2.3274 - acc: 0.0980 - LRFinder: val_loss: 2.3023 - lr = 0.00384251 \n",
            " 58/390 [===>..........................] - ETA: 23s - loss: 2.3270 - acc: 0.0979 - LRFinder: val_loss: 2.3022 - lr = 0.00393434 \n",
            " 59/390 [===>..........................] - ETA: 23s - loss: 2.3267 - acc: 0.0981 - LRFinder: val_loss: 2.3009 - lr = 0.00402836 \n",
            " 60/390 [===>..........................] - ETA: 23s - loss: 2.3264 - acc: 0.0984 - LRFinder: val_loss: 2.3017 - lr = 0.00412463 \n",
            " 61/390 [===>..........................] - ETA: 23s - loss: 2.3260 - acc: 0.0991 - LRFinder: val_loss: 2.3013 - lr = 0.00422319 \n",
            " 62/390 [===>..........................] - ETA: 22s - loss: 2.3253 - acc: 0.0997 - LRFinder: val_loss: 2.3018 - lr = 0.00432412 \n",
            " 63/390 [===>..........................] - ETA: 22s - loss: 2.3252 - acc: 0.0996 - LRFinder: val_loss: 2.3043 - lr = 0.00442745 \n",
            " 64/390 [===>..........................] - ETA: 22s - loss: 2.3253 - acc: 0.0989 - LRFinder: val_loss: 2.3018 - lr = 0.00453326 \n",
            " 65/390 [====>.........................] - ETA: 22s - loss: 2.3248 - acc: 0.0999 - LRFinder: val_loss: 2.3050 - lr = 0.00464159 \n",
            " 66/390 [====>.........................] - ETA: 22s - loss: 2.3246 - acc: 0.1000 - LRFinder: val_loss: 2.3016 - lr = 0.00475251 \n",
            " 67/390 [====>.........................] - ETA: 22s - loss: 2.3248 - acc: 0.0996 - LRFinder: val_loss: 2.3038 - lr = 0.00486608 \n",
            " 68/390 [====>.........................] - ETA: 22s - loss: 2.3249 - acc: 0.0996 - LRFinder: val_loss: 2.3039 - lr = 0.00498237 \n",
            " 69/390 [====>.........................] - ETA: 22s - loss: 2.3245 - acc: 0.1002 - LRFinder: val_loss: 2.3039 - lr = 0.00510143 \n",
            " 70/390 [====>.........................] - ETA: 22s - loss: 2.3242 - acc: 0.0999 - LRFinder: val_loss: 2.3030 - lr = 0.00522335 \n",
            " 71/390 [====>.........................] - ETA: 21s - loss: 2.3244 - acc: 0.0995 - LRFinder: val_loss: 2.3027 - lr = 0.00534817 \n",
            " 72/390 [====>.........................] - ETA: 21s - loss: 2.3243 - acc: 0.0996 - LRFinder: val_loss: 2.3044 - lr = 0.00547598 \n",
            " 73/390 [====>.........................] - ETA: 21s - loss: 2.3245 - acc: 0.0996 - LRFinder: val_loss: 2.3031 - lr = 0.00560684 \n",
            " 74/390 [====>.........................] - ETA: 21s - loss: 2.3243 - acc: 0.0996 - LRFinder: val_loss: 2.3022 - lr = 0.00574083 \n",
            " 75/390 [====>.........................] - ETA: 21s - loss: 2.3239 - acc: 0.1000 - LRFinder: val_loss: 2.3037 - lr = 0.00587802 \n",
            " 76/390 [====>.........................] - ETA: 21s - loss: 2.3235 - acc: 0.1004 - LRFinder: val_loss: 2.3051 - lr = 0.00601849 \n",
            " 77/390 [====>.........................] - ETA: 21s - loss: 2.3236 - acc: 0.1001 - LRFinder: val_loss: 2.3034 - lr = 0.00616231 \n",
            " 78/390 [=====>........................] - ETA: 21s - loss: 2.3232 - acc: 0.1003 - LRFinder: val_loss: 2.3028 - lr = 0.00630957 \n",
            " 79/390 [=====>........................] - ETA: 21s - loss: 2.3229 - acc: 0.1012 - LRFinder: val_loss: 2.3035 - lr = 0.00646036 \n",
            " 80/390 [=====>........................] - ETA: 20s - loss: 2.3225 - acc: 0.1016 - LRFinder: val_loss: 2.3010 - lr = 0.00661474 \n",
            " 81/390 [=====>........................] - ETA: 20s - loss: 2.3224 - acc: 0.1015 - LRFinder: val_loss: 2.3035 - lr = 0.00677282 \n",
            " 82/390 [=====>........................] - ETA: 20s - loss: 2.3222 - acc: 0.1013 - LRFinder: val_loss: 2.3018 - lr = 0.00693467 \n",
            " 83/390 [=====>........................] - ETA: 20s - loss: 2.3221 - acc: 0.1008 - LRFinder: val_loss: 2.3031 - lr = 0.00710039 \n",
            " 84/390 [=====>........................] - ETA: 20s - loss: 2.3218 - acc: 0.1008 - LRFinder: val_loss: 2.3043 - lr = 0.00727007 \n",
            " 85/390 [=====>........................] - ETA: 20s - loss: 2.3216 - acc: 0.1009 - LRFinder: val_loss: 2.3048 - lr = 0.00744380 \n",
            " 86/390 [=====>........................] - ETA: 20s - loss: 2.3216 - acc: 0.1006 - LRFinder: val_loss: 2.3037 - lr = 0.00762169 \n",
            " 87/390 [=====>........................] - ETA: 20s - loss: 2.3214 - acc: 0.1004 - LRFinder: val_loss: 2.3016 - lr = 0.00780383 \n",
            " 88/390 [=====>........................] - ETA: 20s - loss: 2.3212 - acc: 0.1005 - LRFinder: val_loss: 2.3018 - lr = 0.00799032 \n",
            " 89/390 [=====>........................] - ETA: 20s - loss: 2.3210 - acc: 0.1003 - LRFinder: val_loss: 2.3047 - lr = 0.00818127 \n",
            " 90/390 [=====>........................] - ETA: 20s - loss: 2.3211 - acc: 0.1002 - LRFinder: val_loss: 2.3018 - lr = 0.00837678 \n",
            " 91/390 [======>.......................] - ETA: 19s - loss: 2.3209 - acc: 0.1002 - LRFinder: val_loss: 2.3019 - lr = 0.00857696 \n",
            " 92/390 [======>.......................] - ETA: 19s - loss: 2.3206 - acc: 0.1002 - LRFinder: val_loss: 2.3031 - lr = 0.00878193 \n",
            " 93/390 [======>.......................] - ETA: 19s - loss: 2.3205 - acc: 0.1002 - LRFinder: val_loss: 2.3037 - lr = 0.00899179 \n",
            " 94/390 [======>.......................] - ETA: 19s - loss: 2.3206 - acc: 0.0998 - LRFinder: val_loss: 2.3035 - lr = 0.00920667 \n",
            " 95/390 [======>.......................] - ETA: 19s - loss: 2.3203 - acc: 0.0999 - LRFinder: val_loss: 2.3047 - lr = 0.00942668 \n",
            " 96/390 [======>.......................] - ETA: 19s - loss: 2.3202 - acc: 0.1003 - LRFinder: val_loss: 2.3020 - lr = 0.00965196 \n",
            " 97/390 [======>.......................] - ETA: 19s - loss: 2.3202 - acc: 0.0999 - LRFinder: val_loss: 2.3037 - lr = 0.00988261 \n",
            " 98/390 [======>.......................] - ETA: 19s - loss: 2.3200 - acc: 0.0998 - LRFinder: val_loss: 2.3039 - lr = 0.01011878 \n",
            " 99/390 [======>.......................] - ETA: 19s - loss: 2.3199 - acc: 0.0997 - LRFinder: val_loss: 2.3027 - lr = 0.01036059 \n",
            "100/390 [======>.......................] - ETA: 19s - loss: 2.3197 - acc: 0.0998 - LRFinder: val_loss: 2.3036 - lr = 0.01060818 \n",
            "101/390 [======>.......................] - ETA: 19s - loss: 2.3196 - acc: 0.1001 - LRFinder: val_loss: 2.3044 - lr = 0.01086169 \n",
            "102/390 [======>.......................] - ETA: 18s - loss: 2.3196 - acc: 0.1002 - LRFinder: val_loss: 2.3032 - lr = 0.01112126 \n",
            "103/390 [======>.......................] - ETA: 18s - loss: 2.3195 - acc: 0.1003 - LRFinder: val_loss: 2.3018 - lr = 0.01138702 \n",
            "104/390 [=======>......................] - ETA: 18s - loss: 2.3193 - acc: 0.1004 - LRFinder: val_loss: 2.3031 - lr = 0.01165914 \n",
            "105/390 [=======>......................] - ETA: 18s - loss: 2.3193 - acc: 0.1002 - LRFinder: val_loss: 2.3007 - lr = 0.01193777 \n",
            "106/390 [=======>......................] - ETA: 18s - loss: 2.3192 - acc: 0.1001 - LRFinder: val_loss: 2.3022 - lr = 0.01222305 \n",
            "107/390 [=======>......................] - ETA: 18s - loss: 2.3190 - acc: 0.0998 - LRFinder: val_loss: 2.3013 - lr = 0.01251515 \n",
            "108/390 [=======>......................] - ETA: 18s - loss: 2.3188 - acc: 0.0998 - LRFinder: val_loss: 2.3037 - lr = 0.01281422 \n",
            "109/390 [=======>......................] - ETA: 18s - loss: 2.3187 - acc: 0.1002 - LRFinder: val_loss: 2.2991 - lr = 0.01312045 \n",
            "110/390 [=======>......................] - ETA: 18s - loss: 2.3185 - acc: 0.1004 - LRFinder: val_loss: 2.3034 - lr = 0.01343399 \n",
            "111/390 [=======>......................] - ETA: 18s - loss: 2.3184 - acc: 0.1002 - LRFinder: val_loss: 2.3030 - lr = 0.01375503 \n",
            "112/390 [=======>......................] - ETA: 18s - loss: 2.3183 - acc: 0.1001 - LRFinder: val_loss: 2.3030 - lr = 0.01408374 \n",
            "113/390 [=======>......................] - ETA: 18s - loss: 2.3181 - acc: 0.1000 - LRFinder: val_loss: 2.3019 - lr = 0.01442030 \n",
            "114/390 [=======>......................] - ETA: 18s - loss: 2.3181 - acc: 0.1001 - LRFinder: val_loss: 2.3023 - lr = 0.01476491 \n",
            "115/390 [=======>......................] - ETA: 17s - loss: 2.3181 - acc: 0.0998 - LRFinder: val_loss: 2.3022 - lr = 0.01511775 \n",
            "116/390 [=======>......................] - ETA: 17s - loss: 2.3180 - acc: 0.1001 - LRFinder: val_loss: 2.3006 - lr = 0.01547903 \n",
            "117/390 [========>.....................] - ETA: 17s - loss: 2.3179 - acc: 0.0999 - LRFinder: val_loss: 2.3039 - lr = 0.01584893 \n",
            "118/390 [========>.....................] - ETA: 17s - loss: 2.3178 - acc: 0.0997 - LRFinder: val_loss: 2.3029 - lr = 0.01622768 \n",
            "119/390 [========>.....................] - ETA: 17s - loss: 2.3177 - acc: 0.0997 - LRFinder: val_loss: 2.3034 - lr = 0.01661548 \n",
            "120/390 [========>.....................] - ETA: 17s - loss: 2.3176 - acc: 0.0995 - LRFinder: val_loss: 2.3039 - lr = 0.01701254 \n",
            "121/390 [========>.....................] - ETA: 17s - loss: 2.3175 - acc: 0.0995 - LRFinder: val_loss: 2.3010 - lr = 0.01741910 \n",
            "122/390 [========>.....................] - ETA: 17s - loss: 2.3173 - acc: 0.0998 - LRFinder: val_loss: 2.3030 - lr = 0.01783537 \n",
            "123/390 [========>.....................] - ETA: 17s - loss: 2.3173 - acc: 0.1000 - LRFinder: val_loss: 2.3018 - lr = 0.01826159 \n",
            "124/390 [========>.....................] - ETA: 17s - loss: 2.3172 - acc: 0.0999 - LRFinder: val_loss: 2.3030 - lr = 0.01869799 \n",
            "125/390 [========>.....................] - ETA: 17s - loss: 2.3171 - acc: 0.0998 - LRFinder: val_loss: 2.3006 - lr = 0.01914482 \n",
            "126/390 [========>.....................] - ETA: 17s - loss: 2.3169 - acc: 0.0998 - LRFinder: val_loss: 2.3031 - lr = 0.01960233 \n",
            "127/390 [========>.....................] - ETA: 17s - loss: 2.3168 - acc: 0.1003 - LRFinder: val_loss: 2.3015 - lr = 0.02007077 \n",
            "128/390 [========>.....................] - ETA: 17s - loss: 2.3167 - acc: 0.1002 - LRFinder: val_loss: 2.3033 - lr = 0.02055041 \n",
            "129/390 [========>.....................] - ETA: 16s - loss: 2.3166 - acc: 0.1002 - LRFinder: val_loss: 2.3019 - lr = 0.02104151 \n",
            "130/390 [=========>....................] - ETA: 16s - loss: 2.3165 - acc: 0.1002 - LRFinder: val_loss: 2.3023 - lr = 0.02154435 \n",
            "131/390 [=========>....................] - ETA: 16s - loss: 2.3164 - acc: 0.1004 - LRFinder: val_loss: 2.3019 - lr = 0.02205920 \n",
            "132/390 [=========>....................] - ETA: 16s - loss: 2.3163 - acc: 0.1004 - LRFinder: val_loss: 2.3029 - lr = 0.02258636 \n",
            "133/390 [=========>....................] - ETA: 16s - loss: 2.3162 - acc: 0.1004 - LRFinder: val_loss: 2.3023 - lr = 0.02312611 \n",
            "134/390 [=========>....................] - ETA: 16s - loss: 2.3162 - acc: 0.1002 - LRFinder: val_loss: 2.3016 - lr = 0.02367876 \n",
            "135/390 [=========>....................] - ETA: 16s - loss: 2.3161 - acc: 0.1005 - LRFinder: val_loss: 2.3014 - lr = 0.02424462 \n",
            "136/390 [=========>....................] - ETA: 16s - loss: 2.3160 - acc: 0.1008 - LRFinder: val_loss: 2.3031 - lr = 0.02482400 \n",
            "137/390 [=========>....................] - ETA: 16s - loss: 2.3158 - acc: 0.1010 - LRFinder: val_loss: 2.3026 - lr = 0.02541723 \n",
            "138/390 [=========>....................] - ETA: 16s - loss: 2.3157 - acc: 0.1011 - LRFinder: val_loss: 2.3025 - lr = 0.02602463 \n",
            "139/390 [=========>....................] - ETA: 16s - loss: 2.3156 - acc: 0.1011 - LRFinder: val_loss: 2.3021 - lr = 0.02664656 \n",
            "140/390 [=========>....................] - ETA: 16s - loss: 2.3155 - acc: 0.1009 - LRFinder: val_loss: 2.3012 - lr = 0.02728334 \n",
            "141/390 [=========>....................] - ETA: 16s - loss: 2.3154 - acc: 0.1012 - LRFinder: val_loss: 2.3021 - lr = 0.02793533 \n",
            "142/390 [=========>....................] - ETA: 15s - loss: 2.3153 - acc: 0.1011 - LRFinder: val_loss: 2.3007 - lr = 0.02860291 \n",
            "143/390 [==========>...................] - ETA: 15s - loss: 2.3152 - acc: 0.1013 - LRFinder: val_loss: 2.3019 - lr = 0.02928645 \n",
            "144/390 [==========>...................] - ETA: 15s - loss: 2.3152 - acc: 0.1011 - LRFinder: val_loss: 2.3019 - lr = 0.02998631 \n",
            "145/390 [==========>...................] - ETA: 15s - loss: 2.3150 - acc: 0.1012 - LRFinder: val_loss: 2.3015 - lr = 0.03070291 \n",
            "146/390 [==========>...................] - ETA: 15s - loss: 2.3149 - acc: 0.1012 - LRFinder: val_loss: 2.3020 - lr = 0.03143663 \n",
            "147/390 [==========>...................] - ETA: 15s - loss: 2.3149 - acc: 0.1011 - LRFinder: val_loss: 2.3006 - lr = 0.03218788 \n",
            "148/390 [==========>...................] - ETA: 15s - loss: 2.3148 - acc: 0.1009 - LRFinder: val_loss: 2.3030 - lr = 0.03295708 \n",
            "149/390 [==========>...................] - ETA: 15s - loss: 2.3147 - acc: 0.1007 - LRFinder: val_loss: 2.3028 - lr = 0.03374467 \n",
            "150/390 [==========>...................] - ETA: 15s - loss: 2.3146 - acc: 0.1003 - LRFinder: val_loss: 2.3026 - lr = 0.03455108 \n",
            "151/390 [==========>...................] - ETA: 15s - loss: 2.3145 - acc: 0.1003 - LRFinder: val_loss: 2.3044 - lr = 0.03537676 \n",
            "152/390 [==========>...................] - ETA: 15s - loss: 2.3145 - acc: 0.1004 - LRFinder: val_loss: 2.3029 - lr = 0.03622216 \n",
            "153/390 [==========>...................] - ETA: 15s - loss: 2.3144 - acc: 0.1003 - LRFinder: val_loss: 2.3036 - lr = 0.03708778 \n",
            "154/390 [==========>...................] - ETA: 15s - loss: 2.3144 - acc: 0.1003 - LRFinder: val_loss: 2.3020 - lr = 0.03797407 \n",
            "155/390 [==========>...................] - ETA: 15s - loss: 2.3143 - acc: 0.1005 - LRFinder: val_loss: 2.3031 - lr = 0.03888155 \n",
            "156/390 [===========>..................] - ETA: 15s - loss: 2.3142 - acc: 0.1006 - LRFinder: val_loss: 2.3023 - lr = 0.03981072 \n",
            "157/390 [===========>..................] - ETA: 14s - loss: 2.3142 - acc: 0.1003 - LRFinder: val_loss: 2.3023 - lr = 0.04076209 \n",
            "158/390 [===========>..................] - ETA: 14s - loss: 2.3142 - acc: 0.1000 - LRFinder: val_loss: 2.3024 - lr = 0.04173619 \n",
            "159/390 [===========>..................] - ETA: 14s - loss: 2.3141 - acc: 0.1002 - LRFinder: val_loss: 2.3017 - lr = 0.04273358 \n",
            "160/390 [===========>..................] - ETA: 14s - loss: 2.3141 - acc: 0.1000 - LRFinder: val_loss: 2.3017 - lr = 0.04375479 \n",
            "161/390 [===========>..................] - ETA: 14s - loss: 2.3140 - acc: 0.1001 - LRFinder: val_loss: 2.3020 - lr = 0.04480042 \n",
            "162/390 [===========>..................] - ETA: 14s - loss: 2.3140 - acc: 0.1001 - LRFinder: val_loss: 2.3022 - lr = 0.04587103 \n",
            "163/390 [===========>..................] - ETA: 14s - loss: 2.3139 - acc: 0.1000 - LRFinder: val_loss: 2.3034 - lr = 0.04696722 \n",
            "164/390 [===========>..................] - ETA: 14s - loss: 2.3139 - acc: 0.0999 - LRFinder: val_loss: 2.3012 - lr = 0.04808961 \n",
            "165/390 [===========>..................] - ETA: 14s - loss: 2.3138 - acc: 0.0998 - LRFinder: val_loss: 2.3026 - lr = 0.04923883 \n",
            "166/390 [===========>..................] - ETA: 14s - loss: 2.3138 - acc: 0.0998 - LRFinder: val_loss: 2.3022 - lr = 0.05041551 \n",
            "167/390 [===========>..................] - ETA: 14s - loss: 2.3137 - acc: 0.1001 - LRFinder: val_loss: 2.3025 - lr = 0.05162030 \n",
            "168/390 [===========>..................] - ETA: 14s - loss: 2.3137 - acc: 0.1001 - LRFinder: val_loss: 2.3022 - lr = 0.05285389 \n",
            "169/390 [============>.................] - ETA: 14s - loss: 2.3136 - acc: 0.0999 - LRFinder: val_loss: 2.3026 - lr = 0.05411696 \n",
            "170/390 [============>.................] - ETA: 14s - loss: 2.3136 - acc: 0.1001 - LRFinder: val_loss: 2.3030 - lr = 0.05541021 \n",
            "171/390 [============>.................] - ETA: 13s - loss: 2.3135 - acc: 0.1000 - LRFinder: val_loss: 2.3025 - lr = 0.05673437 \n",
            "172/390 [============>.................] - ETA: 13s - loss: 2.3134 - acc: 0.0997 - LRFinder: val_loss: 2.3023 - lr = 0.05809016 \n",
            "173/390 [============>.................] - ETA: 13s - loss: 2.3133 - acc: 0.1000 - LRFinder: val_loss: 2.3031 - lr = 0.05947837 \n",
            "174/390 [============>.................] - ETA: 13s - loss: 2.3133 - acc: 0.0999 - LRFinder: val_loss: 2.3015 - lr = 0.06089974 \n",
            "175/390 [============>.................] - ETA: 13s - loss: 2.3133 - acc: 0.0998 - LRFinder: val_loss: 2.3020 - lr = 0.06235508 \n",
            "176/390 [============>.................] - ETA: 13s - loss: 2.3132 - acc: 0.1000 - LRFinder: val_loss: 2.3020 - lr = 0.06384520 \n",
            "177/390 [============>.................] - ETA: 13s - loss: 2.3131 - acc: 0.1000 - LRFinder: val_loss: 2.3027 - lr = 0.06537093 \n",
            "178/390 [============>.................] - ETA: 13s - loss: 2.3130 - acc: 0.1002 - LRFinder: val_loss: 2.3022 - lr = 0.06693312 \n",
            "179/390 [============>.................] - ETA: 13s - loss: 2.3130 - acc: 0.1005 - LRFinder: val_loss: 2.3023 - lr = 0.06853265 \n",
            "180/390 [============>.................] - ETA: 13s - loss: 2.3129 - acc: 0.1004 - LRFinder: val_loss: 2.3028 - lr = 0.07017039 \n",
            "181/390 [============>.................] - ETA: 13s - loss: 2.3129 - acc: 0.1004 - LRFinder: val_loss: 2.3026 - lr = 0.07184728 \n",
            "182/390 [=============>................] - ETA: 13s - loss: 2.3128 - acc: 0.1005 - LRFinder: val_loss: 2.3026 - lr = 0.07356424 \n",
            "183/390 [=============>................] - ETA: 13s - loss: 2.3128 - acc: 0.1004 - LRFinder: val_loss: 2.3020 - lr = 0.07532223 \n",
            "184/390 [=============>................] - ETA: 13s - loss: 2.3127 - acc: 0.1002 - LRFinder: val_loss: 2.3021 - lr = 0.07712223 \n",
            "185/390 [=============>................] - ETA: 13s - loss: 2.3127 - acc: 0.1004 - LRFinder: val_loss: 2.3025 - lr = 0.07896524 \n",
            "186/390 [=============>................] - ETA: 12s - loss: 2.3126 - acc: 0.1004 - LRFinder: val_loss: 2.3035 - lr = 0.08085230 \n",
            "187/390 [=============>................] - ETA: 12s - loss: 2.3126 - acc: 0.1003 - LRFinder: val_loss: 2.3042 - lr = 0.08278445 \n",
            "188/390 [=============>................] - ETA: 12s - loss: 2.3125 - acc: 0.1005 - LRFinder: val_loss: 2.3007 - lr = 0.08476278 \n",
            "189/390 [=============>................] - ETA: 12s - loss: 2.3125 - acc: 0.1004 - LRFinder: val_loss: 2.3025 - lr = 0.08678839 \n",
            "190/390 [=============>................] - ETA: 12s - loss: 2.3124 - acc: 0.1004 - LRFinder: val_loss: 2.3031 - lr = 0.08886240 \n",
            "191/390 [=============>................] - ETA: 12s - loss: 2.3124 - acc: 0.1003 - LRFinder: val_loss: 2.3026 - lr = 0.09098597 \n",
            "192/390 [=============>................] - ETA: 12s - loss: 2.3124 - acc: 0.1002 - LRFinder: val_loss: 2.3030 - lr = 0.09316029 \n",
            "193/390 [=============>................] - ETA: 12s - loss: 2.3123 - acc: 0.1003 - LRFinder: val_loss: 2.3018 - lr = 0.09538658 \n",
            "194/390 [=============>................] - ETA: 12s - loss: 2.3123 - acc: 0.1004 - LRFinder: val_loss: 2.3011 - lr = 0.09766606 \n",
            "195/390 [==============>...............] - ETA: 12s - loss: 2.3122 - acc: 0.1003 - LRFinder: val_loss: 2.3037 - lr = 0.10000002 \n",
            "196/390 [==============>...............] - ETA: 12s - loss: 2.3122 - acc: 0.1002 - LRFinder: val_loss: 2.3028 - lr = 0.10238976 \n",
            "197/390 [==============>...............] - ETA: 12s - loss: 2.3122 - acc: 0.1001 - LRFinder: val_loss: 2.3040 - lr = 0.10483660 \n",
            "198/390 [==============>...............] - ETA: 12s - loss: 2.3121 - acc: 0.1001 - LRFinder: val_loss: 2.3027 - lr = 0.10734192 \n",
            "199/390 [==============>...............] - ETA: 12s - loss: 2.3121 - acc: 0.1003 - LRFinder: val_loss: 2.3039 - lr = 0.10990711 \n",
            "200/390 [==============>...............] - ETA: 12s - loss: 2.3121 - acc: 0.1005 - LRFinder: val_loss: 2.3045 - lr = 0.11253359 \n",
            "201/390 [==============>...............] - ETA: 11s - loss: 2.3120 - acc: 0.1004 - LRFinder: val_loss: 2.3039 - lr = 0.11522284 \n",
            "202/390 [==============>...............] - ETA: 11s - loss: 2.3120 - acc: 0.1004 - LRFinder: val_loss: 2.3040 - lr = 0.11797636 \n",
            "203/390 [==============>...............] - ETA: 11s - loss: 2.3120 - acc: 0.1005 - LRFinder: val_loss: 2.3032 - lr = 0.12079568 \n",
            "204/390 [==============>...............] - ETA: 11s - loss: 2.3119 - acc: 0.1006 - LRFinder: val_loss: 2.3040 - lr = 0.12368238 \n",
            "205/390 [==============>...............] - ETA: 11s - loss: 2.3119 - acc: 0.1007 - LRFinder: val_loss: 2.3030 - lr = 0.12663806 \n",
            "206/390 [==============>...............] - ETA: 11s - loss: 2.3119 - acc: 0.1008 - LRFinder: val_loss: 2.3046 - lr = 0.12966437 \n",
            "207/390 [==============>...............] - ETA: 11s - loss: 2.3118 - acc: 0.1009 - LRFinder: val_loss: 2.3024 - lr = 0.13276299 \n",
            "208/390 [===============>..............] - ETA: 11s - loss: 2.3118 - acc: 0.1008 - LRFinder: val_loss: 2.3030 - lr = 0.13593568 \n",
            "209/390 [===============>..............] - ETA: 11s - loss: 2.3117 - acc: 0.1009 - LRFinder: val_loss: 2.3035 - lr = 0.13918418 \n",
            "210/390 [===============>..............] - ETA: 11s - loss: 2.3117 - acc: 0.1007 - LRFinder: val_loss: 2.3032 - lr = 0.14251031 \n",
            "211/390 [===============>..............] - ETA: 11s - loss: 2.3116 - acc: 0.1007 - LRFinder: val_loss: 2.3041 - lr = 0.14591593 \n",
            "212/390 [===============>..............] - ETA: 11s - loss: 2.3116 - acc: 0.1005 - LRFinder: val_loss: 2.3035 - lr = 0.14940293 \n",
            "213/390 [===============>..............] - ETA: 11s - loss: 2.3116 - acc: 0.1006 - LRFinder: val_loss: 2.3009 - lr = 0.15297326 \n",
            "214/390 [===============>..............] - ETA: 11s - loss: 2.3116 - acc: 0.1006 - LRFinder: val_loss: 2.3034 - lr = 0.15662892 \n",
            "215/390 [===============>..............] - ETA: 11s - loss: 2.3115 - acc: 0.1004 - LRFinder: val_loss: 2.3046 - lr = 0.16037193 \n",
            "216/390 [===============>..............] - ETA: 10s - loss: 2.3115 - acc: 0.1002 - LRFinder: val_loss: 2.3037 - lr = 0.16420439 \n",
            "217/390 [===============>..............] - ETA: 10s - loss: 2.3114 - acc: 0.1001 - LRFinder: val_loss: 2.3042 - lr = 0.16812844 \n",
            "218/390 [===============>..............] - ETA: 10s - loss: 2.3114 - acc: 0.1002 - LRFinder: val_loss: 2.3038 - lr = 0.17214625 \n",
            "219/390 [===============>..............] - ETA: 10s - loss: 2.3114 - acc: 0.1001 - LRFinder: val_loss: 2.3043 - lr = 0.17626008 \n",
            "220/390 [===============>..............] - ETA: 10s - loss: 2.3113 - acc: 0.1000 - LRFinder: val_loss: 2.3050 - lr = 0.18047223 \n",
            "221/390 [================>.............] - ETA: 10s - loss: 2.3113 - acc: 0.1000 - LRFinder: val_loss: 2.3052 - lr = 0.18478503 \n",
            "222/390 [================>.............] - ETA: 10s - loss: 2.3112 - acc: 0.1002 - LRFinder: val_loss: 2.3056 - lr = 0.18920089 \n",
            "223/390 [================>.............] - ETA: 10s - loss: 2.3112 - acc: 0.1002 - LRFinder: val_loss: 2.3018 - lr = 0.19372229 \n",
            "224/390 [================>.............] - ETA: 10s - loss: 2.3112 - acc: 0.1000 - LRFinder: val_loss: 2.3045 - lr = 0.19835174 \n",
            "225/390 [================>.............] - ETA: 10s - loss: 2.3112 - acc: 0.0998 - LRFinder: val_loss: 2.3041 - lr = 0.20309182 \n",
            "226/390 [================>.............] - ETA: 10s - loss: 2.3112 - acc: 0.0997 - LRFinder: val_loss: 2.3027 - lr = 0.20794517 \n",
            "227/390 [================>.............] - ETA: 10s - loss: 2.3111 - acc: 0.0996 - LRFinder: val_loss: 2.3039 - lr = 0.21291450 \n",
            "228/390 [================>.............] - ETA: 10s - loss: 2.3111 - acc: 0.0996 - LRFinder: val_loss: 2.3030 - lr = 0.21800258 \n",
            "229/390 [================>.............] - ETA: 10s - loss: 2.3110 - acc: 0.0995 - LRFinder: val_loss: 2.3044 - lr = 0.22321227 \n",
            "230/390 [================>.............] - ETA: 10s - loss: 2.3110 - acc: 0.0995 - LRFinder: val_loss: 2.3010 - lr = 0.22854645 \n",
            "231/390 [================>.............] - ETA: 9s - loss: 2.3110 - acc: 0.0995  - LRFinder: val_loss: 2.3030 - lr = 0.23400811 \n",
            "232/390 [================>.............] - ETA: 9s - loss: 2.3109 - acc: 0.0994 - LRFinder: val_loss: 2.3047 - lr = 0.23960027 \n",
            "233/390 [================>.............] - ETA: 9s - loss: 2.3109 - acc: 0.0994 - LRFinder: val_loss: 2.3032 - lr = 0.24532608 \n",
            "234/390 [=================>............] - ETA: 9s - loss: 2.3109 - acc: 0.0995 - LRFinder: val_loss: 2.3047 - lr = 0.25118871 \n",
            "235/390 [=================>............] - ETA: 9s - loss: 2.3109 - acc: 0.0993 - LRFinder: val_loss: 2.3056 - lr = 0.25719143 \n",
            "236/390 [=================>............] - ETA: 9s - loss: 2.3108 - acc: 0.0994 - LRFinder: val_loss: 2.3044 - lr = 0.26333761 \n",
            "237/390 [=================>............] - ETA: 9s - loss: 2.3108 - acc: 0.0992 - LRFinder: val_loss: 2.3043 - lr = 0.26963068 \n",
            "238/390 [=================>............] - ETA: 9s - loss: 2.3108 - acc: 0.0991 - LRFinder: val_loss: 2.3024 - lr = 0.27607412 \n",
            "239/390 [=================>............] - ETA: 9s - loss: 2.3108 - acc: 0.0991 - LRFinder: val_loss: 2.3026 - lr = 0.28267155 \n",
            "240/390 [=================>............] - ETA: 9s - loss: 2.3108 - acc: 0.0993 - LRFinder: val_loss: 2.3030 - lr = 0.28942664 \n",
            "241/390 [=================>............] - ETA: 9s - loss: 2.3108 - acc: 0.0993 - LRFinder: val_loss: 2.3032 - lr = 0.29634315 \n",
            "242/390 [=================>............] - ETA: 9s - loss: 2.3107 - acc: 0.0995 - LRFinder: val_loss: 2.3029 - lr = 0.30342496 \n",
            "243/390 [=================>............] - ETA: 9s - loss: 2.3107 - acc: 0.0995 - LRFinder: val_loss: 2.3020 - lr = 0.31067600 \n",
            "244/390 [=================>............] - ETA: 9s - loss: 2.3107 - acc: 0.0995 - LRFinder: val_loss: 2.3051 - lr = 0.31810034 \n",
            "245/390 [=================>............] - ETA: 9s - loss: 2.3106 - acc: 0.0996 - LRFinder: val_loss: 2.3016 - lr = 0.32570208 \n",
            "246/390 [=================>............] - ETA: 9s - loss: 2.3106 - acc: 0.0995 - LRFinder: val_loss: 2.3013 - lr = 0.33348548 \n",
            "247/390 [==================>...........] - ETA: 8s - loss: 2.3106 - acc: 0.0996 - LRFinder: val_loss: 2.3034 - lr = 0.34145490 \n",
            "248/390 [==================>...........] - ETA: 8s - loss: 2.3105 - acc: 0.0997 - LRFinder: val_loss: 2.3050 - lr = 0.34961475 \n",
            "249/390 [==================>...........] - ETA: 8s - loss: 2.3105 - acc: 0.0999 - LRFinder: val_loss: 2.3054 - lr = 0.35796960 \n",
            "250/390 [==================>...........] - ETA: 8s - loss: 2.3104 - acc: 0.0998 - LRFinder: val_loss: 2.3145 - lr = 0.36652413 \n",
            "251/390 [==================>...........] - ETA: 8s - loss: 2.3104 - acc: 0.0996 - LRFinder: val_loss: 2.3026 - lr = 0.37528308 \n",
            "252/390 [==================>...........] - ETA: 8s - loss: 2.3104 - acc: 0.0995 - LRFinder: val_loss: 2.3094 - lr = 0.38425136 \n",
            "253/390 [==================>...........] - ETA: 8s - loss: 2.3104 - acc: 0.0995 - LRFinder: val_loss: 2.3078 - lr = 0.39343394 \n",
            "254/390 [==================>...........] - ETA: 8s - loss: 2.3104 - acc: 0.0994 - LRFinder: val_loss: 2.3056 - lr = 0.40283595 \n",
            "255/390 [==================>...........] - ETA: 8s - loss: 2.3103 - acc: 0.0993 - LRFinder: val_loss: 2.3093 - lr = 0.41246264 \n",
            "256/390 [==================>...........] - ETA: 8s - loss: 2.3103 - acc: 0.0992 - LRFinder: val_loss: 2.3000 - lr = 0.42231941 \n",
            "257/390 [==================>...........] - ETA: 8s - loss: 2.3103 - acc: 0.0991 - LRFinder: val_loss: 2.3027 - lr = 0.43241172 \n",
            "258/390 [==================>...........] - ETA: 8s - loss: 2.3103 - acc: 0.0991 - LRFinder: val_loss: 2.3060 - lr = 0.44274522 \n",
            "259/390 [==================>...........] - ETA: 8s - loss: 2.3103 - acc: 0.0990 - LRFinder: val_loss: 2.3007 - lr = 0.45332564 \n",
            "260/390 [===================>..........] - ETA: 8s - loss: 2.3103 - acc: 0.0990 - LRFinder: val_loss: 2.3040 - lr = 0.46415890 \n",
            "261/390 [===================>..........] - ETA: 8s - loss: 2.3102 - acc: 0.0992 - LRFinder: val_loss: 2.3039 - lr = 0.47525105 \n",
            "262/390 [===================>..........] - ETA: 7s - loss: 2.3102 - acc: 0.0994 - LRFinder: val_loss: 2.3050 - lr = 0.48660828 \n",
            "263/390 [===================>..........] - ETA: 7s - loss: 2.3102 - acc: 0.0994 - LRFinder: val_loss: 2.3036 - lr = 0.49823690 \n",
            "264/390 [===================>..........] - ETA: 7s - loss: 2.3102 - acc: 0.0994 - LRFinder: val_loss: 2.3044 - lr = 0.51014343 \n",
            "265/390 [===================>..........] - ETA: 7s - loss: 2.3102 - acc: 0.0995 - LRFinder: val_loss: 2.3036 - lr = 0.52233446 \n",
            "266/390 [===================>..........] - ETA: 7s - loss: 2.3102 - acc: 0.0993 - LRFinder: val_loss: 2.3025 - lr = 0.53481686 \n",
            "267/390 [===================>..........] - ETA: 7s - loss: 2.3102 - acc: 0.0995 - LRFinder: val_loss: 2.3066 - lr = 0.54759756 \n",
            "268/390 [===================>..........] - ETA: 7s - loss: 2.3101 - acc: 0.0995 - LRFinder: val_loss: 2.3060 - lr = 0.56068365 \n",
            "269/390 [===================>..........] - ETA: 7s - loss: 2.3101 - acc: 0.0995 - LRFinder: val_loss: 2.3049 - lr = 0.57408251 \n",
            "270/390 [===================>..........] - ETA: 7s - loss: 2.3101 - acc: 0.0995 - LRFinder: val_loss: 2.3059 - lr = 0.58780153 \n",
            "271/390 [===================>..........] - ETA: 7s - loss: 2.3101 - acc: 0.0996 - LRFinder: val_loss: 2.3019 - lr = 0.60184840 \n",
            "272/390 [===================>..........] - ETA: 7s - loss: 2.3101 - acc: 0.0996 - LRFinder: val_loss: 2.3031 - lr = 0.61623099 \n",
            "273/390 [====================>.........] - ETA: 7s - loss: 2.3101 - acc: 0.0996 - LRFinder: val_loss: 2.3048 - lr = 0.63095724 \n",
            "274/390 [====================>.........] - ETA: 7s - loss: 2.3100 - acc: 0.0996 - LRFinder: val_loss: 2.3074 - lr = 0.64603544 \n",
            "275/390 [====================>.........] - ETA: 7s - loss: 2.3101 - acc: 0.0996 - LRFinder: val_loss: 2.3066 - lr = 0.66147395 \n",
            "276/390 [====================>.........] - ETA: 7s - loss: 2.3100 - acc: 0.0997 - LRFinder: val_loss: 2.3092 - lr = 0.67728139 \n",
            "277/390 [====================>.........] - ETA: 7s - loss: 2.3100 - acc: 0.0997 - LRFinder: val_loss: 2.3078 - lr = 0.69346660 \n",
            "278/390 [====================>.........] - ETA: 6s - loss: 2.3100 - acc: 0.0998 - LRFinder: val_loss: 2.3019 - lr = 0.71003860 \n",
            "279/390 [====================>.........] - ETA: 6s - loss: 2.3100 - acc: 0.0997 - LRFinder: val_loss: 2.3025 - lr = 0.72700663 \n",
            "280/390 [====================>.........] - ETA: 6s - loss: 2.3100 - acc: 0.0996 - LRFinder: val_loss: 2.3075 - lr = 0.74438013 \n",
            "281/390 [====================>.........] - ETA: 6s - loss: 2.3100 - acc: 0.0998 - LRFinder: val_loss: 2.3054 - lr = 0.76216881 \n",
            "282/390 [====================>.........] - ETA: 6s - loss: 2.3099 - acc: 0.0998 - LRFinder: val_loss: 2.3088 - lr = 0.78038262 \n",
            "283/390 [====================>.........] - ETA: 6s - loss: 2.3099 - acc: 0.0998 - LRFinder: val_loss: 2.3086 - lr = 0.79903169 \n",
            "284/390 [====================>.........] - ETA: 6s - loss: 2.3099 - acc: 0.1000 - LRFinder: val_loss: 2.3102 - lr = 0.81812640 \n",
            "285/390 [====================>.........] - ETA: 6s - loss: 2.3098 - acc: 0.1001 - LRFinder: val_loss: 2.3091 - lr = 0.83767742 \n",
            "286/390 [=====================>........] - ETA: 6s - loss: 2.3099 - acc: 0.1000 - LRFinder: val_loss: 2.3084 - lr = 0.85769567 \n",
            "287/390 [=====================>........] - ETA: 6s - loss: 2.3098 - acc: 0.0999 - LRFinder: val_loss: 2.3070 - lr = 0.87819234 \n",
            "288/390 [=====================>........] - ETA: 6s - loss: 2.3098 - acc: 0.0997 - LRFinder: val_loss: 2.3075 - lr = 0.89917882 \n",
            "289/390 [=====================>........] - ETA: 6s - loss: 2.3099 - acc: 0.0998 - LRFinder: val_loss: 2.3058 - lr = 0.92066678 \n",
            "290/390 [=====================>........] - ETA: 6s - loss: 2.3099 - acc: 0.0997 - LRFinder: val_loss: 2.3040 - lr = 0.94266823 \n",
            "291/390 [=====================>........] - ETA: 6s - loss: 2.3099 - acc: 0.0998 - LRFinder: val_loss: 2.3075 - lr = 0.96519551 \n",
            "292/390 [=====================>........] - ETA: 6s - loss: 2.3098 - acc: 0.0999 - LRFinder: val_loss: 2.3119 - lr = 0.98826113 \n",
            "293/390 [=====================>........] - ETA: 6s - loss: 2.3098 - acc: 0.0998 - LRFinder: val_loss: 2.3129 - lr = 1.01187790 \n",
            "294/390 [=====================>........] - ETA: 5s - loss: 2.3098 - acc: 0.0997 - LRFinder: val_loss: 2.3087 - lr = 1.03605907 \n",
            "295/390 [=====================>........] - ETA: 5s - loss: 2.3099 - acc: 0.0996 - LRFinder: val_loss: 2.3040 - lr = 1.06081807 \n",
            "296/390 [=====================>........] - ETA: 5s - loss: 2.3099 - acc: 0.0996 - LRFinder: val_loss: 2.3087 - lr = 1.08616880 \n",
            "297/390 [=====================>........] - ETA: 5s - loss: 2.3098 - acc: 0.0995 - LRFinder: val_loss: 2.3122 - lr = 1.11212530 \n",
            "298/390 [=====================>........] - ETA: 5s - loss: 2.3098 - acc: 0.0995 - LRFinder: val_loss: 2.3029 - lr = 1.13870210 \n",
            "299/390 [======================>.......] - ETA: 5s - loss: 2.3098 - acc: 0.0994 - LRFinder: val_loss: 2.3095 - lr = 1.16591409 \n",
            "300/390 [======================>.......] - ETA: 5s - loss: 2.3098 - acc: 0.0994 - LRFinder: val_loss: 2.3023 - lr = 1.19377629 \n",
            "301/390 [======================>.......] - ETA: 5s - loss: 2.3098 - acc: 0.0993 - LRFinder: val_loss: 2.3065 - lr = 1.22230431 \n",
            "302/390 [======================>.......] - ETA: 5s - loss: 2.3098 - acc: 0.0994 - LRFinder: val_loss: 2.3071 - lr = 1.25151415 \n",
            "303/390 [======================>.......] - ETA: 5s - loss: 2.3098 - acc: 0.0994 - LRFinder: val_loss: 2.3044 - lr = 1.28142204 \n",
            "304/390 [======================>.......] - ETA: 5s - loss: 2.3098 - acc: 0.0992 - LRFinder: val_loss: 2.3050 - lr = 1.31204458 \n",
            "305/390 [======================>.......] - ETA: 5s - loss: 2.3098 - acc: 0.0992 - LRFinder: val_loss: 2.3059 - lr = 1.34339898 \n",
            "306/390 [======================>.......] - ETA: 5s - loss: 2.3098 - acc: 0.0992 - LRFinder: val_loss: 2.3023 - lr = 1.37550258 \n",
            "307/390 [======================>.......] - ETA: 5s - loss: 2.3098 - acc: 0.0991 - LRFinder: val_loss: 2.3114 - lr = 1.40837342 \n",
            "308/390 [======================>.......] - ETA: 5s - loss: 2.3098 - acc: 0.0991 - LRFinder: val_loss: 2.3021 - lr = 1.44202984 \n",
            "309/390 [======================>.......] - ETA: 5s - loss: 2.3098 - acc: 0.0992 - LRFinder: val_loss: 2.3103 - lr = 1.47649050 \n",
            "310/390 [======================>.......] - ETA: 4s - loss: 2.3098 - acc: 0.0992 - LRFinder: val_loss: 2.3107 - lr = 1.51177468 \n",
            "311/390 [======================>.......] - ETA: 4s - loss: 2.3097 - acc: 0.0992 - LRFinder: val_loss: 2.3083 - lr = 1.54790204 \n",
            "312/390 [=======================>......] - ETA: 4s - loss: 2.3097 - acc: 0.0992 - LRFinder: val_loss: 2.3091 - lr = 1.58489271 \n",
            "313/390 [=======================>......] - ETA: 4s - loss: 2.3097 - acc: 0.0992 - LRFinder: val_loss: 2.3064 - lr = 1.62276745 \n",
            "314/390 [=======================>......] - ETA: 4s - loss: 2.3097 - acc: 0.0993 - LRFinder: val_loss: 2.3048 - lr = 1.66154726 \n",
            "315/390 [=======================>......] - ETA: 4s - loss: 2.3097 - acc: 0.0994 - LRFinder: val_loss: 2.3048 - lr = 1.70125385 \n",
            "316/390 [=======================>......] - ETA: 4s - loss: 2.3097 - acc: 0.0993 - LRFinder: val_loss: 2.3072 - lr = 1.74190932 \n",
            "317/390 [=======================>......] - ETA: 4s - loss: 2.3097 - acc: 0.0993 - LRFinder: val_loss: 2.3009 - lr = 1.78353625 \n",
            "318/390 [=======================>......] - ETA: 4s - loss: 2.3097 - acc: 0.0993 - LRFinder: val_loss: 2.3057 - lr = 1.82615795 \n",
            "319/390 [=======================>......] - ETA: 4s - loss: 2.3097 - acc: 0.0992 - LRFinder: val_loss: 2.3046 - lr = 1.86979823 \n",
            "320/390 [=======================>......] - ETA: 4s - loss: 2.3097 - acc: 0.0992 - LRFinder: val_loss: 2.3066 - lr = 1.91448137 \n",
            "321/390 [=======================>......] - ETA: 4s - loss: 2.3097 - acc: 0.0992 - LRFinder: val_loss: 2.3035 - lr = 1.96023239 \n",
            "322/390 [=======================>......] - ETA: 4s - loss: 2.3097 - acc: 0.0991 - LRFinder: val_loss: 2.3010 - lr = 2.00707670 \n",
            "323/390 [=======================>......] - ETA: 4s - loss: 2.3097 - acc: 0.0992 - LRFinder: val_loss: 2.3059 - lr = 2.05504051 \n",
            "324/390 [=======================>......] - ETA: 4s - loss: 2.3097 - acc: 0.0991 - LRFinder: val_loss: 2.3077 - lr = 2.10415058 \n",
            "325/390 [========================>.....] - ETA: 4s - loss: 2.3097 - acc: 0.0991 - LRFinder: val_loss: 2.3104 - lr = 2.15443411 \n",
            "326/390 [========================>.....] - ETA: 3s - loss: 2.3097 - acc: 0.0992 - LRFinder: val_loss: 2.3025 - lr = 2.20591943 \n",
            "327/390 [========================>.....] - ETA: 3s - loss: 2.3097 - acc: 0.0993 - LRFinder: val_loss: 2.3095 - lr = 2.25863509 \n",
            "328/390 [========================>.....] - ETA: 3s - loss: 2.3096 - acc: 0.0994 - LRFinder: val_loss: 2.3180 - lr = 2.31261039 \n",
            "329/390 [========================>.....] - ETA: 3s - loss: 2.3096 - acc: 0.0994 - LRFinder: val_loss: 2.3180 - lr = 2.36787560 \n",
            "330/390 [========================>.....] - ETA: 3s - loss: 2.3097 - acc: 0.0994 - LRFinder: val_loss: 2.3109 - lr = 2.42446148 \n",
            "331/390 [========================>.....] - ETA: 3s - loss: 2.3096 - acc: 0.0995 - LRFinder: val_loss: 2.3164 - lr = 2.48239952 \n",
            "332/390 [========================>.....] - ETA: 3s - loss: 2.3096 - acc: 0.0994 - LRFinder: val_loss: 2.3130 - lr = 2.54172218 \n",
            "333/390 [========================>.....] - ETA: 3s - loss: 2.3096 - acc: 0.0995 - LRFinder: val_loss: 2.3088 - lr = 2.60246267 \n",
            "334/390 [========================>.....] - ETA: 3s - loss: 2.3096 - acc: 0.0995 - LRFinder: val_loss: 2.3130 - lr = 2.66465468 \n",
            "335/390 [========================>.....] - ETA: 3s - loss: 2.3097 - acc: 0.0994 - LRFinder: val_loss: 2.3027 - lr = 2.72833286 \n",
            "336/390 [========================>.....] - ETA: 3s - loss: 2.3097 - acc: 0.0994 - LRFinder: val_loss: 2.3101 - lr = 2.79353262 \n",
            "337/390 [========================>.....] - ETA: 3s - loss: 2.3096 - acc: 0.0993 - LRFinder: val_loss: 2.3029 - lr = 2.86029058 \n",
            "338/390 [=========================>....] - ETA: 3s - loss: 2.3097 - acc: 0.0993 - LRFinder: val_loss: 2.2997 - lr = 2.92864383 \n",
            "339/390 [=========================>....] - ETA: 3s - loss: 2.3097 - acc: 0.0992 - LRFinder: val_loss: 2.3141 - lr = 2.99863071 \n",
            "340/390 [=========================>....] - ETA: 3s - loss: 2.3097 - acc: 0.0992 - LRFinder: val_loss: 2.3073 - lr = 3.07029003 \n",
            "341/390 [=========================>....] - ETA: 3s - loss: 2.3097 - acc: 0.0992 - LRFinder: val_loss: 2.3134 - lr = 3.14366182 \n",
            "342/390 [=========================>....] - ETA: 2s - loss: 2.3098 - acc: 0.0991 - LRFinder: val_loss: 2.3121 - lr = 3.21878686 \n",
            "343/390 [=========================>....] - ETA: 2s - loss: 2.3098 - acc: 0.0991 - LRFinder: val_loss: 2.3107 - lr = 3.29570737 \n",
            "344/390 [=========================>....] - ETA: 2s - loss: 2.3098 - acc: 0.0990 - LRFinder: val_loss: 2.3042 - lr = 3.37446607 \n",
            "345/390 [=========================>....] - ETA: 2s - loss: 2.3098 - acc: 0.0989 - LRFinder: val_loss: 2.3113 - lr = 3.45510691 \n",
            "346/390 [=========================>....] - ETA: 2s - loss: 2.3098 - acc: 0.0989 - LRFinder: val_loss: 2.3069 - lr = 3.53767480 \n",
            "347/390 [=========================>....] - ETA: 2s - loss: 2.3098 - acc: 0.0989 - LRFinder: val_loss: 2.3072 - lr = 3.62221589 \n",
            "348/390 [=========================>....] - ETA: 2s - loss: 2.3098 - acc: 0.0989 - LRFinder: val_loss: 2.3221 - lr = 3.70877727 \n",
            "349/390 [=========================>....] - ETA: 2s - loss: 2.3098 - acc: 0.0989 - LRFinder: val_loss: 2.3177 - lr = 3.79740706 \n",
            "350/390 [=========================>....] - ETA: 2s - loss: 2.3098 - acc: 0.0988 - LRFinder: val_loss: 2.3093 - lr = 3.88815504 \n",
            "351/390 [==========================>...] - ETA: 2s - loss: 2.3098 - acc: 0.0988 - LRFinder: val_loss: 2.3125 - lr = 3.98107150 \n",
            "352/390 [==========================>...] - ETA: 2s - loss: 2.3098 - acc: 0.0988 - LRFinder: val_loss: 2.3084 - lr = 4.07620845 \n",
            "353/390 [==========================>...] - ETA: 2s - loss: 2.3098 - acc: 0.0986 - LRFinder: val_loss: 2.3132 - lr = 4.17361909 \n",
            "354/390 [==========================>...] - ETA: 2s - loss: 2.3098 - acc: 0.0986 - LRFinder: val_loss: 2.3088 - lr = 4.27335762 \n",
            "355/390 [==========================>...] - ETA: 2s - loss: 2.3099 - acc: 0.0986 - LRFinder: val_loss: 2.3175 - lr = 4.37547922 \n",
            "356/390 [==========================>...] - ETA: 2s - loss: 2.3099 - acc: 0.0985 - LRFinder: val_loss: 2.3131 - lr = 4.48004149 \n",
            "357/390 [==========================>...] - ETA: 2s - loss: 2.3098 - acc: 0.0985 - LRFinder: val_loss: 2.3251 - lr = 4.58710253 \n",
            "358/390 [==========================>...] - ETA: 1s - loss: 2.3100 - acc: 0.0983 - LRFinder: val_loss: 2.3228 - lr = 4.69672191 \n",
            "359/390 [==========================>...] - ETA: 1s - loss: 2.3099 - acc: 0.0985 - LRFinder: val_loss: 2.3313 - lr = 4.80896114 \n",
            "360/390 [==========================>...] - ETA: 1s - loss: 2.3100 - acc: 0.0985 - LRFinder: val_loss: 2.3223 - lr = 4.92388224 \n",
            "361/390 [==========================>...] - ETA: 1s - loss: 2.3101 - acc: 0.0984 - LRFinder: val_loss: 2.3063 - lr = 5.04154964 \n",
            "362/390 [==========================>...] - ETA: 1s - loss: 2.3101 - acc: 0.0984 - LRFinder: val_loss: 2.3237 - lr = 5.16202926 \n",
            "363/390 [==========================>...] - ETA: 1s - loss: 2.3101 - acc: 0.0984 - LRFinder: val_loss: 2.3054 - lr = 5.28538798 \n",
            "364/390 [===========================>..] - ETA: 1s - loss: 2.3101 - acc: 0.0984 - LRFinder: val_loss: 2.3136 - lr = 5.41169465 \n",
            "365/390 [===========================>..] - ETA: 1s - loss: 2.3102 - acc: 0.0984 - LRFinder: val_loss: 2.3065 - lr = 5.54101957 \n",
            "366/390 [===========================>..] - ETA: 1s - loss: 2.3102 - acc: 0.0984 - LRFinder: val_loss: 2.3102 - lr = 5.67343501 \n",
            "367/390 [===========================>..] - ETA: 1s - loss: 2.3102 - acc: 0.0985 - LRFinder: val_loss: 2.3156 - lr = 5.80901516 \n",
            "368/390 [===========================>..] - ETA: 1s - loss: 2.3101 - acc: 0.0986 - LRFinder: val_loss: 2.3195 - lr = 5.94783523 \n",
            "369/390 [===========================>..] - ETA: 1s - loss: 2.3102 - acc: 0.0986 - LRFinder: val_loss: 2.3156 - lr = 6.08997283 \n",
            "370/390 [===========================>..] - ETA: 1s - loss: 2.3102 - acc: 0.0987 - LRFinder: val_loss: 2.3135 - lr = 6.23550706 \n",
            "371/390 [===========================>..] - ETA: 1s - loss: 2.3102 - acc: 0.0986 - LRFinder: val_loss: 2.3477 - lr = 6.38451898 \n",
            "372/390 [===========================>..] - ETA: 1s - loss: 2.3103 - acc: 0.0987 - LRFinder: val_loss: 2.3072 - lr = 6.53709206 \n",
            "373/390 [===========================>..] - ETA: 1s - loss: 2.3103 - acc: 0.0987 - LRFinder: val_loss: 2.3108 - lr = 6.69331125 \n",
            "374/390 [===========================>..] - ETA: 0s - loss: 2.3103 - acc: 0.0987 - LRFinder: val_loss: 2.3171 - lr = 6.85326348 \n",
            "375/390 [===========================>..] - ETA: 0s - loss: 2.3103 - acc: 0.0988 - LRFinder: val_loss: 2.3046 - lr = 7.01703807 \n",
            "376/390 [===========================>..] - ETA: 0s - loss: 2.3103 - acc: 0.0989 - LRFinder: val_loss: 2.3113 - lr = 7.18472633 \n",
            "377/390 [============================>.] - ETA: 0s - loss: 2.3103 - acc: 0.0990 - LRFinder: val_loss: 2.3138 - lr = 7.35642201 \n",
            "378/390 [============================>.] - ETA: 0s - loss: 2.3103 - acc: 0.0990 - LRFinder: val_loss: 2.3136 - lr = 7.53222079 \n",
            "379/390 [============================>.] - ETA: 0s - loss: 2.3103 - acc: 0.0991 - LRFinder: val_loss: 2.3265 - lr = 7.71222080 \n",
            "380/390 [============================>.] - ETA: 0s - loss: 2.3103 - acc: 0.0991 - LRFinder: val_loss: 2.3148 - lr = 7.89652215 \n",
            "381/390 [============================>.] - ETA: 0s - loss: 2.3103 - acc: 0.0991 - LRFinder: val_loss: 2.3115 - lr = 8.08522783 \n",
            "382/390 [============================>.] - ETA: 0s - loss: 2.3103 - acc: 0.0991 - LRFinder: val_loss: 2.3128 - lr = 8.27844332 \n",
            "383/390 [============================>.] - ETA: 0s - loss: 2.3103 - acc: 0.0992 - LRFinder: val_loss: 2.3189 - lr = 8.47627602 \n",
            "384/390 [============================>.] - ETA: 0s - loss: 2.3103 - acc: 0.0992 - LRFinder: val_loss: 2.3139 - lr = 8.67883677 \n",
            "385/390 [============================>.] - ETA: 0s - loss: 2.3103 - acc: 0.0992 - LRFinder: val_loss: 2.3523 - lr = 8.88623785 \n",
            "386/390 [============================>.] - ETA: 0s - loss: 2.3104 - acc: 0.0993 - LRFinder: val_loss: 2.3468 - lr = 9.09859546 \n",
            "387/390 [============================>.] - ETA: 0s - loss: 2.3104 - acc: 0.0992 - LRFinder: val_loss: 2.3227 - lr = 9.31602776 \n",
            "388/390 [============================>.] - ETA: 0s - loss: 2.3104 - acc: 0.0993 - LRFinder: val_loss: 2.3330 - lr = 9.53865582 \n",
            "389/390 [============================>.] - ETA: 0s - loss: 2.3105 - acc: 0.0993 - LRFinder: val_loss: 2.3601 - lr = 9.76660465 \n",
            " - LRFinder: val_loss: 2.3669 - lr = 10.00000019 \n",
            "390/390 [==============================] - 25s 63ms/step - loss: 2.3106 - acc: 0.0992 - val_loss: 2.3604 - val_acc: 0.1000\n",
            "\tLR Finder : Saved the losses and learning rate values in path : {/content/drive/My Drive/Colab Notebooks/12/}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8985c8de80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh46zTQxQMg8",
        "colab_type": "code",
        "outputId": "68c8404b-598b-4b89-dcef-7e3380cffb32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        }
      },
      "source": [
        "lr_finder.plot_schedule(clip_beginning=10, clip_endding=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAESCAYAAADzBx6nAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4J06WetQYEw",
        "colab_type": "code",
        "outputId": "25b53fd4-a126-45ef-8a79-c73606b71b1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Activation, Input\n",
        "from keras.optimizers import SGD, Adam\n",
        "\n",
        "# from clr import OneCycleLR\n",
        "\n",
        "plt.style.use('seaborn-white')\n",
        "\n",
        "# Constants\n",
        "NUM_SAMPLES = 2000\n",
        "NUM_EPOCHS = 100\n",
        "BATCH_SIZE = 500\n",
        "MAX_LR = 0.1\n",
        "\n",
        "# Data\n",
        "# X = np.random.rand(NUM_SAMPLES, 10)\n",
        "# Y = np.random.randint(0, 2, size=NUM_SAMPLES)\n",
        "\n",
        "X = X_train[:2000]\n",
        "Y = Y_train[:2000]\n",
        "\n",
        "clr_triangular = OneCycleLR( MAX_LR,end_percentage=0.2, scale_percentage=0.2)\n",
        "#NUM_SAMPLES, NUM_EPOCHS, BATCH_SIZE,\n",
        "\n",
        "model.compile(optimizer=SGD(0.1), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X, Y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, callbacks=[clr_triangular], verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "2000/2000 [==============================] - 1s 443us/step - loss: 0.3313 - acc: 0.9000\n",
            " - lr: 0.02712 - momentum: 0.95 \n",
            "Epoch 2/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.101693). Check your callbacks.\n",
            "  % delta_t_median)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2000/2000 [==============================] - 0s 135us/step - loss: 0.3313 - acc: 0.9000\n",
            " - lr: 0.03663 - momentum: 0.95 \n",
            "Epoch 3/100\n",
            "2000/2000 [==============================] - 0s 137us/step - loss: 0.3312 - acc: 0.9000\n",
            " - lr: 0.04612 - momentum: 0.94 \n",
            "Epoch 4/100\n",
            "2000/2000 [==============================] - 0s 134us/step - loss: 0.3310 - acc: 0.9000\n",
            " - lr: 0.05562 - momentum: 0.94 \n",
            "Epoch 5/100\n",
            "2000/2000 [==============================] - 0s 139us/step - loss: 0.3307 - acc: 0.9000\n",
            " - lr: 0.06513 - momentum: 0.94 \n",
            "Epoch 6/100\n",
            "2000/2000 [==============================] - 0s 137us/step - loss: 0.3305 - acc: 0.9000\n",
            " - lr: 0.07463 - momentum: 0.94 \n",
            "Epoch 7/100\n",
            "2000/2000 [==============================] - 0s 139us/step - loss: 0.3301 - acc: 0.9000\n",
            " - lr: 0.08412 - momentum: 0.93 \n",
            "Epoch 8/100\n",
            "2000/2000 [==============================] - 0s 136us/step - loss: 0.3297 - acc: 0.9000\n",
            " - lr: 0.09363 - momentum: 0.93 \n",
            "Epoch 9/100\n",
            "2000/2000 [==============================] - 0s 137us/step - loss: 0.3294 - acc: 0.9000\n",
            " - lr: 0.10312 - momentum: 0.93 \n",
            "Epoch 10/100\n",
            "2000/2000 [==============================] - 0s 137us/step - loss: 0.3289 - acc: 0.9000\n",
            " - lr: 0.11263 - momentum: 0.93 \n",
            "Epoch 11/100\n",
            "2000/2000 [==============================] - 0s 137us/step - loss: 0.3285 - acc: 0.9000\n",
            " - lr: 0.12212 - momentum: 0.92 \n",
            "Epoch 12/100\n",
            "2000/2000 [==============================] - 0s 136us/step - loss: 0.3281 - acc: 0.9000\n",
            " - lr: 0.13162 - momentum: 0.92 \n",
            "Epoch 13/100\n",
            "2000/2000 [==============================] - 0s 137us/step - loss: 0.3277 - acc: 0.9000\n",
            " - lr: 0.14112 - momentum: 0.92 \n",
            "Epoch 14/100\n",
            "2000/2000 [==============================] - 0s 137us/step - loss: 0.3273 - acc: 0.9000\n",
            " - lr: 0.15063 - momentum: 0.92 \n",
            "Epoch 15/100\n",
            "2000/2000 [==============================] - 0s 137us/step - loss: 0.3270 - acc: 0.9000\n",
            " - lr: 0.16013 - momentum: 0.91 \n",
            "Epoch 16/100\n",
            "2000/2000 [==============================] - 0s 138us/step - loss: 0.3266 - acc: 0.9000\n",
            " - lr: 0.16962 - momentum: 0.91 \n",
            "Epoch 17/100\n",
            "2000/2000 [==============================] - 0s 137us/step - loss: 0.3263 - acc: 0.9000\n",
            " - lr: 0.17912 - momentum: 0.91 \n",
            "Epoch 18/100\n",
            "2000/2000 [==============================] - 0s 138us/step - loss: 0.3260 - acc: 0.9000\n",
            " - lr: 0.18862 - momentum: 0.91 \n",
            "Epoch 19/100\n",
            "2000/2000 [==============================] - 0s 138us/step - loss: 0.3259 - acc: 0.9000\n",
            " - lr: 0.19813 - momentum: 0.90 \n",
            "Epoch 20/100\n",
            "2000/2000 [==============================] - 0s 140us/step - loss: 0.3253 - acc: 0.9000\n",
            " - lr: 0.20763 - momentum: 0.90 \n",
            "Epoch 21/100\n",
            "2000/2000 [==============================] - 0s 137us/step - loss: 0.3251 - acc: 0.9000\n",
            " - lr: 0.21712 - momentum: 0.90 \n",
            "Epoch 22/100\n",
            "2000/2000 [==============================] - 0s 137us/step - loss: 0.3245 - acc: 0.9000\n",
            " - lr: 0.22662 - momentum: 0.90 \n",
            "Epoch 23/100\n",
            "2000/2000 [==============================] - 0s 140us/step - loss: 0.3243 - acc: 0.9000\n",
            " - lr: 0.23613 - momentum: 0.89 \n",
            "Epoch 24/100\n",
            "2000/2000 [==============================] - 0s 137us/step - loss: 0.3235 - acc: 0.9000\n",
            " - lr: 0.24563 - momentum: 0.89 \n",
            "Epoch 25/100\n",
            "2000/2000 [==============================] - 0s 138us/step - loss: 0.3224 - acc: 0.9000\n",
            " - lr: 0.25512 - momentum: 0.89 \n",
            "Epoch 26/100\n",
            "2000/2000 [==============================] - 0s 139us/step - loss: 0.3212 - acc: 0.9000\n",
            " - lr: 0.26463 - momentum: 0.89 \n",
            "Epoch 27/100\n",
            "2000/2000 [==============================] - 0s 138us/step - loss: 0.3191 - acc: 0.9000\n",
            " - lr: 0.27413 - momentum: 0.88 \n",
            "Epoch 28/100\n",
            "2000/2000 [==============================] - 0s 136us/step - loss: 0.3170 - acc: 0.9000\n",
            " - lr: 0.28363 - momentum: 0.88 \n",
            "Epoch 29/100\n",
            "2000/2000 [==============================] - 0s 139us/step - loss: 0.3151 - acc: 0.8999\n",
            " - lr: 0.29313 - momentum: 0.88 \n",
            "Epoch 30/100\n",
            "2000/2000 [==============================] - 0s 138us/step - loss: 0.3135 - acc: 0.9001\n",
            " - lr: 0.30263 - momentum: 0.88 \n",
            "Epoch 31/100\n",
            "2000/2000 [==============================] - 0s 136us/step - loss: 0.3118 - acc: 0.8998\n",
            " - lr: 0.31212 - momentum: 0.87 \n",
            "Epoch 32/100\n",
            "2000/2000 [==============================] - 0s 138us/step - loss: 0.3104 - acc: 0.8992\n",
            " - lr: 0.32162 - momentum: 0.87 \n",
            "Epoch 33/100\n",
            "2000/2000 [==============================] - 0s 137us/step - loss: 0.3090 - acc: 0.8999\n",
            " - lr: 0.33112 - momentum: 0.87 \n",
            "Epoch 34/100\n",
            "2000/2000 [==============================] - 0s 138us/step - loss: 0.3072 - acc: 0.9000\n",
            " - lr: 0.34062 - momentum: 0.87 \n",
            "Epoch 35/100\n",
            "2000/2000 [==============================] - 0s 140us/step - loss: 0.3054 - acc: 0.8995\n",
            " - lr: 0.35013 - momentum: 0.86 \n",
            "Epoch 36/100\n",
            "2000/2000 [==============================] - 0s 136us/step - loss: 0.3034 - acc: 0.8996\n",
            " - lr: 0.35963 - momentum: 0.86 \n",
            "Epoch 37/100\n",
            "2000/2000 [==============================] - 0s 137us/step - loss: 0.3015 - acc: 0.8998\n",
            " - lr: 0.36913 - momentum: 0.86 \n",
            "Epoch 38/100\n",
            "2000/2000 [==============================] - 0s 139us/step - loss: 0.3032 - acc: 0.8997\n",
            " - lr: 0.37863 - momentum: 0.86 \n",
            "Epoch 39/100\n",
            "2000/2000 [==============================] - 0s 137us/step - loss: 0.3031 - acc: 0.8993\n",
            " - lr: 0.38813 - momentum: 0.85 \n",
            "Epoch 40/100\n",
            "2000/2000 [==============================] - 0s 137us/step - loss: 0.2995 - acc: 0.8997\n",
            " - lr: 0.39762 - momentum: 0.85 \n",
            "Epoch 41/100\n",
            "2000/2000 [==============================] - 0s 136us/step - loss: 0.2967 - acc: 0.8995\n",
            " - lr: 0.39287 - momentum: 0.85 \n",
            "Epoch 42/100\n",
            "2000/2000 [==============================] - 0s 137us/step - loss: 0.2944 - acc: 0.8992\n",
            " - lr: 0.38337 - momentum: 0.85 \n",
            "Epoch 43/100\n",
            "2000/2000 [==============================] - 0s 137us/step - loss: 0.2926 - acc: 0.8994\n",
            " - lr: 0.37387 - momentum: 0.86 \n",
            "Epoch 44/100\n",
            "2000/2000 [==============================] - 0s 137us/step - loss: 0.2967 - acc: 0.8991\n",
            " - lr: 0.36437 - momentum: 0.86 \n",
            "Epoch 45/100\n",
            "2000/2000 [==============================] - 0s 137us/step - loss: 0.2915 - acc: 0.8988\n",
            " - lr: 0.35487 - momentum: 0.86 \n",
            "Epoch 46/100\n",
            "2000/2000 [==============================] - 0s 138us/step - loss: 0.2887 - acc: 0.8997\n",
            " - lr: 0.34538 - momentum: 0.86 \n",
            "Epoch 47/100\n",
            "2000/2000 [==============================] - 0s 137us/step - loss: 0.2865 - acc: 0.9001\n",
            " - lr: 0.33588 - momentum: 0.87 \n",
            "Epoch 48/100\n",
            "2000/2000 [==============================] - 0s 137us/step - loss: 0.2836 - acc: 0.9004\n",
            " - lr: 0.32638 - momentum: 0.87 \n",
            "Epoch 49/100\n",
            "2000/2000 [==============================] - 0s 138us/step - loss: 0.2823 - acc: 0.8991\n",
            " - lr: 0.31688 - momentum: 0.87 \n",
            "Epoch 50/100\n",
            "2000/2000 [==============================] - 0s 135us/step - loss: 0.2800 - acc: 0.9005\n",
            " - lr: 0.30738 - momentum: 0.87 \n",
            "Epoch 51/100\n",
            "2000/2000 [==============================] - 0s 136us/step - loss: 0.2814 - acc: 0.9016\n",
            " - lr: 0.29787 - momentum: 0.88 \n",
            "Epoch 52/100\n",
            "2000/2000 [==============================] - 0s 137us/step - loss: 0.2799 - acc: 0.9002\n",
            " - lr: 0.28837 - momentum: 0.88 \n",
            "Epoch 53/100\n",
            "2000/2000 [==============================] - 0s 138us/step - loss: 0.2780 - acc: 0.9002\n",
            " - lr: 0.27887 - momentum: 0.88 \n",
            "Epoch 54/100\n",
            "2000/2000 [==============================] - 0s 136us/step - loss: 0.2788 - acc: 0.9008\n",
            " - lr: 0.26937 - momentum: 0.88 \n",
            "Epoch 55/100\n",
            "2000/2000 [==============================] - 0s 139us/step - loss: 0.2757 - acc: 0.9014\n",
            " - lr: 0.25987 - momentum: 0.89 \n",
            "Epoch 56/100\n",
            "2000/2000 [==============================] - 0s 137us/step - loss: 0.2755 - acc: 0.9000\n",
            " - lr: 0.25038 - momentum: 0.89 \n",
            "Epoch 57/100\n",
            "2000/2000 [==============================] - 0s 137us/step - loss: 0.2775 - acc: 0.9001\n",
            " - lr: 0.24088 - momentum: 0.89 \n",
            "Epoch 58/100\n",
            "2000/2000 [==============================] - 0s 137us/step - loss: 0.2782 - acc: 0.8998\n",
            " - lr: 0.23137 - momentum: 0.89 \n",
            "Epoch 59/100\n",
            "2000/2000 [==============================] - 0s 137us/step - loss: 0.2720 - acc: 0.9025\n",
            " - lr: 0.22187 - momentum: 0.90 \n",
            "Epoch 60/100\n",
            "2000/2000 [==============================] - 0s 136us/step - loss: 0.2751 - acc: 0.9015\n",
            " - lr: 0.21238 - momentum: 0.90 \n",
            "Epoch 61/100\n",
            "2000/2000 [==============================] - 0s 138us/step - loss: 0.2703 - acc: 0.9028\n",
            " - lr: 0.20288 - momentum: 0.90 \n",
            "Epoch 62/100\n",
            "2000/2000 [==============================] - 0s 136us/step - loss: 0.2637 - acc: 0.9037\n",
            " - lr: 0.19338 - momentum: 0.90 \n",
            "Epoch 63/100\n",
            "2000/2000 [==============================] - 0s 137us/step - loss: 0.2658 - acc: 0.9022\n",
            " - lr: 0.18387 - momentum: 0.91 \n",
            "Epoch 64/100\n",
            "2000/2000 [==============================] - 0s 138us/step - loss: 0.2643 - acc: 0.9037\n",
            " - lr: 0.17437 - momentum: 0.91 \n",
            "Epoch 65/100\n",
            "2000/2000 [==============================] - 0s 136us/step - loss: 0.2649 - acc: 0.9025\n",
            " - lr: 0.16488 - momentum: 0.91 \n",
            "Epoch 66/100\n",
            "2000/2000 [==============================] - 0s 138us/step - loss: 0.2581 - acc: 0.9042\n",
            " - lr: 0.15538 - momentum: 0.91 \n",
            "Epoch 67/100\n",
            "2000/2000 [==============================] - 0s 138us/step - loss: 0.2588 - acc: 0.9045\n",
            " - lr: 0.14588 - momentum: 0.92 \n",
            "Epoch 68/100\n",
            "2000/2000 [==============================] - 0s 136us/step - loss: 0.2544 - acc: 0.9060\n",
            " - lr: 0.13637 - momentum: 0.92 \n",
            "Epoch 69/100\n",
            "2000/2000 [==============================] - 0s 138us/step - loss: 0.2545 - acc: 0.9062\n",
            " - lr: 0.12687 - momentum: 0.92 \n",
            "Epoch 70/100\n",
            "2000/2000 [==============================] - 0s 136us/step - loss: 0.2504 - acc: 0.9074\n",
            " - lr: 0.11738 - momentum: 0.92 \n",
            "Epoch 71/100\n",
            "2000/2000 [==============================] - 0s 137us/step - loss: 0.2536 - acc: 0.9065\n",
            " - lr: 0.10787 - momentum: 0.93 \n",
            "Epoch 72/100\n",
            "2000/2000 [==============================] - 0s 137us/step - loss: 0.2549 - acc: 0.9049\n",
            " - lr: 0.09838 - momentum: 0.93 \n",
            "Epoch 73/100\n",
            "2000/2000 [==============================] - 0s 138us/step - loss: 0.2504 - acc: 0.9079\n",
            " - lr: 0.08888 - momentum: 0.93 \n",
            "Epoch 74/100\n",
            "2000/2000 [==============================] - 0s 136us/step - loss: 0.2508 - acc: 0.9080\n",
            " - lr: 0.07937 - momentum: 0.93 \n",
            "Epoch 75/100\n",
            "2000/2000 [==============================] - 0s 139us/step - loss: 0.2491 - acc: 0.9076\n",
            " - lr: 0.06988 - momentum: 0.94 \n",
            "Epoch 76/100\n",
            "2000/2000 [==============================] - 0s 137us/step - loss: 0.2487 - acc: 0.9082\n",
            " - lr: 0.06038 - momentum: 0.94 \n",
            "Epoch 77/100\n",
            "2000/2000 [==============================] - 0s 138us/step - loss: 0.2523 - acc: 0.9060\n",
            " - lr: 0.05088 - momentum: 0.94 \n",
            "Epoch 78/100\n",
            "2000/2000 [==============================] - 0s 135us/step - loss: 0.2499 - acc: 0.9089\n",
            " - lr: 0.04138 - momentum: 0.94 \n",
            "Epoch 79/100\n",
            "2000/2000 [==============================] - 0s 138us/step - loss: 0.2492 - acc: 0.9067\n",
            " - lr: 0.03187 - momentum: 0.95 \n",
            "Epoch 80/100\n",
            "2000/2000 [==============================] - 0s 136us/step - loss: 0.2493 - acc: 0.9073\n",
            " - lr: 0.02238 - momentum: 0.95 \n",
            "Epoch 81/100\n",
            "2000/2000 [==============================] - 0s 138us/step - loss: 0.2456 - acc: 0.9089\n",
            " - lr: 0.01926 - momentum: 0.95 \n",
            "Epoch 82/100\n",
            "2000/2000 [==============================] - 0s 136us/step - loss: 0.2433 - acc: 0.9089\n",
            " - lr: 0.01827 - momentum: 0.95 \n",
            "Epoch 83/100\n",
            "2000/2000 [==============================] - 0s 137us/step - loss: 0.2418 - acc: 0.9107\n",
            " - lr: 0.01728 - momentum: 0.95 \n",
            "Epoch 84/100\n",
            "2000/2000 [==============================] - 0s 138us/step - loss: 0.2409 - acc: 0.9109\n",
            " - lr: 0.01629 - momentum: 0.95 \n",
            "Epoch 85/100\n",
            "2000/2000 [==============================] - 0s 138us/step - loss: 0.2396 - acc: 0.9093\n",
            " - lr: 0.01530 - momentum: 0.95 \n",
            "Epoch 86/100\n",
            "2000/2000 [==============================] - 0s 136us/step - loss: 0.2393 - acc: 0.9099\n",
            " - lr: 0.01431 - momentum: 0.95 \n",
            "Epoch 87/100\n",
            "2000/2000 [==============================] - 0s 136us/step - loss: 0.2364 - acc: 0.9111\n",
            " - lr: 0.01332 - momentum: 0.95 \n",
            "Epoch 88/100\n",
            "2000/2000 [==============================] - 0s 137us/step - loss: 0.2355 - acc: 0.9119\n",
            " - lr: 0.01233 - momentum: 0.95 \n",
            "Epoch 89/100\n",
            "2000/2000 [==============================] - 0s 138us/step - loss: 0.2365 - acc: 0.9121\n",
            " - lr: 0.01134 - momentum: 0.95 \n",
            "Epoch 90/100\n",
            "2000/2000 [==============================] - 0s 137us/step - loss: 0.2362 - acc: 0.9104\n",
            " - lr: 0.01035 - momentum: 0.95 \n",
            "Epoch 91/100\n",
            "2000/2000 [==============================] - 0s 136us/step - loss: 0.2364 - acc: 0.9123\n",
            " - lr: 0.00936 - momentum: 0.95 \n",
            "Epoch 92/100\n",
            "2000/2000 [==============================] - 0s 136us/step - loss: 0.2328 - acc: 0.9116\n",
            " - lr: 0.00837 - momentum: 0.95 \n",
            "Epoch 93/100\n",
            "2000/2000 [==============================] - 0s 136us/step - loss: 0.2359 - acc: 0.9108\n",
            " - lr: 0.00738 - momentum: 0.95 \n",
            "Epoch 94/100\n",
            "2000/2000 [==============================] - 0s 136us/step - loss: 0.2330 - acc: 0.9122\n",
            " - lr: 0.00639 - momentum: 0.95 \n",
            "Epoch 95/100\n",
            "2000/2000 [==============================] - 0s 137us/step - loss: 0.2341 - acc: 0.9122\n",
            " - lr: 0.00540 - momentum: 0.95 \n",
            "Epoch 96/100\n",
            "2000/2000 [==============================] - 0s 138us/step - loss: 0.2335 - acc: 0.9133\n",
            " - lr: 0.00441 - momentum: 0.95 \n",
            "Epoch 97/100\n",
            "2000/2000 [==============================] - 0s 136us/step - loss: 0.2338 - acc: 0.9128\n",
            " - lr: 0.00342 - momentum: 0.95 \n",
            "Epoch 98/100\n",
            "2000/2000 [==============================] - 0s 137us/step - loss: 0.2327 - acc: 0.9125\n",
            " - lr: 0.00243 - momentum: 0.95 \n",
            "Epoch 99/100\n",
            "2000/2000 [==============================] - 0s 136us/step - loss: 0.2342 - acc: 0.9125\n",
            " - lr: 0.00144 - momentum: 0.95 \n",
            "Epoch 100/100\n",
            "2000/2000 [==============================] - 0s 135us/step - loss: 0.2323 - acc: 0.9127\n",
            " - lr: 0.00045 - momentum: 0.95 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8a17bb7e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VClXjtk3Q7V-",
        "colab_type": "code",
        "outputId": "280c9cec-c60f-47a1-8458-185b4935844f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        }
      },
      "source": [
        "print(\"LR Range : \", min(clr_triangular.history['lr']), max(clr_triangular.history['lr']))\n",
        "print(\"Momentum Range : \", min(clr_triangular.history['momentum']), max(clr_triangular.history['momentum']))\n",
        "\n",
        "\n",
        "plt.xlabel('Training Iterations')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.title(\"CLR\")\n",
        "plt.plot(clr_triangular.history['lr'])\n",
        "plt.show()\n",
        "\n",
        "plt.xlabel('Training Iterations')\n",
        "plt.ylabel('Momentum')\n",
        "plt.title(\"CLR\")\n",
        "plt.plot(clr_triangular.history['momentum'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LR Range :  0.0004475 0.4\n",
            "Momentum Range :  0.85 0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAESCAYAAADwnNLKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAESCAYAAADwnNLKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCDMnRSCEEGZOikAIIczc/wOBVTHhIHjPZAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyfxaM-Tdho0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "cc4deb29-7a42-4852-be49-bafb4c2593e8",
        "id": "SnFGvAVhdiHn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Convolution2D(48, 3, 3, border_mode='same', input_shape=(32, 32, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Convolution2D(48, 3, 3))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Convolution2D(96, 3, 3, border_mode='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Convolution2D(96, 3, 3))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Convolution2D(192, 3, 3, border_mode='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Convolution2D(192, 3, 3))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(48, (3, 3), input_shape=(32, 32, 3..., padding=\"same\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(48, (3, 3))`\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(96, (3, 3), padding=\"same\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(96, (3, 3))`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(192, (3, 3), padding=\"same\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(192, (3, 3))`\n",
            "  app.launch_new_instance()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoJGeVwarp9q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau\n",
        "from keras import optimizers\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath='../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5', monitor='val_acc', mode='max', verbose = 1, save_best_only=True)\n",
        "# lr_reducer = ReduceLROnPlateau(monitor='val_acc',factor=0.8, cooldown=0, patience=5, min_lr=0.5e-9,verbose = 1)\n",
        "\n",
        "# Compile the model\n",
        "sgd = optimizers.SGD(lr=0.01, decay=0, momentum=0.9, nesterov=False)\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=sgd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDdokG1hR8gU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "lr_manager = OneCycleLR(max_lr=0.1, maximum_momentum=0.95, verbose=True)\n",
        "# train the model\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe2qgh8NTSMN",
        "colab_type": "code",
        "outputId": "ab136275-b581-4865-943c-ca9ab4a8149b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "start = time.time()\n",
        "# Train the model\n",
        "model_info = model.fit(X_train, Y_train,\n",
        "                      batch_size = 512,\n",
        "            #           samples_per_epoch = train_features.shape[0],\n",
        "                      epochs = 100, \n",
        "                      validation_data = (X_test, Y_test),\n",
        "                      callbacks=[checkpoint,lr_manager],verbose=1)\n",
        "\n",
        "end = time.time()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            " 2048/50000 [>.............................] - ETA: 23s - loss: 2.3138 - acc: 0.1001"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.137639). Check your callbacks.\n",
            "  % delta_t_median)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "50000/50000 [==============================] - 8s 165us/step - loss: 2.2376 - acc: 0.1463 - val_loss: 2.0293 - val_acc: 0.2245\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.22450, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.01197 - momentum: 0.95 \n",
            "Epoch 2/100\n",
            "50000/50000 [==============================] - 7s 149us/step - loss: 1.9017 - acc: 0.2610 - val_loss: 1.7278 - val_acc: 0.3570\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.22450 to 0.35700, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.01395 - momentum: 0.95 \n",
            "Epoch 3/100\n",
            "50000/50000 [==============================] - 7s 149us/step - loss: 1.6823 - acc: 0.3616 - val_loss: 1.5521 - val_acc: 0.4277\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.35700 to 0.42770, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.01594 - momentum: 0.94 \n",
            "Epoch 4/100\n",
            "50000/50000 [==============================] - 7s 147us/step - loss: 1.5062 - acc: 0.4354 - val_loss: 1.3412 - val_acc: 0.5099\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.42770 to 0.50990, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.01793 - momentum: 0.94 \n",
            "Epoch 5/100\n",
            "50000/50000 [==============================] - 7s 146us/step - loss: 1.3602 - acc: 0.5031 - val_loss: 1.1856 - val_acc: 0.5725\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.50990 to 0.57250, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.01992 - momentum: 0.94 \n",
            "Epoch 6/100\n",
            "50000/50000 [==============================] - 7s 146us/step - loss: 1.2283 - acc: 0.5569 - val_loss: 1.1077 - val_acc: 0.6030\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.57250 to 0.60300, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.02190 - momentum: 0.94 \n",
            "Epoch 7/100\n",
            "50000/50000 [==============================] - 7s 145us/step - loss: 1.1201 - acc: 0.6010 - val_loss: 1.0382 - val_acc: 0.6316\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.60300 to 0.63160, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.02389 - momentum: 0.93 \n",
            "Epoch 8/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 1.0102 - acc: 0.6436 - val_loss: 0.9102 - val_acc: 0.6824\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.63160 to 0.68240, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.02588 - momentum: 0.93 \n",
            "Epoch 9/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 0.9509 - acc: 0.6675 - val_loss: 0.8300 - val_acc: 0.7152\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.68240 to 0.71520, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.02787 - momentum: 0.93 \n",
            "Epoch 10/100\n",
            "50000/50000 [==============================] - 7s 145us/step - loss: 0.8609 - acc: 0.7003 - val_loss: 0.7441 - val_acc: 0.7436\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.71520 to 0.74360, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.02985 - momentum: 0.93 \n",
            "Epoch 11/100\n",
            "50000/50000 [==============================] - 7s 146us/step - loss: 0.8039 - acc: 0.7220 - val_loss: 0.7038 - val_acc: 0.7587\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.74360 to 0.75870, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.03184 - momentum: 0.93 \n",
            "Epoch 12/100\n",
            "50000/50000 [==============================] - 7s 145us/step - loss: 0.7477 - acc: 0.7433 - val_loss: 0.6965 - val_acc: 0.7655\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.75870 to 0.76550, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.03383 - momentum: 0.92 \n",
            "Epoch 13/100\n",
            "50000/50000 [==============================] - 7s 146us/step - loss: 0.7226 - acc: 0.7548 - val_loss: 0.6703 - val_acc: 0.7732\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.76550 to 0.77320, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.03582 - momentum: 0.92 \n",
            "Epoch 14/100\n",
            "50000/50000 [==============================] - 7s 146us/step - loss: 0.6567 - acc: 0.7783 - val_loss: 0.6222 - val_acc: 0.7896\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.77320 to 0.78960, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.03780 - momentum: 0.92 \n",
            "Epoch 15/100\n",
            "50000/50000 [==============================] - 7s 146us/step - loss: 0.6376 - acc: 0.7818 - val_loss: 0.6306 - val_acc: 0.7895\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.78960\n",
            " - lr: 0.03979 - momentum: 0.92 \n",
            "Epoch 16/100\n",
            "50000/50000 [==============================] - 7s 146us/step - loss: 0.6154 - acc: 0.7916 - val_loss: 0.5966 - val_acc: 0.8026\n",
            "\n",
            "Epoch 00016: val_acc improved from 0.78960 to 0.80260, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.04178 - momentum: 0.91 \n",
            "Epoch 17/100\n",
            "50000/50000 [==============================] - 7s 145us/step - loss: 0.5769 - acc: 0.8045 - val_loss: 0.6089 - val_acc: 0.7966\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.80260\n",
            " - lr: 0.04377 - momentum: 0.91 \n",
            "Epoch 18/100\n",
            "50000/50000 [==============================] - 7s 145us/step - loss: 0.5557 - acc: 0.8120 - val_loss: 0.5735 - val_acc: 0.8082\n",
            "\n",
            "Epoch 00018: val_acc improved from 0.80260 to 0.80820, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.04575 - momentum: 0.91 \n",
            "Epoch 19/100\n",
            "50000/50000 [==============================] - 7s 145us/step - loss: 0.5358 - acc: 0.8170 - val_loss: 0.5956 - val_acc: 0.7985\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.80820\n",
            " - lr: 0.04774 - momentum: 0.91 \n",
            "Epoch 20/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 0.5199 - acc: 0.8251 - val_loss: 0.5640 - val_acc: 0.8128\n",
            "\n",
            "Epoch 00020: val_acc improved from 0.80820 to 0.81280, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.04973 - momentum: 0.91 \n",
            "Epoch 21/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 0.5122 - acc: 0.8249 - val_loss: 0.5480 - val_acc: 0.8197\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.81280 to 0.81970, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.05171 - momentum: 0.90 \n",
            "Epoch 22/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 0.4794 - acc: 0.8390 - val_loss: 0.5701 - val_acc: 0.8076\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.81970\n",
            " - lr: 0.05370 - momentum: 0.90 \n",
            "Epoch 23/100\n",
            "50000/50000 [==============================] - 7s 145us/step - loss: 0.4766 - acc: 0.8403 - val_loss: 0.5570 - val_acc: 0.8163\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.81970\n",
            " - lr: 0.05569 - momentum: 0.90 \n",
            "Epoch 24/100\n",
            "50000/50000 [==============================] - 7s 145us/step - loss: 0.4439 - acc: 0.8498 - val_loss: 0.5368 - val_acc: 0.8262\n",
            "\n",
            "Epoch 00024: val_acc improved from 0.81970 to 0.82620, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.05768 - momentum: 0.90 \n",
            "Epoch 25/100\n",
            "50000/50000 [==============================] - 7s 145us/step - loss: 0.4352 - acc: 0.8534 - val_loss: 0.5566 - val_acc: 0.8166\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.82620\n",
            " - lr: 0.05966 - momentum: 0.89 \n",
            "Epoch 26/100\n",
            "50000/50000 [==============================] - 7s 146us/step - loss: 0.4314 - acc: 0.8563 - val_loss: 0.5251 - val_acc: 0.8310\n",
            "\n",
            "Epoch 00026: val_acc improved from 0.82620 to 0.83100, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.06165 - momentum: 0.89 \n",
            "Epoch 27/100\n",
            "50000/50000 [==============================] - 7s 145us/step - loss: 0.4159 - acc: 0.8591 - val_loss: 0.5282 - val_acc: 0.8260\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.83100\n",
            " - lr: 0.06364 - momentum: 0.89 \n",
            "Epoch 28/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 0.4030 - acc: 0.8633 - val_loss: 0.5186 - val_acc: 0.8326\n",
            "\n",
            "Epoch 00028: val_acc improved from 0.83100 to 0.83260, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.06563 - momentum: 0.89 \n",
            "Epoch 29/100\n",
            "50000/50000 [==============================] - 7s 145us/step - loss: 0.3929 - acc: 0.8669 - val_loss: 0.5081 - val_acc: 0.8363\n",
            "\n",
            "Epoch 00029: val_acc improved from 0.83260 to 0.83630, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.06761 - momentum: 0.89 \n",
            "Epoch 30/100\n",
            "50000/50000 [==============================] - 7s 145us/step - loss: 0.3810 - acc: 0.8711 - val_loss: 0.5209 - val_acc: 0.8381\n",
            "\n",
            "Epoch 00030: val_acc improved from 0.83630 to 0.83810, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.06960 - momentum: 0.88 \n",
            "Epoch 31/100\n",
            "50000/50000 [==============================] - 7s 145us/step - loss: 0.3704 - acc: 0.8732 - val_loss: 0.5301 - val_acc: 0.8264\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.83810\n",
            " - lr: 0.07159 - momentum: 0.88 \n",
            "Epoch 32/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 0.3725 - acc: 0.8735 - val_loss: 0.5234 - val_acc: 0.8348\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.83810\n",
            " - lr: 0.07358 - momentum: 0.88 \n",
            "Epoch 33/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 0.3604 - acc: 0.8783 - val_loss: 0.5351 - val_acc: 0.8344\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.83810\n",
            " - lr: 0.07556 - momentum: 0.88 \n",
            "Epoch 34/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 0.3492 - acc: 0.8836 - val_loss: 0.5638 - val_acc: 0.8276\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.83810\n",
            " - lr: 0.07755 - momentum: 0.87 \n",
            "Epoch 35/100\n",
            "50000/50000 [==============================] - 7s 145us/step - loss: 0.3507 - acc: 0.8804 - val_loss: 0.4997 - val_acc: 0.8417\n",
            "\n",
            "Epoch 00035: val_acc improved from 0.83810 to 0.84170, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.07954 - momentum: 0.87 \n",
            "Epoch 36/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 0.3313 - acc: 0.8874 - val_loss: 0.5507 - val_acc: 0.8290\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.84170\n",
            " - lr: 0.08153 - momentum: 0.87 \n",
            "Epoch 37/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 0.3405 - acc: 0.8867 - val_loss: 0.5103 - val_acc: 0.8361\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.84170\n",
            " - lr: 0.08351 - momentum: 0.87 \n",
            "Epoch 38/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 0.3176 - acc: 0.8929 - val_loss: 0.5191 - val_acc: 0.8420\n",
            "\n",
            "Epoch 00038: val_acc improved from 0.84170 to 0.84200, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.08550 - momentum: 0.87 \n",
            "Epoch 39/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 0.3130 - acc: 0.8941 - val_loss: 0.5475 - val_acc: 0.8327\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.84200\n",
            " - lr: 0.08749 - momentum: 0.86 \n",
            "Epoch 40/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 0.3175 - acc: 0.8924 - val_loss: 0.5290 - val_acc: 0.8358\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.84200\n",
            " - lr: 0.08947 - momentum: 0.86 \n",
            "Epoch 41/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 0.3223 - acc: 0.8925 - val_loss: 0.5513 - val_acc: 0.8297\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.84200\n",
            " - lr: 0.09146 - momentum: 0.86 \n",
            "Epoch 42/100\n",
            "50000/50000 [==============================] - 7s 145us/step - loss: 0.3143 - acc: 0.8941 - val_loss: 0.5091 - val_acc: 0.8423\n",
            "\n",
            "Epoch 00042: val_acc improved from 0.84200 to 0.84230, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.09345 - momentum: 0.86 \n",
            "Epoch 43/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 0.3061 - acc: 0.8964 - val_loss: 0.5072 - val_acc: 0.8442\n",
            "\n",
            "Epoch 00043: val_acc improved from 0.84230 to 0.84420, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.09544 - momentum: 0.86 \n",
            "Epoch 44/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 0.3031 - acc: 0.8992 - val_loss: 0.5343 - val_acc: 0.8349\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.84420\n",
            " - lr: 0.09742 - momentum: 0.85 \n",
            "Epoch 45/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 0.3097 - acc: 0.8955 - val_loss: 0.5037 - val_acc: 0.8489\n",
            "\n",
            "Epoch 00045: val_acc improved from 0.84420 to 0.84890, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.09941 - momentum: 0.85 \n",
            "Epoch 46/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 0.3052 - acc: 0.8968 - val_loss: 0.5273 - val_acc: 0.8438\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.84890\n",
            " - lr: 0.09860 - momentum: 0.85 \n",
            "Epoch 47/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 0.2894 - acc: 0.9051 - val_loss: 0.5487 - val_acc: 0.8361\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.84890\n",
            " - lr: 0.09661 - momentum: 0.85 \n",
            "Epoch 48/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 0.2778 - acc: 0.9059 - val_loss: 0.5555 - val_acc: 0.8344\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.84890\n",
            " - lr: 0.09463 - momentum: 0.86 \n",
            "Epoch 49/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 0.2836 - acc: 0.9052 - val_loss: 0.5433 - val_acc: 0.8430\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.84890\n",
            " - lr: 0.09264 - momentum: 0.86 \n",
            "Epoch 50/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 0.2874 - acc: 0.9045 - val_loss: 0.5040 - val_acc: 0.8468\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.84890\n",
            " - lr: 0.09065 - momentum: 0.86 \n",
            "Epoch 51/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 0.2734 - acc: 0.9092 - val_loss: 0.5120 - val_acc: 0.8480\n",
            "\n",
            "Epoch 00051: val_acc did not improve from 0.84890\n",
            " - lr: 0.08866 - momentum: 0.86 \n",
            "Epoch 52/100\n",
            "50000/50000 [==============================] - 7s 145us/step - loss: 0.2804 - acc: 0.9062 - val_loss: 0.5259 - val_acc: 0.8435\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 0.84890\n",
            " - lr: 0.08668 - momentum: 0.86 \n",
            "Epoch 53/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 0.2596 - acc: 0.9129 - val_loss: 0.5234 - val_acc: 0.8428\n",
            "\n",
            "Epoch 00053: val_acc did not improve from 0.84890\n",
            " - lr: 0.08469 - momentum: 0.87 \n",
            "Epoch 54/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 0.2561 - acc: 0.9146 - val_loss: 0.5222 - val_acc: 0.8455\n",
            "\n",
            "Epoch 00054: val_acc did not improve from 0.84890\n",
            " - lr: 0.08270 - momentum: 0.87 \n",
            "Epoch 55/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 0.2543 - acc: 0.9147 - val_loss: 0.5227 - val_acc: 0.8453\n",
            "\n",
            "Epoch 00055: val_acc did not improve from 0.84890\n",
            " - lr: 0.08071 - momentum: 0.87 \n",
            "Epoch 56/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 0.2475 - acc: 0.9182 - val_loss: 0.5107 - val_acc: 0.8516\n",
            "\n",
            "Epoch 00056: val_acc improved from 0.84890 to 0.85160, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.07873 - momentum: 0.87 \n",
            "Epoch 57/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 0.2460 - acc: 0.9178 - val_loss: 0.5156 - val_acc: 0.8500\n",
            "\n",
            "Epoch 00057: val_acc did not improve from 0.85160\n",
            " - lr: 0.07674 - momentum: 0.88 \n",
            "Epoch 58/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 0.2475 - acc: 0.9175 - val_loss: 0.5400 - val_acc: 0.8449\n",
            "\n",
            "Epoch 00058: val_acc did not improve from 0.85160\n",
            " - lr: 0.07475 - momentum: 0.88 \n",
            "Epoch 59/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 0.2388 - acc: 0.9211 - val_loss: 0.5338 - val_acc: 0.8500\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.85160\n",
            " - lr: 0.07276 - momentum: 0.88 \n",
            "Epoch 60/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 0.2218 - acc: 0.9270 - val_loss: 0.5429 - val_acc: 0.8477\n",
            "\n",
            "Epoch 00060: val_acc did not improve from 0.85160\n",
            " - lr: 0.07078 - momentum: 0.88 \n",
            "Epoch 61/100\n",
            "50000/50000 [==============================] - 7s 143us/step - loss: 0.2361 - acc: 0.9211 - val_loss: 0.5258 - val_acc: 0.8482\n",
            "\n",
            "Epoch 00061: val_acc did not improve from 0.85160\n",
            " - lr: 0.06879 - momentum: 0.88 \n",
            "Epoch 62/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 0.2247 - acc: 0.9257 - val_loss: 0.5300 - val_acc: 0.8471\n",
            "\n",
            "Epoch 00062: val_acc did not improve from 0.85160\n",
            " - lr: 0.06680 - momentum: 0.89 \n",
            "Epoch 63/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 0.2202 - acc: 0.9281 - val_loss: 0.5671 - val_acc: 0.8469\n",
            "\n",
            "Epoch 00063: val_acc did not improve from 0.85160\n",
            " - lr: 0.06482 - momentum: 0.89 \n",
            "Epoch 64/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 0.2244 - acc: 0.9269 - val_loss: 0.5442 - val_acc: 0.8424\n",
            "\n",
            "Epoch 00064: val_acc did not improve from 0.85160\n",
            " - lr: 0.06283 - momentum: 0.89 \n",
            "Epoch 65/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 0.2199 - acc: 0.9281 - val_loss: 0.5279 - val_acc: 0.8510\n",
            "\n",
            "Epoch 00065: val_acc did not improve from 0.85160\n",
            " - lr: 0.06084 - momentum: 0.89 \n",
            "Epoch 66/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 0.2084 - acc: 0.9310 - val_loss: 0.5447 - val_acc: 0.8478\n",
            "\n",
            "Epoch 00066: val_acc did not improve from 0.85160\n",
            " - lr: 0.05885 - momentum: 0.90 \n",
            "Epoch 67/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 0.2022 - acc: 0.9319 - val_loss: 0.5646 - val_acc: 0.8445\n",
            "\n",
            "Epoch 00067: val_acc did not improve from 0.85160\n",
            " - lr: 0.05687 - momentum: 0.90 \n",
            "Epoch 68/100\n",
            "50000/50000 [==============================] - 7s 143us/step - loss: 0.2017 - acc: 0.9337 - val_loss: 0.5249 - val_acc: 0.8523\n",
            "\n",
            "Epoch 00068: val_acc improved from 0.85160 to 0.85230, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.05488 - momentum: 0.90 \n",
            "Epoch 69/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 0.2045 - acc: 0.9334 - val_loss: 0.5405 - val_acc: 0.8489\n",
            "\n",
            "Epoch 00069: val_acc did not improve from 0.85230\n",
            " - lr: 0.05289 - momentum: 0.90 \n",
            "Epoch 70/100\n",
            "50000/50000 [==============================] - 7s 143us/step - loss: 0.1913 - acc: 0.9371 - val_loss: 0.5499 - val_acc: 0.8445\n",
            "\n",
            "Epoch 00070: val_acc did not improve from 0.85230\n",
            " - lr: 0.05090 - momentum: 0.90 \n",
            "Epoch 71/100\n",
            "50000/50000 [==============================] - 7s 144us/step - loss: 0.1816 - acc: 0.9413 - val_loss: 0.5485 - val_acc: 0.8533\n",
            "\n",
            "Epoch 00071: val_acc improved from 0.85230 to 0.85330, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.04892 - momentum: 0.91 \n",
            "Epoch 72/100\n",
            "50000/50000 [==============================] - 7s 143us/step - loss: 0.1854 - acc: 0.9404 - val_loss: 0.5612 - val_acc: 0.8527\n",
            "\n",
            "Epoch 00072: val_acc did not improve from 0.85330\n",
            " - lr: 0.04693 - momentum: 0.91 \n",
            "Epoch 73/100\n",
            "50000/50000 [==============================] - 7s 143us/step - loss: 0.1729 - acc: 0.9444 - val_loss: 0.5549 - val_acc: 0.8573\n",
            "\n",
            "Epoch 00073: val_acc improved from 0.85330 to 0.85730, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.04494 - momentum: 0.91 \n",
            "Epoch 74/100\n",
            "50000/50000 [==============================] - 7s 143us/step - loss: 0.1719 - acc: 0.9432 - val_loss: 0.5512 - val_acc: 0.8500\n",
            "\n",
            "Epoch 00074: val_acc did not improve from 0.85730\n",
            " - lr: 0.04295 - momentum: 0.91 \n",
            "Epoch 75/100\n",
            "50000/50000 [==============================] - 7s 143us/step - loss: 0.1665 - acc: 0.9458 - val_loss: 0.5370 - val_acc: 0.8529\n",
            "\n",
            "Epoch 00075: val_acc did not improve from 0.85730\n",
            " - lr: 0.04097 - momentum: 0.92 \n",
            "Epoch 76/100\n",
            "50000/50000 [==============================] - 7s 143us/step - loss: 0.1631 - acc: 0.9481 - val_loss: 0.5403 - val_acc: 0.8580\n",
            "\n",
            "Epoch 00076: val_acc improved from 0.85730 to 0.85800, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.03898 - momentum: 0.92 \n",
            "Epoch 77/100\n",
            "50000/50000 [==============================] - 7s 143us/step - loss: 0.1593 - acc: 0.9478 - val_loss: 0.5635 - val_acc: 0.8526\n",
            "\n",
            "Epoch 00077: val_acc did not improve from 0.85800\n",
            " - lr: 0.03699 - momentum: 0.92 \n",
            "Epoch 78/100\n",
            "50000/50000 [==============================] - 7s 143us/step - loss: 0.1411 - acc: 0.9543 - val_loss: 0.5710 - val_acc: 0.8592\n",
            "\n",
            "Epoch 00078: val_acc improved from 0.85800 to 0.85920, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.03500 - momentum: 0.92 \n",
            "Epoch 79/100\n",
            "50000/50000 [==============================] - 7s 143us/step - loss: 0.1449 - acc: 0.9529 - val_loss: 0.5793 - val_acc: 0.8578\n",
            "\n",
            "Epoch 00079: val_acc did not improve from 0.85920\n",
            " - lr: 0.03302 - momentum: 0.92 \n",
            "Epoch 80/100\n",
            "50000/50000 [==============================] - 7s 143us/step - loss: 0.1388 - acc: 0.9552 - val_loss: 0.5903 - val_acc: 0.8574\n",
            "\n",
            "Epoch 00080: val_acc did not improve from 0.85920\n",
            " - lr: 0.03103 - momentum: 0.93 \n",
            "Epoch 81/100\n",
            "50000/50000 [==============================] - 7s 143us/step - loss: 0.1347 - acc: 0.9567 - val_loss: 0.5717 - val_acc: 0.8563\n",
            "\n",
            "Epoch 00081: val_acc did not improve from 0.85920\n",
            " - lr: 0.02904 - momentum: 0.93 \n",
            "Epoch 82/100\n",
            "50000/50000 [==============================] - 7s 143us/step - loss: 0.1305 - acc: 0.9579 - val_loss: 0.5966 - val_acc: 0.8566\n",
            "\n",
            "Epoch 00082: val_acc did not improve from 0.85920\n",
            " - lr: 0.02705 - momentum: 0.93 \n",
            "Epoch 83/100\n",
            "50000/50000 [==============================] - 7s 143us/step - loss: 0.1248 - acc: 0.9588 - val_loss: 0.5867 - val_acc: 0.8587\n",
            "\n",
            "Epoch 00083: val_acc did not improve from 0.85920\n",
            " - lr: 0.02507 - momentum: 0.93 \n",
            "Epoch 84/100\n",
            "50000/50000 [==============================] - 7s 143us/step - loss: 0.1162 - acc: 0.9617 - val_loss: 0.5834 - val_acc: 0.8588\n",
            "\n",
            "Epoch 00084: val_acc did not improve from 0.85920\n",
            " - lr: 0.02308 - momentum: 0.94 \n",
            "Epoch 85/100\n",
            "50000/50000 [==============================] - 7s 143us/step - loss: 0.1091 - acc: 0.9645 - val_loss: 0.6119 - val_acc: 0.8605\n",
            "\n",
            "Epoch 00085: val_acc improved from 0.85920 to 0.86050, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.02109 - momentum: 0.94 \n",
            "Epoch 86/100\n",
            "50000/50000 [==============================] - 7s 143us/step - loss: 0.1080 - acc: 0.9651 - val_loss: 0.6109 - val_acc: 0.8613\n",
            "\n",
            "Epoch 00086: val_acc improved from 0.86050 to 0.86130, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.01911 - momentum: 0.94 \n",
            "Epoch 87/100\n",
            "50000/50000 [==============================] - 7s 143us/step - loss: 0.0991 - acc: 0.9675 - val_loss: 0.5925 - val_acc: 0.8629\n",
            "\n",
            "Epoch 00087: val_acc improved from 0.86130 to 0.86290, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.01712 - momentum: 0.94 \n",
            "Epoch 88/100\n",
            "50000/50000 [==============================] - 7s 142us/step - loss: 0.0960 - acc: 0.9691 - val_loss: 0.5955 - val_acc: 0.8632\n",
            "\n",
            "Epoch 00088: val_acc improved from 0.86290 to 0.86320, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.01513 - momentum: 0.94 \n",
            "Epoch 89/100\n",
            "50000/50000 [==============================] - 7s 142us/step - loss: 0.0881 - acc: 0.9711 - val_loss: 0.6250 - val_acc: 0.8647\n",
            "\n",
            "Epoch 00089: val_acc improved from 0.86320 to 0.86470, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.01314 - momentum: 0.95 \n",
            "Epoch 90/100\n",
            "50000/50000 [==============================] - 7s 142us/step - loss: 0.0841 - acc: 0.9729 - val_loss: 0.6126 - val_acc: 0.8671\n",
            "\n",
            "Epoch 00090: val_acc improved from 0.86470 to 0.86710, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.01116 - momentum: 0.95 \n",
            "Epoch 91/100\n",
            "50000/50000 [==============================] - 7s 142us/step - loss: 0.0757 - acc: 0.9757 - val_loss: 0.6219 - val_acc: 0.8650\n",
            "\n",
            "Epoch 00091: val_acc did not improve from 0.86710\n",
            " - lr: 0.00959 - momentum: 0.95 \n",
            "Epoch 92/100\n",
            "50000/50000 [==============================] - 7s 142us/step - loss: 0.0696 - acc: 0.9779 - val_loss: 0.6278 - val_acc: 0.8679\n",
            "\n",
            "Epoch 00092: val_acc improved from 0.86710 to 0.86790, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.00861 - momentum: 0.95 \n",
            "Epoch 93/100\n",
            "50000/50000 [==============================] - 7s 143us/step - loss: 0.0645 - acc: 0.9786 - val_loss: 0.6192 - val_acc: 0.8700\n",
            "\n",
            "Epoch 00093: val_acc improved from 0.86790 to 0.87000, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.00762 - momentum: 0.95 \n",
            "Epoch 94/100\n",
            "50000/50000 [==============================] - 7s 143us/step - loss: 0.0621 - acc: 0.9799 - val_loss: 0.6314 - val_acc: 0.8704\n",
            "\n",
            "Epoch 00094: val_acc improved from 0.87000 to 0.87040, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.00664 - momentum: 0.95 \n",
            "Epoch 95/100\n",
            "50000/50000 [==============================] - 7s 143us/step - loss: 0.0589 - acc: 0.9811 - val_loss: 0.6362 - val_acc: 0.8711\n",
            "\n",
            "Epoch 00095: val_acc improved from 0.87040 to 0.87110, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.00566 - momentum: 0.95 \n",
            "Epoch 96/100\n",
            "50000/50000 [==============================] - 7s 143us/step - loss: 0.0600 - acc: 0.9800 - val_loss: 0.6273 - val_acc: 0.8723\n",
            "\n",
            "Epoch 00096: val_acc improved from 0.87110 to 0.87230, saving model to ../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5\n",
            " - lr: 0.00467 - momentum: 0.95 \n",
            "Epoch 97/100\n",
            "50000/50000 [==============================] - 7s 143us/step - loss: 0.0524 - acc: 0.9828 - val_loss: 0.6289 - val_acc: 0.8699\n",
            "\n",
            "Epoch 00097: val_acc did not improve from 0.87230\n",
            " - lr: 0.00369 - momentum: 0.95 \n",
            "Epoch 98/100\n",
            "50000/50000 [==============================] - 7s 142us/step - loss: 0.0546 - acc: 0.9824 - val_loss: 0.6324 - val_acc: 0.8704\n",
            "\n",
            "Epoch 00098: val_acc did not improve from 0.87230\n",
            " - lr: 0.00271 - momentum: 0.95 \n",
            "Epoch 99/100\n",
            "50000/50000 [==============================] - 7s 142us/step - loss: 0.0486 - acc: 0.9838 - val_loss: 0.6385 - val_acc: 0.8698\n",
            "\n",
            "Epoch 00099: val_acc did not improve from 0.87230\n",
            " - lr: 0.00172 - momentum: 0.95 \n",
            "Epoch 100/100\n",
            "50000/50000 [==============================] - 7s 142us/step - loss: 0.0496 - acc: 0.9837 - val_loss: 0.6397 - val_acc: 0.8690\n",
            "\n",
            "Epoch 00100: val_acc did not improve from 0.87230\n",
            " - lr: 0.00074 - momentum: 0.95 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro3GlGcETuZ7",
        "colab_type": "code",
        "outputId": "b26a64dc-c564-462b-9081-5888032c5164",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "source": [
        "print (\"Model took %0.2f seconds to train\"%(end - start))\n",
        "# plot model history\n",
        "plot_model_history(model_info)\n",
        "# compute test accuracy\n",
        "print (\"Accuracy on test data is: %0.2f\"%accuracy(test_features, test_labels, model))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model took 742.00 seconds to train\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3IAAAFICAYAAAAVqcwDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDxgRBk1s+l1VAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy on test data is: 31.13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoNIlzUyu2t6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "model=keras.models.load_model('../content/drive/My Drive/Colab Notebooks/EVA/12/Assingment_12.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9-fiECub_tX",
        "colab_type": "code",
        "outputId": "19523e5f-37d1-43b4-adfd-1a8257bbd092",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model.evaluate(X_test,Y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 114us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6397096080780029, 0.869]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wx_epZj8kwrY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}