1.How many layers:
  Layers depends on our requirement.
  Usually the layers are add so that our prediction layers receptive field should 
  be higher than image size.

2.MaxPooling:
  MaxPooling is done to capture Maximum values from the input image or in other words
  capturing the loudest features in the input image
  Maxpooling is used to reduce the dimension of channels and to capture the loudest features in the input image. 
  With a stride of 2 maxpooling halves the size of input, and it doubles the receptive field of output. 
  It helps in reducing the number of layers to achieve the required receptive field.

3.1x1 Convolutions:
  In 1x1 Convolutions our kernel size is 1x1 .
  It does not extract features ,rather than it combines the useful features.
  This involves no convolution operation but instead combining of channels. 
  1x1 will merge relevant channels to form specific feature maps. 
  It combines the pre existing features that are found together.
  
4.3x3 Convolutions:
  3x3 is the most simplest convolutions for feature extraction.
  It is computationally less expensive than other higher convolution(5x5,7x7..)
  3x3 is widely used industry now-a-days .


5.Receptive Field:
  Receptive Field can be defined as how much information(pixels) of input 
  image present in one pixel of the output image or channel.
 
 6.SoftMax:
  Softmax is an activaton function mostly used in the Prediction layer ,as it gives most covincing output.
  Softmax is not actually probability ,it is more over probability like. 
  
  softmax = e^a/(e^a + e^b + e^c)  , where a,b,c are the classes.
  
7.Learning Rate:
  Learning rate is the extra parameter used in the backprogation .
  Learning rate is atlered based rate we want our model to learn.
  Using a optimum learning rate is very important .
  Because using low learning rate will make our training forever 
  and using high learning rate might cause over shooting of errors. 
  
8.Kernels and how do we decide the number of kernels:
  Kernels are the filters used to perform convolution operation on input. 3x3 is the most widely used kernel size.
  Our model accuracy increaces with increase in the kernels.
  large number of kernels gives large amount of extracted features.
  But the number of kernels is decided based on our required accuracy cutoff and the hardware to use.

9.Batch Normalization:
  Batch normalization is similar to image normalization .
  Here the normalization is done on the images channel wise with the model.
  As the name indicates the normalization is done in batches.
  It makes the kernels equally/efficiently for all the classes. 

10.Image Normalization : 
  Image Normalazation is done to get all the values into similar scales.
  Ex : 0 to 1 , -3 to 3 
  In the MinMax normalization the normalized values ranges from 0 to 1
  In the Standard normalization the mean becomes 0 and standard deviation is 1.
  Normalization helps in faster convergence as the values are similar sclaes.
  Image normalization is done on images before it is given into model.

11.Position of MaxPooling:
  Maxpooling should be placed only after the extraction of edges and gradients are done.

12.Concept of Transition Layers:
  Transition layer is a combination maxpooling and 1x1 convolution. 
  This block is used to reduce the size and number of channels. 
  The transition layers helps to keep in check the number of parameters and combine relevant features to form feature maps.

13.Position of Transition Layer:
  Transaction block/layers are placed in between of the convolution blocks.
  It is placed after edges, texture, parts of objects are extracted from convolution.
  
14.Number of Epochs and when to increase them:
  Number of Epochs is the number of times the complete training data passed through the model.
  The  number of epochs are increased to train the model more or to increase the traing accuracy. 
  
15.Dropout:
  Dropouts are used for regularization.
  When the dropouts are given certain amount/ Percentage of data is dropped .
  Dropouts is of two types that is Kernel dropout and Pixel dropout.
  As the dropout drops out certain features , 
  the kernels are forced to extract other (more) features to attain the accuracy. 
  
 
16.When do we introduce DropOut, or when do we know we have some overfitting:
  Dropout is added when training and validation accuracy are not close to each other. 
  This means the model has overfit the training data and is failing to generalize for testing data.

17.The distance of MaxPooling from Prediction:
  The distance of Maxpooling from Prediction should be minimum of two or three layers.
  Usage of Maxpooling near Prediction layer will give poor model performance as most of the information are ignored.
  The prediction should be done on as many features as possible.

18.The distance of Batch Normalization from Prediction:
  Batch Normalization shoukd not be used before the Prediction layer.

19.When do we stop convolutions and go ahead with a larger kernel or some other alternative (which we have not yet covered):
  This is done in the last layer so as to get one pixel value for each channel.
  Which are then fed to softmax of prediction.
 
20.How do we know our network is not going well, comparatively, very early:
  When the training the model if the training accuracy of the first two epochs 
  are lower than expected we can know the network is not going well.

21.Batch Size, and effects of batch size:
  Batch size is the number of images that is passed at a time or single batch  through the network. 
  As the batch size increases the time/epoch decreases.
  Generally a larger batch size is good but the performance drops as the batch size increases beyond the opimum size.

22.When to add validation checks:
  Validation checks should be added early in modelling to know the validation accuracy in each epoch 
  and to save the best model.

23.LR schedule and concept behind it:
  LR scheduler is used to schedule the learning rate.
  During the model training process the model tries to attain the global minima .
  When model reaches near global minima the learning should be reduced to attain the global minima.
  This can be done with the help of LR scheduler.

24.Adam vs SGD
  Both are optimizers used to update the weights of network. 
  SGD is a variant of Gradient Descent. In SGD the back propagation takes place for every batch.
  Adam is an algorithm for gradient-based optimization of stochastic objective functions.
  It combines the advantages of two SGD extensions — Root Mean Square Propagation (RMSProp) and Adaptive Gradient Algorithm (AdaGrad) 
  — and computes individual adaptive learning rates for different parameters.
